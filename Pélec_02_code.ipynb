{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","\n","pd.options.plotting.backend = 'plotly'\n","import numpy as np\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from sklearn import metrics\n","from sklearn.preprocessing import RobustScaler, StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","\n","from Pélec_04_fonctions import *\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 17] File exists: './Figures/'\n","[Errno 17] File exists: './Tableaux/'\n"]}],"source":["write_data = True\n","\n","if write_data is True:\n","    try:\n","        os.mkdir(\"./Figures/\")\n","    except OSError as error:\n","        print(error)\n","    try:\n","        os.mkdir(\"./Tableaux/\")\n","    except OSError as error:\n","        print(error)\n","else:\n","    print(\"\"\"Visualisation uniquement dans le notebook\n","    pas de création de figures ni de tableaux\"\"\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["BEB = pd.read_csv('BEB.csv')\n","\n","BEBM = BEB.drop(columns=['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'])\n","SiteEnergyUse = np.array(BEB['SiteEnergyUse(kBtu)']).reshape(-1, 1)\n","TotalGHGEmissions = np.array(BEB.TotalGHGEmissions).reshape(-1, 1)\n","\n","BEBM_train, BEBM_test, SiteEnergyUse_train, SiteEnergyUse_test = train_test_split(\n","    BEBM, SiteEnergyUse, test_size=.2)\n","\n","score = 'neg_root_mean_squared_error'\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Scaler moins sensible aux outlier d'après la doc\n","scaler = RobustScaler(quantile_range=(10, 90))\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# ACP sur toutes les colonnes\n","numPCA = BEBM.select_dtypes('number').drop(columns='DataYear').dropna().values\n","RobPCA = make_pipeline(StandardScaler(), PCA())\n","components = RobPCA.fit_transform(numPCA)\n","pca = RobPCA.named_steps['pca']\n","loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"Composantes=%{x}<br>Variance expliquée cumulée=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"stackgroup":"1","type":"scatter","x":[1,2,3,4,5,6],"xaxis":"x","y":[0.639683279146419,0.8551499910170413,0.9483422002934013,0.9944143074603773,0.9999999999999998,0.9999999999999998],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"margin":{"t":60},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Scree plot"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"Composantes"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"Variance expliquée cumulée"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# visualisation de la variance expliquée de chaque composante (cumulée)\n","exp_var_cum = np.cumsum(pca.explained_variance_ratio_)\n","fig = px.area(x=range(1, exp_var_cum.shape[0] + 1),\n","              y=exp_var_cum,\n","              labels={\n","                  'x': 'Composantes',\n","                  'y': 'Variance expliquée cumulée'\n","              })\n","fig.update_layout(title='Scree plot')\n","fig.show()\n","if write_data is True:\n","    fig.write_image('./Figures/ScreePlot.pdf', height=300)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# création des graphiques\n","for a1, a2 in [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]:\n","    fig = visuPCA(\n","        BEBM.select_dtypes('number').drop(columns='DataYear').dropna(),\n","        pca,\n","        components,\n","        loadings, [(a1, a2)],\n","        color=None)\n","    fig.show('browser')\n","    if write_data is True:\n","        fig.write_image('./Figures/PCAF{}F{}.pdf'.format(a1 + 1, a2 + 1),\n","                        width=1100,\n","                        height=1100)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["r2 : 0.3008225957652758\n","rmse : 11628901.343601733\n"]}],"source":["# modèle régression linéaire\n","pipeLR = make_pipeline(scaler, LinearRegression())\n","\n","pipeLR.fit(BEBM_train, SiteEnergyUse_train)\n","\n","SiteEnergyUse_pred = pipeLR.predict(BEBM_test)\n","\n","LRr2 = metrics.r2_score(SiteEnergyUse_test, SiteEnergyUse_pred)\n","print(\"r2 :\", LRr2)\n","LRrmse = metrics.mean_squared_error(SiteEnergyUse_test,\n","                                    SiteEnergyUse_pred,\n","                                    squared=False)\n","print(\"rmse :\", LRrmse)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre     Ridge()\n","0  ridge__alpha  182.499324\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predRidge=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[3305354.845489564,3959833.014897084,1258353.008315027,632284.1122404253,3167440.14111569,1158106.0553100596,397265.4301533364,4838506.228311141,45764437.91666983,1672932.7071066261,1887928.1536810084,12903878.783295523,1375287.8806349006,1684547.514966597,2269507.6360750166,602549.7678460553,7474655.029168252,3684438.1058222535,4101782.655646129,1064241.079990618,5257401.80967498,2204972.668760463,4346335.267674513,659346.5443976847,1183947.6648980253,57905003.33113688,4046042.0558486665,743272.6157393255,946504.9440046167,12682609.666087408,4178447.0270628547,1609078.7495503668,438552.60106718703,1363244.9784445076,2105947.299163531,3184731.2406400084,8442848.530874856,990690.3744660171,2692454.626007271,1440412.9389700363,759869.3852478522,1259464.523341539,1565155.0155669241,1278419.1300646607,5653933.9830135675,1967364.4425122894,14332031.878353257,1107504.9680775946,3524674.5160294166,783046.5948005344,2097214.294599054,4937277.269094236,17872946.07467027,1771902.5511170947,1944166.8523366963,1784218.8115898562,27097996.13047957,3634670.076397053,8794463.969608907,8664500.614985272,1129218.844479193,2760940.8758792523,1309558.6713973326,19822893.152130265,2653417.372846701,560380.84123051,5013448.670030888,1611213.499448257,-286162.4939245796,610180.5903882137,1459414.219586078,-197613.66241663694,5119349.573675108,3672256.7199503677,-755636.7461373569,1185514.1186518236,5090254.29759185,8484813.231540896,5931050.915266896,1293154.9004661604,559026.2413981641,2923001.69446925,1892687.037619467,6836509.1105639655,-129633.59554420738,3311507.1994832684,2154361.165642247,534352.2569546616,-374516.9956043698,1937397.942790058,267368.08949172916,67560.9384343645,2837387.322905915,8319852.260936594,1158978.140683909,1950879.8761179415,1235795.5430098376,687308.7339114468,977295.8429202144,1938692.9646149457,1708482.9329833721,570476.4824530017,3057953.225784216,2079847.7349019363,830079.9958436524,885734.1369176281,3995962.6355182566,4520603.405025571,7947003.567779324,3327295.3837530026,1942764.1267207963,2273968.5606475165,2835457.487126752,1293628.6378234304,2474131.532903539,5034055.061674184,1683305.328406533,960271.7839861992,2421988.0274481736,5771235.932177214,5112382.309058235,1780066.552264344,3475964.4569793902,4570660.109062076,1338640.1685692482,5245000.755374363,719818.9884661259,3092452.6500714696,-9308.012175976299,9305993.68454782,769297.0591718799,1660382.0273540867,-79554.17521379795,2463819.6751444773,4210463.834664751,2215895.836276711,1387615.2963085952,694161.2595857817,370947.5944929435,4661111.590427654,2309034.1225151448,868180.9591269027,5056833.813020922,692500.8420778152,1589334.0791531804,1208185.4756404688,19872972.572460674,774164.7852541287,2155355.635155436,14645788.19838854,661637.2064828556,905343.9195874843,896425.5236742073,34463310.80228553,1129418.5172777183,4068146.3307899907,219958.05105681252,3511227.2102443236,19090869.222905535,1224245.712575558,16322953.255690996,5135708.165741765,3294242.4327547103,4401206.40873045,879143.6899258611,12938165.984937575,-2228513.085404613,6941123.131433507,14392960.148228146,2924340.606565806,57955082.75146729,4517934.702712762,1097714.1623438103,5430805.593712652,1069966.4040129902,787654.523512969,6621731.085097043,11761937.158794776,9080920.949908428,361490.1358336271,4828423.746139333,5652905.8433700465,7615283.196261273,1807068.6595885565,1114157.3910811702,36403.74780615582,614078.6364846413,2270526.713985184,3469806.5249133087,1809361.9623344173,1521693.0437575646,2025451.30543374,6153160.527185083,-115468.05849979678,3824046.897374016,2087897.115715833,1340274.4647968956,4290940.232231956,19750114.734100223,1036876.9593947246,456865.10312809516,2323570.14013051,1124734.2834754288,-94941.41588485194,4732532.124766109,1754432.7606071807,2316470.448279068,790760.3138034851,13241907.643563375,14855839.805164486,2369200.6691245446,1456513.6288094488,3079631.7122215936,4185483.235783813,2432495.30535847,5194921.335043953,6175772.361768175,12613196.122908156,1314863.0960733532,1433981.7366055925,1982727.114346777,1573159.8630119613,4680708.454230078,1090238.130382718,11563081.028786391,26785187.08485794,12853799.362965114,8677913.034510907,4325990.126405059,753502.9066137676,1320049.7648758017,2262104.399820806,3661133.3478591153,5512407.494475968,1149461.388372432,338614.69287985284,4875266.389846778,88378.54366213735,12591876.11098877,625043.6187043956,324278.13723905245,3443184.7917410363,1322930.7200704236,210947.6325654902,1697735.8637991422,5713760.847785361,1140342.9660925658,1918893.4959295543,1718657.6547640283,2680316.365004243,18262827.16394836,17838736.9653249,5125184.187614173,3700936.4051636583,1806680.3263702565,837192.3652219481,8818749.390816491,2010629.3950826935,2323495.1968401484,885315.5148752045,1605215.5873277164,1037830.7063754792,3257803.7059091646,2053437.4629455484,2845933.420597637,4012225.025410364,379625.38092970033,1589641.6580822384,1917285.0221818802,2398060.6490599895,1670142.9361348029,782792.3241460817,788117.6259545507,2214000.2496118825,12954187.24258909,-125043.91390489228,2792783.1449888065,490084.45409522206,857963.0860060998,294099.63485875865,1265721.2724407269,2285436.543235214,1370146.7982173888,2358141.500268374,7626029.074119379,5169428.994005517,715340.885535425,2362789.6229383424,3458482.8759360933,1140317.5507131272,6667472.385558415,2783704.3418765008,1847872.4774110215,3200613.5402934114,1897218.583917421,1373682.2814167696,498247.2147853123,4904354.156919551,7218155.788096629,1203573.5586026385,-150137.00010174373,8188136.610293866,1412126.1943204436,491600.87937865383,652889.7689319076,870276.2472447983,4622827.953612108,2013375.6774408813,2293532.5274743238,1832530.9634977179,31930675.690758847,3352853.01610702,913257.6649570246,991020.7066287908,3557975.0411994816,3527157.8580317074,1512783.2656244775,1008527.519860635,125620767.88789257,2812066.599389577,1207839.5426338355,1261147.9948205699,2194602.363171295,13882269.927096173,10026718.44632727,1214079.9157838721,4943563.372722976,4288738.047661584,1471306.6552924688,1040513.9000490222,2224523.375168664,1719180.4124184758,1742329.188444424,270855.7641927544,9354801.035074946,986797.5390643154,3302773.5957766105,2444032.450673458,5495154.040785013,1792801.0424909163,4652876.48995711,3234352.056296981,3614876.6774077034,21755873.596990235,4572260.718484075,6926.147288694279,7123925.546010676,11878220.974563226,3332560.0580004076,7140485.219384557,2846264.24448791,9197293.159934249,4369718.130558738,5280247.913368169,2314639.1182854595,910394.0141153308,540377.1374429383,3371111.2463473086,5751534.627223751,1352430.5720495451,1920551.2531066916,4289967.893078691,684875.6843216936,4888794.452852862,7229606.08257331,1419841.0281239438,4601380.992507093,1621607.4808260314,693142.7144777414,2970771.1335529415,738116.3905255077,15572654.307276685,3868301.7131282184,8048976.3511006,867355.0609543617,268698.09523857106,77937.0817188262,4655418.884099101,1053476.7000187025,11183825.921410352,1090447.031983757,744196.4052076016,2408359.6371795833,1054907.357508524,1308432.4286454362,587272.7649139685,5379636.840803188,9639023.79444895,1022518.4494378411,10508253.30315927,3343987.6344764153,12980771.867191501,5543748.880567629,5103936.245252678,983506.1028788663,3534580.1400129283,3424651.420494025,3997315.1697965763,2120793.023126785,2656280.6701203315,2780601.705729844,692300.6351948732,6203083.065265755,1675650.9294376578,505793.6533253491,586902.2650422296,610651.0799634568,1185170.9950036518,6191896.378613206,599829.7160673549,2316118.715633373,9126122.90011803,1239608.3295825173,1695850.9520972795,2494516.5929007167,596826.0040370398,662920.0334397603,5502161.644707706,481884.38891049544,6996557.955497014,9799612.280626813,757314.9860503196,1216622.9852954056,742615.0441657915,1571059.3804997716,482848.49776532967,1760222.5304956282,4083700.814189746,5624800.701251783,2129927.1552323457,1169832.4099914692,338154.5423154249,1705883.1141710351,11373005.54592963,18381439.944256343,2147581.5490475846,4637095.53112575,5247358.246687357,-197258.46105881967,51380570.805730134,7157236.4247609405,51957545.026963,7496042.29014203,1560802.0455886358,5371729.520711575,12722189.907135159,2188686.932038029,4128367.606732446,3897652.4158664015,796360.331404946,733552.8438348041,13099489.763451122,7707011.7108731875,1702068.3139674026,10956837.874371491,947360.5642837242,879066.613995942,1083407.4246993288,1064312.9821994728,2563073.09006717,7105389.780926199,2796995.5838196594,1003421.20338391,1365787.7859327598,3793238.649914251,1945128.5319059135,3528775.2634399263,358619.21010478307,1004471.2836039462,2523292.731652891,796341.8334081471,1669612.101265392,4865745.880705697,4365839.419084594,26805815.11057659,1568487.8544062064,7168076.36776622,1847142.4891991313,4839604.04314491,1128430.8390319026,-245691.82915296126,15398344.59841133,6008457.859085443,470133.1334623152,5131206.735566946,195322.12701247144,5380726.173382243,6191282.595191912,508793.30134418816,1688014.5465097337,2195965.4090797547,3605997.646595912,519456.5158383786,857344.0784604526,1230329.1648810492,2540166.5681307623,1393596.5150783716,151176.87074073777,6966846.027431474,509185.97767248796,1847331.1853855322,1386015.758240966,4560101.345177258,2210626.277642399,3587306.547168596,79081186.88064823,3839536.2844418185,1796595.8175745402,886796.5824244204,3478695.843109517,382896.03732950473,1190987.1496807588,1072597.8697682503,3225498.012964907,1043364.4010926257,3251989.2702420047,4482685.165521869,244212.29259127146,650606.1701959809,306734.9255986891,7714927.141072072,1083116.5676780052,1589918.4779929717,2561837.3680371833,9409574.45912102,3080080.243073662,1943875.9953153725,2018608.3004866415,1353068.2077216809,2320606.1343155936,788207.2633304172,4543080.863036832,25167055.434188463,1249366.7516885574,8915200.303121582,1170199.2963533988,2888088.622395524,825831.2773070505,2048905.7533892056,54810510.134065025,-77855.3936034916,3356595.1367960675,5473671.504139464,1506593.0491398578,177957.37805526168,792735.1341447663,610094.4596252097,3300814.3137377016,1747815.2841295514,6761505.200320927,3938530.502490567,7740333.17979408,1145212.66983583,3830926.5505302697,4855063.096047675,1673457.876059514,1670616.161601724,1575419.8882459197,5796355.164271004,-283424.8471351722,2332803.114376778,1324420.1011119732,1425367.30096531,10400797.975730736,442507.7219699437,5740589.445447957,2580164.6080183517,6752364.331789686,1740009.718077098,1542756.7671136458,27668587.551872216,16351659.666072834,174715.05490993476,2451847.9738348094,2894227.844186198,2453278.8507218254,2140171.596630444,5053856.824922269,2821506.4818315078,3507903.1878480986,11666208.828832727,745133.240130234,946648.3948542632,932699.1864270833,3104259.651276826,2676760.180315095,630028.4832972339,6765591.18599394,5666840.936852062,1382656.3860035476,32952586.48108231,1342971.9670721858,1145293.2294678474,18629306.98009945,1838705.6241545947,1095040.1257860293,1826348.0043667736,377022.53269203333,1573960.8854351416,1489526.973187211,1973384.7212282193,54492697.68599202,1201655.7037482024,1153513.8943499224,1568044.9425687385,2749143.148864305,6355612.043547934,8618643.803387221,1152883.2334370576,3402831.4511086782,4690661.918733595,849793.4458891554,13266301.449694928,21603173.3918295,36910457.516223446,6224880.778772677,6629459.553965424,7076472.424676688,1728062.2219648815,-348954.41519646766,2371227.110341322,1463317.4832454287,5762420.810620956,874535.4370023173,649451.1506461792,8426892.3836101,718129.2438606685,6178253.971365746,4138585.304289843,1326153.346280028,4989610.888656942,1423537.0317283669,2806042.1038906705,2200997.845686609,2215721.726446645,35789155.47796345,245398.9576592464,11821.999130286276,561981.0764626565,1012698.8539902358,17888816.38565531,1663320.2924125935,3895937.806997097,1390151.1501940298,945336.853461162,9864866.333131444,51330491.38539973,24519443.765123434,12266267.222258464,2701915.441284945,2970173.704928263,3698575.597023131,560101.1700578043,1044890.65478003,935762.5100673023,2571196.6941946605,4292996.226790568,287047.2195984798,193941.15943458467,703135.3241692372,2217840.0327880485,20544149.709688775,3950656.2117201826,1343104.7980756795,2007979.095593227,1401603.422331726,474534.08643301483,4671954.268150134,25049909.512402676,4301207.109473269,2481209.680131648,17927009.08441192,5462204.883154691,2506538.8719319995,125570688.46756217,5493480.093729798,3012462.1615535435,10088305.643383801,3000974.6902202675,47041022.1289426,829971.9371494357,5881367.383085952,11554669.903160196,1878657.675772335,1634706.882171979,1444555.0934329387,6695946.88072217,1265598.885824743,2342496.885815366,2562489.398363507,1884224.209252119,7514146.425525112,5722240.827259376,39173305.48654738,1975371.8851033305,957940.3253638356,2406653.230371513,3577992.7609802997,3311111.5139724626,6648254.749173053,1381488.295460093,943104.219854604,4201219.619456232,5934134.524919782,11622872.851294048,739278.1678571764,9914884.486805333,588712.3256756659,1135091.5746732429,2711521.004783393,3178889.5618184437,791218.2125784941,4274490.485761325,5231561.447538044,52007624.447293416,6128174.551035337,25604301.230467267,432975.45765991416,224794.47524034395,3980028.451732897,3692657.170030898,527981.5037132334,712343.8598576509,1301100.750886608,335058.508190705,5650493.2722045425,782372.3520840486,985172.3001985438,4800068.64732776,7507515.076074506,640963.9177592262,5090687.952212897,428003.3173820139,10545072.738295935,10461300.246929102,711716.6268132648,1500729.2153356192,1780141.8326630166,7821847.150510906,24371062.93780937,23252703.48542165,1846550.466699041,3413583.6962499963,4994954.334446486,-205028.00051639695,1372453.4267501188,592545.4490212002,691196.4945323293,-63988.45940300869,1050562.4753329596,8361470.19626414,1204790.025645534,1842661.5080088568,2312051.5726874955,2147293.7149294633,1450094.5569951823,2380483.3584282966,553088.2390366215,10214960.809152987,706451.8476496576,2058058.5159236365,2602539.2453614045,1877823.6557352208,2370040.5717183654,1453831.7250312201,1811775.411566764,901512.2045689248,1049166.410428204,2157233.4656507024,30658960.507615834,2981845.285446492,5541878.8203487685,88086.3519142901,2017735.215782289,67435005.82954147,1823991.6213977158,5505770.328225609,5048401.38857397,957487.1429652341,684932.5080861929,-348266.0631596707,1016670.9388556657,597414.7991964007,5589051.986456565,3705179.946408724,1614907.9249530516,856449.8941534578,326318.1646553504,2732609.396688674,650571.149867842,70773154.33521864,928861.6156225081,871112.8961030506,25194872.767122116,985029.5604624406,568034.197441288,718925.2822885136,805650.6400364728,17787571.04094822,3753348.036813345,2643750.405167253,760939.9748631255,228036.79838567087,1536970.7606211894,4057089.826263898,9363565.483452395,1924089.0189680625,4683242.3230570955,44280336.24820732,662320.9704346834,2601644.0163110048,-936590.6594403661,2288338.887611179,4939531.468326533,993183.6401850132,826324.839793921,460345.6303160675,4230657.427312721,2751142.7293100227,1448947.6695462393,2252771.385592963,6548422.998332365,2827487.20086448,9823322.178487815,1364113.0883462462,21875064.55090584,3646826.153042967,3862464.4564128853,1091796.3731968058,820121.6335446546,493957.09302208247,2145885.9887493453,501349.64035380096,1270497.0050969408,6006775.719445476,4712521.834165107,1000483.0550025506,-1125378.010042185,16563524.463291608,6855757.341492504,5927093.704182278,407173.2540887457,264990.02127244696,1793185.0613960545,10253529.845313959,25554340.608282544,2817702.065391294,10551445.801690593,1925614.3518658918,1622853.2867762167,1162385.0057467797,2552459.8250309955,600988.3672762546,1766263.693244308,8567928.857945867,639769.3725783003,2889227.043957128,11709304.21644599,4439815.980473656,3491638.9707608866,3141145.4180727424,8498943.207336716,2854287.681475864,644424.7036421162,17527909.476012796,4706629.821957115,950679.7826713475,280886.0750770448,3014250.14781119,2028292.8637533865,503008.81870621233,2170978.9543719054,2285383.4966732953,1509037.718802494,3910896.6695105117,5585318.763441805,12236098.312178653,7656933.2826888235,5831287.962755543,1026289.1612924307,5581784.550547073,-278342.93186265416,2163920.8292814735,2966393.1181274015,1216829.2246677158,3022023.1801876943,996227.4345055823,1555136.1669973072,5913851.511929474,8106534.918501326,13109421.01694654,1367100.8238399117,4173786.1638566656,4244744.793546953,8446806.614791665,2072319.6029647682,9027432.251882073,1550333.1455273165,1698712.8558545385,100041.70926322066,2095996.1434844148,2676674.603411985,1893418.5000022764,11351576.272083305,9652691.936388757,4838245.59539052,2366198.1359637817,2099620.6255165036,733807.1144892564,27891449.83714609,12225116.07067705,4839076.296078291,4607604.735653289,5770620.883082932,3778776.1106928596,2401768.5535044,2714328.433676208,643315.4255380477,3022804.3058360196,2898939.5449085645,2924609.178836886,335203.3595848754,1500780.7611934007,4440790.872564078,672304.5238302175,2805362.862657502,271994.55132051,10826204.621674944,2324209.697498689,8068781.345657927,1509146.2073782205,2043633.5127181248,23087226.070581812,979366.226486055,2410502.603311064,3054180.230946417,2694065.9006328974,4954630.095249173,1641431.5355934117,5798705.320977766,5790719.046810415,1133748.8897425767,1335351.4099530492,26298204.497503545,3338829.5400422406,1180128.1019627843,4840748.818231114,1121815.2645115093,8056455.498170916,7088163.15490766,2110371.3988356735,1560558.0415379726,1290151.5216130272,292333.67726500845,12374400.415068157,1208919.4189738245,3521289.5913169347,1346426.290052177,671237.0797283822,631049.8303063703,1095133.249505421,10258552.095869772,1447325.8849607033,1438608.202198455,8520332.716372605,612241.5501042742,7156888.42699598,5695086.4326766245,2267919.453118458,455846.03299073176,3484839.7500644038,633686.8378563249,1621138.8008301808,4786135.274897079,835236.0945447953,-183299.12968129478,4575414.69483517,3149217.5976508,6733720.988656519,12268194.295002032,1405997.5148540344,8744384.549278498,4991312.642036615,256859.38496438786,4168469.807785256,10232853.208593376,438845.4357495266,-133601.04870657017,2500454.2190130195,875246.5035100838,13993713.197154865,3471210.1709865257,-266361.7427397724,339509.8148245169,2553277.4606317626,1174482.805354201,304292.89624374895,444376.6426895419,2755124.3676340813,3905561.0979789514,905974.9765677413,5354404.51837983,4313827.0939337555,1932384.8885515947,225984.87337057665,927010.5913295769,4078005.7899831347,4989607.815037254,10886026.38899228,44887781.04953469,1276048.4436851596,311557.94100018684,10885808.146205176,2833783.7622069097,1861684.0482998576,11434275.956114862,7882834.528011017,698592.346567981,2684947.478353031,794158.4767482274,1417961.984579558,3929949.031402488,13975925.050620224,1773912.2010673066,1427645.170117208,1183648.522646265,1068854.5354184143,3359766.949012013,-20197.836626878474,1055129.161766035,25688523.142416522,10239407.033842172,927002.3522227779,742906.4313095275,2308062.0799379647,4968689.801727515,4557525.31532288,3885743.6347794156,4485388.371000957,1080726.1423032323,18433080.308583573,4126498.1385910315,2758959.8057489493,5681383.119455929,1601498.520720338,3469415.9072685814,1413238.0629150192,957501.2950283193,1185514.1186518236,7910362.546653873,10460831.814909741,6523646.2769028405,2595591.3634232464,860060.3231304691,2120782.996274284,57883918.02102695,1820534.8675112622,4510021.924846849,2387494.009502915,2611916.7883675927,1582002.754962416,3432231.592037144,2153582.0498313042,3903370.698970768,2008618.7711634384,925097.7322072657,6139979.352291573,6958948.620001565,8135750.045155606,1606772.3968803133,3966026.23327009,19524979.06553089,-173652.66503115185,5384776.550717784,6700441.712609563,3334958.122861334,9163285.97625564,1250875.9020455368,1942766.457949876,-124589.34668706357,2012722.6843608452,907262.974882609,7676108.494449788,460254.4460932687,2020728.3995753129,1968404.5512482126,6532783.6689148275,996623.3304352202,2196729.8451360636,1269970.3445453926,1297599.1640533875,1479431.0911174058,2618377.31255195,838084.925435991,9681313.370721903,990264.7496582184,51867109.450350985,811368.8959970847,854015.8081624014,2100590.717905512,4102182.943563455,787468.4716614855,21805568.80746413,1763858.617637265,3453848.4561726423,1350741.9080240158,3939734.7937044157,366152.32816504897,3700625.9747996577,3685072.067593416,6003266.810507759,3042958.3503036033,2096199.3321913285,1140907.7293503496,5237979.923786929,1216097.7732243107,624036.482451376,223071.26639807643,1031269.3312724372,1218255.3929937622,16876468.9690574,20884339.2669874,11473114.85143635,237781.91065751784,1506834.9222818818,888164.3457664005,5938456.769758191,1595039.5599037725,1869469.6695116637,6465282.622721992,2673093.529389205,55221.371662828606,4355398.651456705,8147390.312627058,3298918.398667437,1509493.6399164873,10555403.506616615,600004.8973696986,290444.8462454055,10579768.066642087,5318689.633583503,1462703.845294068,2922804.3518007435,691551.8793985404,5618671.286465235,2825278.065257851,2737349.1365169967,1647365.6615693246,1025181.4708808269,19040789.802575126,7103430.540578958,14556451.216094527,1387808.2643666742,9982811.532064818,5880971.494936486,2732030.2816498713,1916429.544252975,1373929.2064720164,19865480.50048777,2755962.6835602615,2738675.884390608,5043816.082403736,7984991.405034577,105696039.50433156,3847572.995535992,20044740.071462836,1025957.3089322937,1560190.6083317315,1768930.4116996743,1107455.018984042,999641.4257862845,5428292.737882185,2347981.2287295805,2149898.0945524867,288007.0974871686,6293967.103665312,1728535.4474318027,4976670.721140599,632247.1723054899,709187.3743336177,1399864.7216154293,5979003.0226257825,-132548.70641519642,2173204.3156540086,3188322.2622661423,11294287.586114902,201810.39759802865,807264.6581300434,30323835.8822117,23372716.23140926,1025682.1550186092,4532764.585852278,15622051.032633826,1701818.8237198587,3090573.799443837,950753.6625412183,11829088.526222263,1249709.7891630975,2630236.9446738334,1169102.9362090798,4122850.769679085,645814.3394885471,754358.1334366659,2128033.163094751,2644461.683430425,4042175.3499079095,996727.8151846724,505174.3485477348,2152305.824280187,4032538.00811666,1628543.2949566054,1176087.1814296416,697446.8603044352,8408291.64768105,1238452.6987181287,1793269.397967117,2819361.00066764,1453204.6231095828,2569665.0784674566,1734139.3912594467,2627085.845129337,1137188.4582295478,774184.0672680722,3476067.2401693827,267899.93872235855,7656932.290542779,3789456.8641114095,1288207.1891051251,1255757.723204282,3815600.537218533,12477500.319225457,3382152.171706735,35149656.394888595,1592178.4479809306,5021754.734212941,4905142.516378084,4092254.770238319,2811020.2962096618,3175418.592634498,1582581.7749663154,20787095.137656253,1405644.844546966,8546703.60875591,11752473.98877379,1952759.7825227936,3148345.6986158346,5088402.065519138,4993642.793053385,18468754.603701618,951110.7159782944,4918610.381397106,5050035.617792323,7790412.600124489,300551.05790093075,8940180.15108403,443877.67269167304,1232909.257887873,4399252.841495962,517416.131906813,1161882.0782550576,3398960.3741575996,5538972.566126156,322133.70009808615,17422917.36505266,2357435.8932684595,305603.5534714197,445280.0394456289,1657910.8291395348,19849955.53566601,12008612.255315326,1295629.6975186882,1620536.741271315,1957918.3656263663,3624765.9583859183,10460636.877115477,5343506.588639476,2529945.0797767914,-247337.8813892291,1532636.0129455128,3250734.8934072927,1215519.4654943338,1907007.294965493,9852101.314629182,1320576.4254273497,3879352.3084397847,2134365.6381308725,3677752.1399116907,4579331.875227334,623921.7054801418,3329441.5438321857,7595380.0099580735,2661441.584452984,2199977.5148828956,3946017.2273275065,27210624.787227727,1934303.6295825285,2177016.3950905604,468699.6165396245,14169068.041959234,2382154.7475985396,796341.8334081471,2722463.1115192324,-815293.6105443677,2071316.5765139828,885921.6801313693,8472474.235952277,1635382.4652345688,4483418.351559186,6675862.595923248],"xaxis":"x","y":[1437220,1614322,1416488.75,780410,5912066,1318947.25,281191,9146727,53332648,600528,2842760,6711975,2177075,8054334,962971,766628,3009967,2161828,2962476,860443,2075833,1121259,2709429,1406687.5,764094.5,26689658,5276777,1372475,1078936,9162048,4678999,768325.6875,572654.3125,562495.8125,2504814.5,4696639,875943.3125,436604.0938,1210262,841891,437372,1267348.5,2214604.25,1696448,4933374.5,1691021,3920913,1813864,1545519.375,1988760.875,1471914.25,5827595.5,6508595,804620.5,4236396.5,1524620,25307744,1661448,15232788,9512829,847298.8125,1489363.875,642052,22104512,1904641.75,131810,2462408.25,1261714,577642,479358,1355062.75,715064,63835192,1479728.75,592739.3125,557863.6875,5543575,2634166,2756494,433871,508057,308400.4063,1430356.75,12187231,592899,2216979,1884194.625,690750,673834,2478045.5,847767,647792.875,1810353,13458948,1072855,1606981.25,2291194,347437.6875,1423687,2045592,835258,2978555,2203026.5,1750419,1070381,614044.6875,5337334,2409429,3093378,430500,5767765,3418003,4618360,3546459,1310226,2729167.75,1639849.25,695955.8125,5061274,5454482,1498457.875,9190442,2996502,2890379,1795493,2364351,610744.6875,1424157.25,1003659,5907068,784584,725487,585851,1513700.625,4547129,362874,917100.3125,674961,619545.125,1906597,2001936.5,980356,1883132,721549.625,939840,1098965,23230284,533669.125,805751,7860942,883197.6875,784584,1049436.25,48143224,745575,1792425,603223.1875,1938613.125,15088676,1297041,13631141,2131817,1153276,4397572,960987.8125,19609472,7768294,2757855.5,7559797,1325972.75,25475474,1841968,559805,12051984,1469556.25,1314300,3375091.5,4665069,12726789,508264,9796649,1973429,17074142,7542332,531737.1875,1672142,1739693.25,3307792.5,1436773.875,843622.875,987987.3125,9997511,25478086,713660,3594908,2755829,1147582,2349950,27717026,1982967,429591,1645748.25,895901,947534,3528086.75,841821,973242,188745.7031,7564643,20311228,1593160,495481,5570472,2594119,336745.4063,2284133.75,5229981,13146645,838394.8125,2520919,3218565,1514279,9177040,1048781,6456026,24882924,6772289,10813735,4993962,628676.375,888561,892227,2842819.5,2265836,196410,541451.625,2371304.5,1325206,17812788,481206,519317,971155.625,1509492,1419453,1093724.375,8141155.5,797435,1162131,451738,1162519,5696695,6769410.5,2467851,6376820,1821047,600543,2637656,1299001.625,1857387,436848,2062419,1082076,1145535.75,3242077.75,1021754.688,1298039.875,395346.3125,1489392,1858224.625,1406987,1056099,2055534,840549.375,2126288,8819864,956598,10420096,350264.5938,507976,1565475,788976,599390,2345314,402136,2220053.25,65225380,880590,1916309,1241023.75,1093627,3879728,7442314.5,1256575.875,448676,1198291,1452804,645451.8125,2557933.75,4785692,522972.1875,919746.3125,1118392,145573,1093757.75,1493131.75,975327,2015280.125,861643.6875,1199742,699382,18760766,6471427,967492.875,2788860,2104255,3632613.75,5657046,866343,59991984,3497215.5,462192,682249.625,13140574,13349651,6856153.5,3033340,2646879,3925806,2852102.75,595978,2559486,841242,456397,618366,1555420.25,2058532.875,4383100.5,2321336,14697487,2241280.5,3059758,1260212.75,1791715.625,19594950,0,1689631,4863803,8259071,1820085,3983784,14084606,10287986,2966309,2059083.25,3040978,667871.125,869835,4704936.5,2091869,2067352.5,1199392,1878442.25,1297872,4917420,3591653,3318151.5,2105451,1391569,520206.6875,3292049,540465,6219841.5,2014500.375,5328224,421430,844000.3125,649874.875,10557910,4841347.5,4606967.5,962577,439521.0938,1275236.125,1426023,1299232,2655387.5,4252035,6320060,520011.8125,22388122,3371659.75,10769230,17225444,8787353,722951,3493093.75,1403094.75,2994822,710275.375,6454983,4328527,722891,2311532,1539191,1186835.125,2213115.5,456953,2601945,3986982.75,3537977,2339234,5417598,4169431.5,1222949.25,3646053.75,593543.3125,1266387,1951856.25,609368,5535626,9097980,758322,1509775,1417728.25,2076289.625,849745.6875,2228083.75,4471029,2334260,1773880,892086,1392299.75,847419.375,3644982.25,18558162,1162007.625,1686765.875,1253745.5,967835,56762408,11061916,65047284,4217623,928409,1742870,9734582,2046914,4817062,9329202,690304,646994,8241967,3612830,925293.3125,13214416,2019672.875,2717863.25,706505.6875,1242666.25,2154119,4567879,1128634.875,1176615,747994.8125,2477499,876569.6875,2666634,627854.125,1525624,1245187.5,688145,1820814.75,2468322.5,1885930,36378588,826488.6875,4391348,2447861,4045980,688641.8125,977150,10055690,850568,609101,5669431,2160420,12086616,5401277.5,835093.1875,694189,1378155,2625396,438557,3766069,4777648,1248599,579888.3125,528411.875,3155496,1359579,1769137,1686262.25,5917000,1004253,2564004,163945984,84980760,595327,1327591,2551022.25,585077.125,868163,507947,1331031,864751.8125,2093011,3586479.25,2892779,2946756.25,724153,3603866,2998208.25,1134195.5,2213421.5,4180298,2828950.5,1034827.313,1552645,2168281,3168809,730765,27137190,16614404,1360340,3091348.25,12015936,1445538.625,652449,1245197.5,157606480,896403,2512443,1933811.625,474811,1533245.75,1369190,1180231,3988729,1032857,836348,3185509,4192895,747326,3835202,13962749,960550,873776,1989017,686019.1875,588965,1072500,3996541.5,1777554,7655751,5689125,4728488.5,2654804,4585070.5,591249,1227772,21346100,6481807.5,489557.0938,2926166,1712279.875,1815350,952635.375,8873485,2920918,4329966,18628834,739711,767642.8125,504656,715446,317581,577023,4983934,2982450,972608,10614234,576038.1875,905926.875,20657852,896976,1965529,544172.375,638201,866242,914023,1111858,41655852,2440980,3605755.5,1093743,1516377,10340282,1015060.188,1045640,2726269,12087498,566000.6875,284867168,33729336,39403320,5974993.5,11924933,2993610,766381,1608224,1056782.125,2347893,4405368,357915,633408,4634655,8204897,2271263,2536760.75,694479,1271021,1315298,1484807,846274,1348497.125,11320378,1840259,502254,666385,807185.5,6860521,1758581,2971013.5,1162226.375,876871,40183424,48479988,30083496,6649938,1435558.125,1259959,4369513.5,493969.6875,1696978.75,777915,9143764,1052244.75,758266,1109851,696840,1608884.25,10084950,1224554,507217,4855036.5,919266.125,631195,5375518,9341427,1998767,3730953.5,59107620,2751694,3473109,59757440,3714139,5424718,6344171,1427294.375,131373880,2520483.25,6012303,12931002,5040382,9620496,267135.0938,36436388,1380283,1240875,1240924.625,1133028.875,6205886,2503948,32552428,9393108,811870,2786648.75,3913410,1084048,6298131.5,523133,439266.9063,6981428,6668568.5,7574092,1872278.625,2757588,5170809.5,2640980.25,8381744,1750362.25,781890,2100813.5,7335905.5,63811044,2330242,24603420,609257,549834,3322250,1695593,1357116,756196,980418,493000,7325166,703847.375,1356874.75,4826430,2057288,1360847,5327249.5,697824,4039667,23262244,842923,713326.875,7067404,2577115.25,2512319,25909504,1647746,19061780,2125323.5,636266,4729846.5,1601955.625,0,913899,529072,3404992,522791.0938,1187540.25,1527917,1182575,926261.1875,1725336.25,1152639,6915293,739869.6875,4904803,1172785,1230757,1063269.125,3467070,971990.3125,576174.125,1929118,918177,18576900,1988784,9230223,1163748.875,1333016.625,47952868,2074152,2660396,1552051.25,2056814,781682.5,1070740,734495,1059424,8063959,2145603.5,2217046,625988.625,2188093,3220635.25,791640,37980712,912509,728876,53466684,763693,1027230.875,948676.5,654664,16373232,3518459.5,4335178,1581036.75,1241865,1140620,7459418,52960796,819003.3125,2161240.25,59124384,2083584,906103,792015,8687690,1441665.375,449955,557193,455798.9063,2196718,4519115,1313366,5459657,861972.875,1012341,6669954,1332591.5,1507270,3893073.5,3132453,789209.8125,561473.875,519573,2715249.75,1140137,1791071.375,12783255,340270,588856.125,1970956,6582088.5,11047668,2925780,1055033,839011.3125,1605522,624180,20828130,10394330,10716391,1026921,659652.625,1987073,1172903.375,609218,2595783.5,3534690.75,2171445,980773.375,14431245,13852986,2668769,859236.625,3276819,1242311.5,696522,19459304,595936.5,165472,2108012,1848878.5,1090032,1201781.125,6584966.5,8631444,1290284,8688676,2599171.75,6843855,2712983.75,5952875,1750099.75,2856934,666355,3050455.25,1881516.125,700899,15240688,1625308.625,2502081,8227025,11208498,5885427.5,908834,1866802.375,4590246.5,1976920,948657,763364.5,1097572,3468606,2827772.75,1959777,1834290,3347567,4253577.5,3085505,0,2254220,1718845,755856.875,8386493,1738376,7369591,1886007,3075425,3724098.25,2979413.5,6881405.5,473874.0938,455153,3300549.25,422133,7606084.5,831577,519806.0938,776427,3158743,793436,9974370,28229320,3574542.25,630040,1243282,27273538,864420.875,953059.875,430473.6875,1269847.75,2471623,1372706,3182301,2556131,1440554.25,849504,8976533,3133849.5,1302636,5549121,659265,13491170,8915523,1045838.5,1921801.25,1231546,1210229,45313836,2338567.75,2322314,2327611,1372834,983687,741999.6875,3590731,3635600,4376927,18074716,2108861.75,55073120,2105912.75,1456039,843520,4673827,620154.375,2248776,21042926,547084.375,936336,1688612.5,3735308.5,5328051.5,4759256,9546165,14856199,6345009.5,601813.125,10077532,19036416,472532,653366,1856230.625,784329.5,42566736,2495819,1083156.5,362823,378714.4063,957632,488181,867184.375,1800331,1692323,3778355,1983509,2337146,1187261,618670.875,2219190,1650279.25,6042399.5,8555790,45163308,1415494,1125724,5965171,7291418,887403,5851209,3162976,1605669,161311.0938,4367202,731092,3548858.25,0,1619177.125,478690.0938,641713,572139,876304,783651.5,1852926.625,11110405,28731502,609921.6875,4857581.5,586589.1875,629036,1586388.75,1675510.875,3371455,1270582.125,20361158,1633499,3704086.5,6858533.5,832985,4202335,1906384.875,606886,743528.125,16016644,4130053,13951571,1743198,2266983,1331122.125,35891484,1386983.375,6062770.5,1904480,2674034,764832.6875,3282191,662012,1406648.125,1024033,1612797,5662234,11521702,16246106,664539,1465535,24017770,610512,3022275,7273155.5,390726,25631512,650205.8125,1350190,1740568.25,924192.6875,1139980.625,2185193,471484.8125,15473117,1922953.25,7577723,563298,1119640.25,915286.1875,960790.8125,863703,1591528.5,1358519,8835104,862945.125,36681188,571384,673907.3125,2222097.5,1681424,3005926,10444297,2356607,1754477.25,901679,2331952.75,535649.875,1870069,13924967,1835804.875,1346952,1152368.25,885865.875,2799874,776248.6875,1615855,4952487,767720,2139713,11788719,15571642,9686734,656980,985236,1834570,8442782,935442,1748629.875,2352982.75,1832808.75,932083.5,1612965,2650112,5471735,1324965,8690179,827618,1079004,19490284,13720714,5571501.5,1457118,833747,17186624,4345698,1371061.25,942089,1100499.75,14361382,6578370,5987079,1011399,6579348,3017709.25,1360400.5,2313978.75,44293.5,5118939,1497496.5,2439061,4660015,9343428,57514116,8422861,13256662,2181833.5,1076733.25,987288.8125,692127.875,671004.625,5143676.5,1073156.375,982153.875,391058,8058913.5,2277316,1377184,454296,690259,4801472,5755308,823032.375,1416470.25,1460307,9274650,1261634.5,3532966.5,13223740,8890235,1870457.75,3361743,3459543,953432.1875,4476997,1494252,7657568,597210,866126.375,648022,3494383,1258978.375,1679127,842593,2771820.5,930611.875,991283,622051,3936421.5,1961982,1160989,672915,3678939,9683979,1381407,777696,318364.8125,2470642,11007235,1517410.5,2845946,497556,549438,2520310,660508,3736960,75073888,1166069.875,705621.6875,1417778,16865978,3652708.75,14856602,97690.39844,4028075.5,14064603,1113488,1387773,1388812,1392153,8355988.5,1528642,17695420,61762380,1305578,1814947,2671115,2303288,20781390,2473302,605971.1875,2115666,4407932,804002,7753080,524144.6875,2466100,3579008.75,2188051.25,788027,2558573,7802205.5,560823.625,8987012,984657,1100575,1459786,1078535.75,33270518,5475367,872555,770175.8125,872114.5,1963481.625,45252356,2802428,3073778,979442.3125,752441,4145920.5,1294615.25,1730886,41696464,1826203,3234787.75,1188444,1501767,1819329,1049088,10105379,16238985,8305255,978463,2549892,16467486,1095744,440658,532670,7945081,444458,1122837,2454812,1247972,1130097,701422,6235947,1128311.125,6424029,2960159.75],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Ridge() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predRidge"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["alphasridge = np.logspace(-3, 5, 1000)\n","param_gridRidge = {'ridge__alpha': alphasridge}\n","\n","GridRidge, \\\n","BestParametresRidge, \\\n","ScoresRidge, \\\n","SiteEnergyUse_predRidge, \\\n","figRidge = reg_modelGrid(model=Ridge(),\n","                            scaler=scaler,\n","                            X_train=BEBM_train,\n","                            X_test=BEBM_test,\n","                            y_train=SiteEnergyUse_train,\n","                            y_test=SiteEnergyUse_test,\n","                            y_test_name='SiteEnergyUse_test',\n","                            y_pred_name='SiteEnergyUse_predRidge',\n","                            score=score,\n","                            param_grid=param_gridRidge)\n","\n","print(BestParametresRidge)\n","ScoresRidge\n","figRidge.show()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[10313305.152038112,10313305.100387719,10313305.047776127,10313304.994185438,10313304.939597445,10313304.883993575,10313304.827354934,10313304.769662261,10313304.710895943,10313304.651035998,10313304.590062076,10313304.527953446,10313304.464688994,10313304.400247205,10313304.334606178,10313304.267743591,10313304.199636709,10313304.130262386,10313304.059597027,10313303.987616614,10313303.914296666,10313303.839612264,10313303.763538014,10313303.686048051,10313303.607116032,10313303.52671512,10313303.444817979,10313303.36139677,10313303.276423126,10313303.189868163,10313303.101702454,10313303.011896018,10313302.920418339,10313302.827238295,10313302.732324228,10313302.635643851,10313302.537164308,10313302.436852116,10313302.334673174,10313302.230592737,10313302.124575425,10313302.016585194,10313301.906585338,10313301.79453845,10313301.680406444,10313301.564150518,10313301.445731148,10313301.32510808,10313301.202240296,10313301.07708604,10313300.949602755,10313300.819747102,10313300.687474938,10313300.552741295,10313300.415500369,10313300.275705507,10313300.13330918,10313299.988262981,10313299.840517603,10313299.690022822,10313299.536727473,10313299.380579442,10313299.22152565,10313299.059512023,10313298.894483492,10313298.72638395,10313298.555156257,10313298.380742203,10313298.203082494,10313298.022116741,10313297.837783426,10313297.650019888,10313297.458762297,10313297.263945635,10313297.065503683,10313296.863368984,10313296.657472821,10313296.447745208,10313296.234114852,10313296.016509136,10313295.794854086,10313295.569074364,10313295.339093212,10313295.104832463,10313294.866212483,10313294.623152167,10313294.375568891,10313294.123378495,10313293.866495263,10313293.604831873,10313293.33829939,10313293.066807212,10313292.79026306,10313292.508572938,10313292.221641097,10313291.92937001,10313291.631660338,10313291.32841089,10313291.019518595,10313290.70487846,10313290.38438355,10313290.05792493,10313289.725391641,10313289.386670662,10313289.041646875,10313288.690203011,10313288.332219625,10313287.96757505,10313287.596145358,10313287.217804303,10313286.832423313,10313286.43987141,10313286.040015182,10313285.632718736,10313285.217843654,10313284.79524894,10313284.36479098,10313283.92632349,10313283.479697455,10313283.0247611,10313282.561359826,10313282.089336151,10313281.608529676,10313281.11877701,10313280.619911727,10313280.111764308,10313279.594162073,10313279.066929145,10313278.529886365,10313277.98285125,10313277.425637918,10313276.858057031,10313276.279915737,10313275.691017596,10313275.091162506,10313274.480146658,10313273.857762445,10313273.223798407,10313272.578039143,10313271.920265254,10313271.250253266,10313270.567775538,10313269.87260021,10313269.164491106,10313268.443207655,10313267.708504822,10313266.960133014,10313266.197837995,10313265.421360813,10313264.630437687,10313263.824799955,10313263.00417393,10313262.168280873,10313261.316836838,10313260.449552618,10313259.566133622,10313258.666279787,10313257.74968547,10313256.816039357,10313255.865024347,10313254.896317426,10313253.90958961,10313252.904505778,10313251.8807246,10313250.837898377,10313249.775672987,10313248.693687696,10313247.591575086,10313246.468960896,10313245.32546394,10313244.16069593,10313242.974261358,10313241.765757391,10313240.534773698,10313239.280892337,10313238.003687592,10313236.702725843,10313235.377565427,10313234.02775646,10313232.652840719,10313231.252351463,10313229.825813292,10313228.37274197,10313226.892644264,10313225.38501781,10313223.84935088,10313222.285122275,10313220.691801107,10313219.068846628,10313217.415708069,10313215.731824419,10313214.01662426,10313212.269525576,10313210.489935536,10313208.677250316,10313206.830854887,10313204.950122798,10313203.034415986,10313201.083084537,10313199.095466487,10313197.070887586,10313195.008661082,10313192.908087473,10313190.76845429,10313188.58903585,10313186.369093005,10313184.107872887,10313181.804608703,10313179.458519403,10313177.06880948,10313174.634668658,10313172.155271647,10313169.62977787,10313167.057331147,10313164.437059436,10313161.768074539,10313159.049471792,10313156.280329758,10313153.459709946,10313150.586656457,10313147.660195688,10313144.679336002,10313141.643067393,10313138.550361149,10313135.400169492,10313132.191425264,10313128.923041523,10313125.593911216,10313122.202906784,10313118.748879809,10313115.23066061,10313111.647057842,10313107.996858133,10313104.27882565,10313100.491701694,10313096.634204278,10313092.705027705,10313088.70284212,10313084.62629308,10313080.474001095,10313076.244561166,10313071.936542321,10313067.548487132,10313063.07891124,10313058.526302848,10313053.88912223,10313049.165801201,10313044.35474262,10313039.454319838,10313034.46287617,10313029.378724333,10313024.200145898,10313018.92539071,10313013.55267631,10313008.08018736,10313002.506075002,10312996.828456283,10312991.04541352,10312985.154993664,10312979.155207645,10312973.044029726,10312966.819396835,10312960.479207877,10312954.021323033,10312947.443563074,10312940.74370863,10312933.919499457,10312926.96863369,10312919.888767112,10312912.677512333,10312905.332438057,10312897.851068242,10312890.230881324,10312882.46930935,10312874.563737174,10312866.511501584,10312858.309890412,10312849.95614167,10312841.447442647,10312832.780928979,10312823.953683715,10312814.962736381,10312805.80506198,10312796.47758004,10312786.977153594,10312777.30058816,10312767.444630716,10312757.405968625,10312747.18122857,10312736.766975472,10312726.159711353,10312715.355874231,10312704.351836953,10312693.143906025,10312681.728320431,10312670.101250406,10312658.258796215,10312646.196986897,10312633.911778992,10312621.399055239,10312608.654623255,10312595.674214207,10312582.453481417,10312568.987999026,10312555.27326052,10312541.304677337,10312527.077577408,10312512.587203637,10312497.828712435,10312482.797172155,10312467.48756154,10312451.894768152,10312436.01358672,10312419.83871755,10312403.364764813,10312386.586234882,10312369.497534586,10312352.092969475,10312334.366742026,10312316.31294984,10312297.925583811,10312279.198526237,10312260.12554894,10312240.700311312,10312220.916358378,10312200.767118786,10312180.24590279,10312159.345900178,10312138.060178198,10312116.381679416,10312094.303219575,10312071.81748538,10312048.91703229,10312025.59428226,10312001.841521408,10311977.65089772,10311953.014418662,10311927.923948772,10311902.371207226,10311876.347765332,10311849.84504405,10311822.854311403,10311795.36667988,10311767.373103827,10311738.864376742,10311709.831128577,10311680.263822969,10311650.152754467,10311619.48804567,10311588.259644356,10311556.457320565,10311524.070663642,10311491.089079216,10311457.50178616,10311423.2978135,10311388.465997284,10311352.9949774,10311316.87319434,10311280.088885969,10311242.630084168,10311204.484611515,10311165.64007787,10311126.08387691,10311085.803182667,10311044.784945982,10311003.015890904,10310960.482511083,10310917.171066074,10310873.06757764,10310828.15782595,10310782.427345801,10310735.861422732,10310688.445089132,10310640.163120266,10310591.000030305,10310540.940068264,10310489.967213895,10310438.06517359,10310385.217376167,10310331.406968653,10310276.616812002,10310220.829476802,10310164.027238883,10310106.192074943,10310047.305658067,10309987.349353269,10309926.304212933,10309864.150972262,10309800.87004464,10309736.441516997,10309670.845145099,10309604.06034884,10309536.066207444,10309466.841454685,10309396.36447402,10309324.613293763,10309251.565582123,10309177.19864231,10309101.489407543,10309024.414436081,10308945.94990618,10308866.071611067,10308784.754953861,10308701.974942487,10308617.706184585,10308531.922882374,10308444.59882753,10308355.707396014,10308265.22154295,10308173.113797454,10308079.356257454,10307983.920584548,10307886.777998831,10307787.899273751,10307687.254730951,10307584.81423516,10307480.547189068,10307374.422528239,10307266.408716036,10307156.473738616,10307044.58509991,10306930.709816659,10306814.814413514,10306696.864918176,10306576.826856572,10306454.665248113,10306330.344601031,10306203.828907749,10306075.081640374,10305944.065746237,10305810.743643565,10305675.077217245,10305537.027814636,10305396.556241622,10305253.62275865,10305108.18707701,10304960.208355183,10304809.645195391,10304656.455640268,10304500.597169733,10304342.026698006,10304180.700570868,10304016.57456305,10303849.603875903,10303679.743135253,10303506.946389493,10303331.167107947,10303152.358179461,10302970.471911306,10302785.46002833,10302597.273672435,10302405.863402372,10302211.179193864,10302013.170440054,10301811.78595235,10301606.97396163,10301398.682119846,10301186.85750203,10300971.446608748,10300752.395369012,10300529.649143616,10300303.15272901,10300072.850361647,10299838.685722882,10299600.601944398,10299358.541614208,10299112.44678327,10298862.25897268,10298607.919181544,10298349.367895473,10298086.54509578,10297819.390269412,10297547.842419557,10297271.840077091,10296991.321312735,10296706.223750087,10296416.484579468,10296122.040572632,10295822.828098401,10295518.783139203,10295209.841308575,10294895.93786968,10294577.007754758,10294252.985585753,10293923.805695903,10293589.402152497,10293249.708780777,10292904.659189006,10292554.186794728,10292198.224852292,10291836.706481658,10291469.564698476,10291096.732445542,10290718.142625619,10290333.72813566,10289943.42190249,10289547.156919956,10289144.866287587,10288736.483250814,10288321.941242773,10287901.173927704,10287474.115246013,10287040.699461007,10286600.861207362,10286154.535541315,10285701.657992631,10285242.164618434,10284775.992058795,10284303.077594254,10283823.359205209,10283336.775633255,10282843.266444448,10282342.77209457,10281835.233996382,10281320.594588913,10280798.797408782,10280269.78716359,10279733.509807399,10279189.91261832,10278638.944278155,10278080.554954246,10277514.696383378,10276941.321957877,10276360.386813793,10275771.84792127,10275175.664177027,10274571.796498965,10273960.207922926,10273340.863701526,10272713.731405104,10272078.78102472,10271435.985077236,10270785.31871237,10270126.759821769,10269460.289150005,10268785.890407499,10268103.550385281,10267413.259071534,10266715.00976994,10266008.79921965,10265294.627716871,10264572.499238016,10263842.421564285,10263104.406407598,10262358.46953782,10261604.630911138,10260842.914799506,10260073.349921022,10259295.969571149,10258510.811754614,10257717.91931786,10256917.340081949,10256109.126975674,10255293.33816883,10254470.037205407,10253639.29313653,10252801.180653043,10251955.780217428,10251103.17819499,10250243.466984015,10249376.745144721,10248503.11752681,10247622.695395354,10246735.596554827,10245841.945471007,10244941.873390522,10244035.518457828,10243123.025829287,10242204.547784144,10241280.243832128,10240350.280817378,10239414.833018487,10238474.082244268,10237528.217925113,10236577.437199518,10235621.944995608,10234661.954107273,10233697.685264701,10232729.367198996,10231757.236700576,10230781.538671106,10229802.526168633,10228820.46044571,10227835.610980142,10226848.255498204,10225858.67998995,10224867.178716442,10223874.054208603,10222879.61725748,10221884.186895685,10220888.0903698,10219891.663103523,10218895.248651419,10217899.198643018,10216903.872717198,10215909.638446663,10214916.87125236,10213925.954307858,10212937.278433438,10211951.241980024,10210968.250702733,10209988.717624238,10209013.062887777,10208041.713599976,10207075.103663517,10206113.673599752,10205157.870361416,10204208.147135569,10203264.96313702,10202328.783392403,10201400.078515194,10200479.324471993,10199567.002340343,10198663.598058501,10197769.60216754,10196885.509546231,10196011.819139149,10195149.03367856,10194297.659400571,10193458.205756165,10192631.185117725,10191817.112481667,10191016.505167883,10190229.882516697,10189457.765584104,10188700.676835973,10187959.13984215,10187233.678971168,10186524.81908648,10185833.085245075,10185159.00239937,10184503.095103312,10183865.887223614,10183247.90165706,10182649.660054892,10182071.682555197,10181514.4875243,10180978.59130817,10180464.507994786,10179972.749188459,10179503.823797146,10179058.237833625,10178636.49423161,10178239.09267767,10177866.529459897,10177519.297334323,10177197.885409798,10176902.779052382,10176634.459809933,10176393.405357746,10176180.08946602,10175994.98198981,10175838.548882168,10175711.252231112,10175613.550320964,10175545.89771861,10175508.745385107,10175502.540813107,10175527.728190353,10175584.748589657,10175674.040185448,10175796.03849712,10175951.17665929,10176139.88571885,10176362.594958939,10176619.732249528,10176911.72442456,10177238.997685248,10177601.978029286,10178001.09170546,10178436.765693216,10178909.428206585,10179419.509221828,10179967.441028073,10180553.65880017,10181178.601192864,10181842.710955415,10182546.43556561,10183290.22788212,10184074.546814125,10184899.858006932,10185766.634542398,10186675.35765287,10187626.517447233,10188620.613647701,10189658.156335888,10190739.666706678,10191865.677828308,10193036.735407162,10194253.398555622,10195516.24056133,10196825.849656248,10198182.829783756,10199587.801362164,10201041.402042836,10202544.287461253,10204097.131979223,10205700.629416544,10207355.493770313,10209062.459920187,10210822.28431782,10212635.745658766,10214503.645535123,10216426.809067229,10218406.085512709,10220442.348851241,10222536.498343397,10224689.459061956,10226902.182394143,10229175.646513224,10231510.856818,10233908.846338734,10236370.676108096,10238897.435495773,10241490.24250542,10244150.244032715,10246878.616083285,10249676.563949348,10252545.32234401,10255486.155492157,10258500.357176958,10261589.25074112,10264754.189042013,10267996.554359872,10271317.758258458,10274719.241397452,10278202.473296046,10281768.952047288,10285420.203982733,10289157.783287022,10292983.27156224,10296898.277341772,10300904.435553607,10305003.406933058,10309196.877384987,10313486.557295635,10317874.18079434,10322361.504965398,10326950.309010554,10331642.393362503,10336439.57875005,10341343.705215545,10346356.631085366,10351480.231894236,10356716.399264337,10362067.03974017,10367534.073580291,10373119.433507066,10378825.0634157,10384652.917043904,10390604.956603568,10396683.151376002,10402889.476272281,10409225.910360366,10415694.435360756,10422297.03411245,10429035.689011129,10435912.380421499,10442929.085065847,10450087.774390854,10457390.412914844,10464838.95655771,10472435.350955732,10480181.52976367,10488079.412946457,10496130.905062955,10504337.893544234,10512702.246968856,10521225.813337747,10529910.418351192,10538757.863690596,10547769.925307585,10556948.351723142,10566294.862339403,10575811.145766769,10585498.85816907,10595359.621629335,10605395.022538949,10615606.61001279,10625995.894332986,10636564.345423918,10647313.391361076,10658244.416916309,10669358.762141995,10680657.72099665,10692142.540014375,10703814.417020548,10715674.499896133,10727723.885392796,10739963.618001144,10752394.688874159,10765018.034807894,10777834.537281472,10790845.021558255,10804050.25584999,10817450.950545704,10831047.75750695,10844841.269430913,10858832.019282866,10873020.479799243,10887407.063062552,10901992.120149225,10916775.94085137,10931758.7534733,10946940.724703547,10962321.959562939,10977902.501429264,10993682.332138818,11009661.372165034,11025839.48087426,11042216.456858631,11058792.03834574,11075565.903684828,11092537.671908941,11109706.903372336,11127073.100462463,11144635.708385399,11162394.116023775,11180347.656865835,11198495.610004256,11216837.201203175,11235371.604031712,11254097.941062097,11273015.285130464,11292122.660658065,11311419.045030702,11330903.37003387,11350574.523341045,11370431.35005242,11390472.654281192,11410697.20078445,11431103.716635522,11451690.892934551,11472457.386553936,11493401.821915155,11514522.79279339,11535818.86414623,11557288.573962731,11578930.435128823,11600742.9373052,11622724.548813576,11644873.718527148,11667188.877761144,11689668.442159109,11712310.81357063,11735114.381916199,11758077.5270347,11781198.620509226,11804476.027466666,11827908.108346663,11851493.220635498,11875229.720560377,11899115.964739818,11923150.311785594,11947331.123851985,11971656.768127982,11996125.618268175,12020736.05575822,12045486.471210707,12070375.265587488,12095400.85134459,12120561.653495852,12145856.110591806,12171282.675610134,12196839.816754485,12222526.018158445,12248339.78049168,12274279.621465433,12300344.076234844,12326531.697695691,12352841.056673398,12379270.742002541,12405819.360495038,12432485.536795873,12459267.913125113,12486165.14890553,12513175.92027538,12540298.919486057,12567532.854184961,12594876.446583962,12622328.432514397,12649887.560369754,12677552.589937693,12705322.291123262,12733195.442565713,12761170.830151554,12789247.24542696,12817423.483913012,12845698.34332757,12874070.621718088,12902539.115509907,12931102.617475117,12959759.914627235,12988509.786047544,13017351.000649055,13046282.314884583,13075302.470405694,13104410.191679474,13133604.183570573,13162883.128896065,13192245.685961012,13221690.486082835,13251216.131112749,13280821.190962732,13310504.201146634,13340263.660344113,13370098.027996253,13400005.72194159,13429985.116101494,13460034.538223648,13490152.267692406,13520336.533414718,13550585.511790037,13580897.324772637,13611270.03803442,13641701.659236055,13672190.13641413,13702733.356491398,13733329.143917209,13763975.259444445,13794669.399049204,13825409.192998702,13856192.205072615,13887015.931942519,13917877.802713424,13948775.178631013,13979705.352957582,14010665.551019019,14041652.930424612,14072664.581460949,14103697.527660366,14134748.726544037,14165815.070538932,14196893.388067499,14227980.44480806,14259072.945123663,14290167.533656191,14321260.797082197,14352349.266026368,14383429.417127859,14414497.675254488,14445550.415859008,14476583.967471492,14507594.614321297,14538578.599081755,14569532.125730377,14600451.362517023,14631332.445032224,14662171.479367618,14692964.545360183,14723707.699911918,14754396.980376255,14785028.408002665,14815597.991430601,14846101.730224054,14876535.618437976,14906895.648207735,14937177.813353062,14967378.112987798,14997492.555127084,15027517.160283703,15057447.965045432,15087281.025625508,15117012.421378653,15146638.25827505,15176154.672325347,15205557.832949674,15234843.94628422,15264009.258419048,15293050.05856137,15321962.6821186,15350743.513696056,15379388.9900044,15407895.602672335,15436259.90096042,15464478.494372254,15492548.055159565,15520465.320718199,15548227.095872322,15575830.255044485,15603271.744309604,15630548.583331188,15657657.86717854,15684596.768023992,15711362.536719423,15737952.504251923,15764364.08307836,15790594.768339222,15816642.138952266,15842503.858586714,15868177.676519144,15893661.428372245,15918953.036738068,15944050.511687428,15968951.95116746,15993655.54128938,16018159.556508785,16042462.359700978,16066562.402133804,16090458.223340934]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[13782750.170729663,13782750.174662866,13782750.17866929,13782750.182750277,13782750.18690722,13782750.191141544,13782750.195454674,13782750.19984809,13782750.204323279,13782750.208881764,13782750.213525098,13782750.218254859,13782750.22307266,13782750.227980128,13782750.23297895,13782750.238070812,13782750.243257448,13782750.248540636,13782750.253922155,13782750.259403853,13782750.264987577,13782750.27067525,13782750.276468778,13782750.28237015,13782750.288381375,13782750.294504497,13782750.300741587,13782750.307094784,13782750.31356624,13782750.32015815,13782750.326872777,13782750.333712388,13782750.340679325,13782750.347775938,13782750.355004666,13782750.362367947,13782750.3698683,13782750.377508271,13782750.385290464,13782750.393217526,13782750.401292149,13782750.40951709,13782750.417895146,13782750.426429158,13782750.435122043,13782750.443976758,13782750.452996304,13782750.462183766,13782750.47154226,13782750.481074985,13782750.490785167,13782750.50067612,13782750.510751214,13782750.521013869,13782750.531467589,13782750.542115925,13782750.552962504,13782750.564011019,13782750.575265227,13782750.586728966,13782750.598406132,13782750.610300707,13782750.622416733,13782750.634758336,13782750.647329723,13782750.660135169,13782750.673179034,13782750.686465768,13782750.699999876,13782750.713785984,13782750.727828784,13782750.742133051,13782750.756703664,13782750.77154557,13782750.786663836,13782750.80206361,13782750.817750132,13782750.833728746,13782750.850004895,13782750.866584128,13782750.883472074,13782750.90067451,13782750.918197278,13782750.93604635,13782750.954227809,13782750.97274785,13782750.991612779,13782751.01082902,13782751.030403126,13782751.050341766,13782751.070651734,13782751.091339948,13782751.112413451,13782751.133879436,13782751.155745212,13782751.178018231,13782751.200706081,13782751.223816503,13782751.24735737,13782751.271336699,13782751.295762673,13782751.320643619,13782751.345988005,13782751.371804487,13782751.39810186,13782751.424889091,13782751.452175315,13782751.479969839,13782751.508282136,13782751.537121853,13782751.566498838,13782751.596423108,13782751.626904871,13782751.657954523,13782751.68958265,13782751.72180005,13782751.75461771,13782751.78804683,13782751.822098807,13782751.856785271,13782751.892118057,13782751.928109214,13782751.964771036,13782752.002116038,13782752.040156955,13782752.078906788,13782752.118378753,13782752.158586334,13782752.19954326,13782752.241263513,13782752.283761343,13782752.32705126,13782752.371148048,13782752.416066788,13782752.461822806,13782752.508431742,13782752.555909514,13782752.604272356,13782752.65353679,13782752.703719646,13782752.75483809,13782752.80690959,13782752.859951949,13782752.913983297,13782752.96902211,13782753.025087219,13782753.08219779,13782753.140373357,13782753.19963383,13782753.25999947,13782753.321490953,13782753.384129295,13782753.447935957,13782753.512932759,13782753.579141969,13782753.646586243,13782753.715288678,13782753.785272803,13782753.85656259,13782753.92918246,13782754.003157282,13782754.078512412,13782754.155273668,13782754.233467374,13782754.313120319,13782754.394259814,13782754.476913687,13782754.561110288,13782754.646878475,13782754.734247696,13782754.823247923,13782754.91390969,13782755.00626413,13782755.100342928,13782755.196178406,13782755.293803466,13782755.393251637,13782755.494557096,13782755.597754642,13782755.702879746,13782755.809968537,13782755.919057848,13782756.030185184,13782756.143388763,13782756.258707546,13782756.376181196,13782756.495850153,13782756.617755622,13782756.741939563,13782756.868444765,13782756.997314798,13782757.128594067,13782757.262327826,13782757.39856217,13782757.537344083,13782757.678721437,13782757.822743008,13782757.969458502,13782758.118918555,13782758.271174792,13782758.426279789,13782758.58428714,13782758.745251454,13782758.909228371,13782759.076274604,13782759.246447928,13782759.419807225,13782759.596412512,13782759.776324932,13782759.9596068,13782760.146321625,13782760.336534103,13782760.53031021,13782760.727717146,13782760.928823398,13782761.133698773,13782761.342414424,13782761.55504283,13782761.771657892,13782761.992334915,13782762.217150636,13782762.446183275,13782762.679512557,13782762.917219728,13782763.159387585,13782763.406100545,13782763.657444615,13782763.913507488,13782764.174378514,13782764.440148786,13782764.710911151,13782764.986760229,13782765.267792488,13782765.55410624,13782765.845801711,13782766.142981049,13782766.445748394,13782766.754209891,13782767.068473747,13782767.388650265,13782767.714851882,13782768.047193235,13782768.385791156,13782768.730764793,13782769.082235564,13782769.44032729,13782769.805166177,13782770.176880918,13782770.555602692,13782770.941465259,13782771.334604982,13782771.735160904,13782772.143274771,13782772.55909113,13782772.982757356,13782773.414423704,13782773.854243398,13782774.302372674,13782774.758970838,13782775.224200336,13782775.698226817,13782776.181219209,13782776.67334978,13782777.174794184,13782777.685731577,13782778.206344664,13782778.736819772,13782779.277346918,13782779.828119928,13782780.389336457,13782780.961198116,13782781.543910528,13782782.137683444,13782782.742730793,13782783.359270785,13782783.987526031,13782784.62772358,13782785.28009506,13782785.944876753,13782786.622309716,13782787.312639855,13782788.016118053,13782788.73300026,13782789.463547615,13782790.208026558,13782790.966708928,13782791.739872105,13782792.527799113,13782793.33077875,13782794.149105709,13782794.983080724,13782795.833010677,13782796.699208751,13782797.581994563,13782798.481694316,13782799.398640925,13782800.333174178,13782801.285640901,13782802.256395096,13782803.245798105,13782804.25421879,13782805.282033682,13782806.32962716,13782807.397391645,13782808.48572775,13782809.595044496,13782810.725759495,13782811.878299123,13782813.05309877,13782814.250602994,13782815.471265765,13782816.715550676,13782817.983931145,13782819.27689068,13782820.594923083,13782821.938532704,13782823.30823468,13782824.70455519,13782826.128031721,13782827.579213329,13782829.058660906,13782830.566947475,13782832.104658471,13782833.672392022,13782835.270759279,13782836.90038471,13782838.561906433,13782840.255976511,13782841.983261349,13782843.744441979,13782845.540214447,13782847.371290192,13782849.238396363,13782851.142276306,13782853.083689835,13782855.063413749,13782857.082242174,13782859.140987054,13782861.240478536,13782863.381565455,13782865.56511581,13782867.792017218,13782870.06317741,13782872.379524762,13782874.742008787,13782877.1516007,13782879.609293928,13782882.116104726,13782884.673072716,13782887.281261511,13782889.941759324,13782892.655679598,13782895.424161674,13782898.248371433,13782901.12950201,13782904.068774492,13782907.06743865,13782910.126773695,13782913.248089053,13782916.432725143,13782919.68205421,13782922.997481177,13782926.38044448,13782929.832416987,13782933.354906924,13782936.949458793,13782940.617654357,13782944.361113649,13782948.181496006,13782952.080501106,13782956.059870098,13782960.121386696,13782964.266878372,13782968.498217508,13782972.817322664,13782977.226159839,13782981.726743745,13782986.321139181,13782991.011462422,13782995.799882619,13783000.688623283,13783005.679963795,13783010.776240963,13783015.979850644,13783021.293249361,13783026.718956048,13783032.259553771,13783037.91769158,13783043.696086325,13783049.597524608,13783055.62486475,13783061.781038834,13783068.069054812,13783074.49199866,13783081.053036613,13783087.75541747,13783094.60247496,13783101.597630188,13783108.744394144,13783116.0463703,13783123.507257301,13783131.130851695,13783138.921050785,13783146.881855551,13783155.017373675,13783163.331822645,13783171.829532966,13783180.514951432,13783189.392644573,13783198.467302145,13783207.743740724,13783217.226907471,13783226.921883933,13783236.83389003,13783246.968288124,13783257.330587203,13783267.926447254,13783278.761683684,13783289.84227194,13783301.17435226,13783312.764234534,13783324.618403345,13783336.743523167,13783349.1464437,13783361.834205374,13783374.81404502,13783388.09340173,13783401.679922862,13783415.581470251,13783429.806126598,13783444.362202039,13783459.258240964,13783474.503028952,13783490.105600003,13783506.075243916,13783522.421513945,13783539.154234631,13783556.283509916,13783573.819731433,13783591.77358714,13783610.15607008,13783628.97848754,13783648.252470331,13783667.989982476,13783688.203331098,13783708.905176599,13783730.108543178,13783751.826829623,13783774.07382041,13783796.86369713,13783820.211050235,13783844.130891137,13783868.63866464,13783893.750261713,13783919.48203262,13783945.850800468,13783972.873875037,13784000.569067108,13784028.954703085,13784058.049640097,13784087.873281455,13784118.445592579,13784149.787117299,13784181.91899468,13784214.862976173,13784248.641443353,13784283.277426051,13784318.79462095,13784355.217410732,13784392.570883656,13784430.880853657,13784470.173881004,13784510.477293387,13784551.819207627,13784594.228551876,13784637.735088352,13784682.369436707,13784728.163097834,13784775.148478374,13784823.358915715,13784872.828703616,13784923.593118468,13784975.688445995,13785029.152008802,13785084.02219433,13785140.338483542,13785198.1414802,13785257.472940793,13785318.375805046,13785380.894227134,13785445.07360747,13785510.960625226,13785578.603271373,13785648.050882472,13785719.354175068,13785792.565280722,13785867.737781676,13785944.926747188,13786024.188770466,13786105.582006246,13786189.166209005,13786275.00277175,13786363.154765438,13786453.686978988,13786546.665959852,13786642.160055177,13786740.239453543,13786840.976227146,13786944.444374593,13787050.719864134,13787159.880677396,13787272.00685355,13787387.180533897,13787505.486006856,13787627.009753292,13787751.840492187,13787880.069226587,13788011.789289827,13788147.096391933,13788286.088666167,13788428.866715776,13788575.5336607,13788726.195184391,13788880.959580489,13789039.93779951,13789203.24349525,13789370.993071057,13789543.305725755,13789720.30349918,13789902.111317333,13790088.8570369,13790280.671489276,13790477.68852381,13790680.04505027,13790887.881080458,13791101.339768799,13791320.567451896,13791545.713686803,13791776.931288123,13792014.376363587,13792258.208348136,13792508.590036366,13792765.687613195,13793029.67068257,13793300.71229424,13793578.98896824,13793864.68071718,13794157.971065976,13794459.04706907,13794768.099324869,13795085.321987282,13795410.912774255,13795745.072973046,13796088.007442208,13796439.924610015,13796801.03646921,13797171.558567988,13797551.709996868,13797941.713371523,13798341.794811212,13798752.183912778,13799173.11371995,13799604.820687916,13800047.544642894,13800501.528736576,13800967.019395329,13801444.266264008,13801933.52214413,13802435.042926429,13802949.08751752,13803475.917760607,13804015.798350148,13804568.996740218,13805135.78304666,13805716.429942755,13806311.21254842,13806920.408312818,13807544.296890285,13808183.160009613,13808837.281336496,13809506.94632927,13810192.442087745,13810894.05719537,13811612.08155443,13812346.806214627,13813098.52319483,13813867.525298223,13814654.10592084,13815458.558853626,13816281.178078134,13817122.257556014,13817982.09101241,13818860.97171353,13819759.192238446,13820677.044245541,13821614.818233715,13822572.803298615,13823551.28688433,13824550.554530649,13825570.889616456,13826612.5730994,13827675.883252464,13828761.095397599,13829868.48163708,13830998.310582886,13832150.847084671,13833326.351956781,13834525.081704855,13835747.2882526,13836993.218669195,13838263.114898022,13839557.213487273,13840875.745323043,13842218.935365556,13843587.002389178,13844980.158726867,13846398.610019732,13847842.554972427,13849312.18511499,13850807.684571862,13852329.22983886,13853876.989568684,13855451.124365754,13857051.786591044,13858679.12017766,13860333.26045775,13862014.334001618,13863722.458469523,13865457.742476992,13867220.285474185,13869010.177640032,13870827.499791738,13872672.323310211,13874544.710082062,13876444.712458692,13878372.373232989,13880327.725634117,13882310.793340929,13884321.590514367,13886360.121849252,13888426.382645912,13890520.358901842,13892642.02742381,13894791.355960581,13896968.303356448,13899172.819725871,13901404.846649129,13903664.317389281,13905951.157130316,13908265.28323656,13910606.605533274,13912975.026608342,13915370.442134865,13917792.741214577,13920241.806741763,13922717.51578745,13925219.740003552,13927748.346046645,13930303.196020916,13932884.147939969,13935491.056206925,13938123.772112366,13940782.144349664,13943466.01954696,13946175.24281545,13948909.658313181,13951669.109823842,13954453.44134985,13957262.497719085,13960096.125204619,13962954.172156727,13965836.489646476,13968742.93212023,13971673.358064359,13974627.63067941,13977605.618563104,13980607.196401438,13983632.245667215,13986680.655325323,13989752.322544169,13992847.153412571,13995965.063661505,13999105.97939019,14002269.837795885,14005456.587906849,14008666.19131808,14011898.622929243,14015153.871684428,14018431.941313345,14021732.851073604,14025056.636493742,14028403.350116812,14031773.062244192,14035165.861679615,14038581.856473103,14042021.1746649,14045483.965029232,14048970.39781805,14052480.665504707,14056014.983527783,14059573.591035165,14063156.75162862,14066764.754109174,14070397.913223516,14074056.570411917,14077741.09455793,14081451.882740466,14085189.360988647,14088953.985039998,14092746.241102604,14096566.646621764,14100415.751051864,14104294.136634141,14108202.419181004,14112141.248867715,14116111.31103216,14120113.326983511,14124148.054820543,14128216.290260468,14132318.867479103,14136456.659963135,14140630.581375418,14144841.58643403,14149090.671805928,14153378.877016034,14157707.28537251,14162077.024908926,14166489.269344121,14170945.239060411,14175446.202100735,14179993.475185405,14184588.424748974,14189232.467997624,14193927.073987586,14198673.764724843,14203474.116286354,14208329.759962995,14213242.383424195,14218213.731904307,14223245.60941041,14228339.879951352,14233498.468787547,14238723.363700971,14244016.616284642,14249380.343250692,14254816.727755992,14260328.020744134,14265916.542302327,14271584.683031648,14277334.905428864,14283169.745277772,14289091.813047875,14295103.795297964,14301208.456081852,14307408.63835344,14313707.265367828,14320107.342075113,14326611.956503177,14333224.281125428,14339947.574209334,14346785.181141105,14353740.535721822,14360817.161429798,14368018.672643792,14375348.775821337,14382811.27062619,14390410.05099852,14398149.106161252,14406032.521555582,14414064.479698448,14422249.260954399,14430591.244214013,14439094.9074708,14447764.828288104,14456605.684147397,14465622.252669074,14474819.411696477,14484202.139233913,14493775.513229005,14503544.711189618,14513515.009625517,14523691.783304665,14534080.504314097,14544686.740915151,14555516.156182922,14566574.506419735,14577867.639332544,14589401.491964351,14601182.088369725,14613215.537025,14625508.02796375,14638065.829628665,14650895.285431303,14664002.81001174,14677394.88519055,14691078.055606406,14705058.9240331,14719344.146370696,14733940.426306369,14748854.509641424,14764093.178282054,14779663.24389247,14795571.541210301,14811824.921025436,14828430.242824743,14845394.36710667,14862724.147371145,14880426.421791814,14898508.004579207,14916975.67704518,14935836.178380685,14955096.196160583,14974762.356591148,14994841.21451756,15015339.243210502,15036262.823952861,15057618.235449076,15079411.643081587,15101649.088040346,15124336.47635299,15147479.567844741,15171083.965058545,15195155.10216707,15219698.233909579,15244718.424587335,15270220.53715239,15296209.22242495,15322688.9084751,15349663.790204996,15377137.81916751,15405114.693657322,15433597.849109981,15462590.448843906,15492095.375179397,15522115.220967796,15552652.281562578,15583708.54726269,15615285.696256857,15647385.088095559,15680007.757715426,15713154.410038598,15746825.41516707,15781020.804189768,15815740.265617218,15850983.14245619,15886748.429933798,15923034.773877641,15959840.469755972,15997163.462378675,16035001.346257277,16073351.366619313,16112210.421069551,16151575.061888212,16191441.498953376,16231805.603272736,16272662.911107346,16314008.628667913,16355837.637362354,16398144.499571323,16440923.464927033,16484168.477069132,16527873.180850213,16572030.929962602,16616634.794957004,16661677.571623243,16707151.789702669,16753049.721901627,16799363.39317532,16846084.590251386,16893204.871362954,16940715.576160993,16988607.83577685,17036872.583005875,17085500.562584437,17134482.341533072,17183808.319539666,17233468.73935781,17283453.697196066,17333753.153075702,17384356.941135176,17435254.779861182,17486436.28222717,17537890.965721756,17589608.26225042,17641577.52789544,17693788.052520204,17746229.069205157,17798889.763504066,17851759.2825103,17904826.74372404,17958081.243712507,18011511.866556197,18065107.692075305,18118857.803831324,18172751.296899855,18226777.28541144,18280924.909858115,18335183.344164018,18389541.80251924,18443989.545976676,18498515.888812236,18553110.204649426,18607761.932349678,18662460.581670403,18717195.73869309,18771957.071024127,18826734.332771663,18881517.36930142,18936296.121775612,18991060.63147848,19045801.043932706,19100507.61281097,19155170.70364694,19209780.797350384,19264328.493530944,19318804.51363525,19373199.703902394,19427505.03814213,19481711.620341145,19535810.687101834,19589793.60991867,19643651.89729687,19697377.196718164,19750961.296458434,19804396.127261907,19857673.763876434,19910786.426454633,19963726.481825136,20016486.444638435,20069058.978391737,20121436.896336794,20173613.162275095,20225580.891244184,20277333.350099217,20328863.95799328,20380166.28676046,20431234.061204974,20482061.159299865,20532641.612298667,20582969.604763072,20633039.47450992,20682845.712480213,20732382.96253318,20781646.021168023,20830629.83717602,20879329.511225417,20927740.295381535,20975857.592564307,21023676.955945514,21071194.088287625,21118404.841226377,21165305.214498773,21211891.35511846,21258159.556499984,21304106.25753362,21349728.04161234,21395021.63561219,21439983.90882767,21484611.871863216,21528902.675482143,21572853.609414306,21616462.10112344,21659725.714535415,21702642.148728482,21745209.236586377,21787424.943415496,21829287.365526963,21870794.72878458,21911945.387119576,21952737.821013168,21993170.635947727,22033242.56082756,22072952.446370177,22112299.263468932,22151282.101528037,22189900.16677083,22228152.780522246,22266039.377466418,22303559.503880452,22340712.815845303,22377499.07743478,22413918.15888368,22449970.03473614,22485654.781975266,22520972.578134995,22555923.699395563,22590508.518663432,22624727.503636945,22658581.214858882,22692070.303757045,22725195.510674126,22757957.662887983,22790357.67262363,22822396.53505813,22854075.32631966,22885395.201481976,22916357.39255553,22946963.20647659,22977214.02309542,23007111.293165103]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"y":[6843860.133346561,6843860.026112571,6843859.916882964,6843859.8056206005,6843859.692287668,6843859.576845607,6843859.459255194,6843859.339476433,6843859.217468606,6843859.093190232,6843858.966599055,6843858.837652033,6843858.706305329,6843858.572514283,6843858.436233407,6843858.29741637,6843858.15601597,6843858.011984135,6843857.865271898,6843857.715829374,6843857.563605756,6843857.40854928,6843857.25060725,6843857.089725953,6843856.925850689,6843856.758925744,6843856.588894371,6843856.415698757,6843856.239280013,6843856.059578177,6843855.87653213,6843855.690079648,6843855.500157352,6843855.306700652,6843855.109643789,6843854.908919755,6843854.704460316,6843854.496195961,6843854.284055883,6843854.0679679485,6843853.847858701,6843853.623653297,6843853.3952755295,6843853.162647741,6843852.925690845,6843852.684324278,6843852.438465992,6843852.188032393,6843851.932938332,6843851.673097096,6843851.408420343,6843851.138818083,6843850.864198662,6843850.5844687205,6843850.299533149,6843850.009295089,6843849.713655855,6843849.412514944,6843849.105769979,6843848.793316679,6843848.475048814,6843848.150858177,6843847.820634568,6843847.484265709,6843847.14163726,6843846.7926327335,6843846.43713348,6843846.075018639,6843845.706165113,6843845.330447499,6843844.947738068,6843844.5579067245,6843844.160820931,6843843.7563457005,6843843.34434353,6843842.924674358,6843842.49719551,6843842.061761669,6843841.618224809,6843841.166434145,6843840.706236098,6843840.237474217,6843839.759989146,6843839.273618575,6843838.778197158,6843838.273556484,6843837.759525003,6843837.235927969,6843836.702587401,6843836.159321978,6843835.605947046,6843835.042274477,6843834.46811267,6843833.88326644,6843833.287536982,6843832.680721789,6843832.0626145955,6843831.433005277,6843830.79167982,6843830.138420222,6843829.473004427,6843828.795206242,6843828.104795276,6843827.401536838,6843826.68519189,6843825.955516931,6843825.212263934,6843824.455180263,6843823.68400858,6843822.898486754,6843822.098347787,6843821.283319712,6843820.453125492,6843819.607482949,6843818.746104658,6843817.868697831,6843816.974964251,6843816.064600151,6843815.137296103,6843814.192736927,6843813.230601596,6843812.250563089,6843811.252288315,6843810.235437984,6843809.199666498,6843808.144621829,6843807.069945393,6843805.975271956,6843804.860229471,6843803.724438988,6843802.567514492,6843801.389062803,6843800.188683426,6843798.965968404,6843797.720502207,6843796.451861573,6843795.159615374,6843793.843324458,6843792.502541498,6843791.136810861,6843789.745668441,6843788.328641487,6843786.885248473,6843785.414998916,6843783.917393199,6843782.3919224255,6843780.838068239,6843779.255302634,6843777.643087797,6843776.000875903,6843774.328108957,6843772.624218566,6843770.88862579,6843769.120740916,6843767.319963267,6843765.485681002,6843763.617270896,6843761.714098138,6843759.775516124,6843757.800866233,6843755.78947757,6843753.740666807,6843751.653737887,6843749.527981825,6843747.362676436,6843745.15708616,6843742.910461705,6843740.622039884,6843738.291043317,6843735.916680183,6843733.498143936,6843731.034613026,6843728.525250653,6843725.969204469,6843723.365606268,6843720.713571718,6843718.012200048,6843715.260573758,6843712.457758279,6843709.602801691,6843706.694734389,6843703.732568735,6843700.715298755,6843697.641899764,6843694.511328075,6843691.322520565,6843688.074394397,6843684.765846591,6843681.395753693,6843677.9629713725,6843674.466334039,6843670.904654455,6843667.276723327,6843663.581308902,6843659.81715655,6843655.982988337,6843652.077502588,6843648.099373471,6843644.047250519,6843639.919758183,6843635.715495383,6843631.4330350235,6843627.070923492,6843622.62768021,6843618.1017970955,6843613.491738081,6843608.795938548,6843604.012804894,6843599.140713874,6843594.178012159,6843589.123015691,6843583.974009192,6843578.729245529,6843573.386945147,6843567.945295474,6843562.402450304,6843556.756529161,6843551.005616687,6843545.147762001,6843539.180977998,6843533.10324074,6843526.912488728,6843520.60662223,6843514.183502571,6843507.640951399,6843500.976749983,6843494.18863843,6843487.274314944,6843480.231435054,6843473.057610833,6843465.750410068,6843458.307355457,6843450.7259237785,6843443.003545061,6843435.137601678,6843427.125427507,6843418.964307016,6843410.651474349,6843402.184112413,6843393.559351925,6843384.774270449,6843375.825891407,6843366.711183108,6843357.427057685,6843347.9703701325,6843338.337917168,6843328.526436225,6843318.532604322,6843308.353036984,6843297.984287082,6843287.4228436835,6843276.665130893,6843265.707506647,6843254.546261491,6843243.177617364,6843231.5977263,6843219.802669167,6843207.788454367,6843195.55101649,6843183.086214954,6843170.389832635,6843157.45757446,6843144.285065976,6843130.867851881,6843117.201394571,6843103.281072595,6843089.102179142,6843074.659920463,6843059.949414296,6843044.965688208,6843029.703677999,6843014.158225955,6842998.324079203,6842982.195887908,6842965.768203564,6842949.035477137,6842931.992057243,6842914.63218828,6842896.950008541,6842878.939548241,6842860.594727576,6842841.909354708,6842822.877123699,6842803.491612466,6842783.74628063,6842763.634467393,6842743.149389327,6842722.2841381375,6842701.031678392,6842679.384845234,6842657.336341983,6842634.878737785,6842612.004465155,6842588.705817487,6842564.974946545,6842540.803859888,6842516.184418252,6842491.108332893,6842465.567162888,6842439.552312372,6842413.0550277205,6842386.066394731,6842358.577335675,6842330.578606406,6842302.060793292,6842273.014310179,6842243.429395322,6842213.296108151,6842182.604326101,6842151.343741316,6842119.503857316,6842087.073985629,6842054.043242296,6842020.400544419,6841986.134606543,6841951.23393706,6841915.686834492,6841879.481383761,6841842.605452331,6841805.04668635,6841766.792506717,6841727.830104999,6841688.146439409,6841647.728230603,6841606.561957477,6841564.633852861,6841521.929899147,6841478.435823845,6841434.137095047,6841389.018916853,6841343.066224704,6841296.26368057,6841248.595668217,6841200.046288214,6841150.599352982,6841100.238381693,6841048.946595149,6840996.70691049,6840943.501935916,6840889.313965208,6840834.12497229,6840777.916605588,6840720.670182352,6840662.366682893,6840602.986744698,6840542.510656454,6840480.91835201,6840418.189404208,6840354.303018624,6840289.2380272,6840222.972881807,6840155.485647687,6840086.753996758,6840016.755200885,6839945.466124989,6839872.863220078,6839798.922516149,6839723.619614987,6839646.929682884,6839568.827443195,6839489.28716882,6839408.282674564,6839325.787309339,6839241.773948347,6839156.214985038,6839069.082323014,6838980.347367808,6838889.981018499,6838797.953659275,6838704.235150795,6838608.7948215045,6838511.601458769,6838412.623299891,6838311.828023024,6838209.182737946,6838104.653976688,6837998.207684046,6837889.809208,6837779.423289912,6837667.014054688,6837552.545000721,6837435.978989808,6837317.278236802,6837196.404299242,6837073.318066773,6836947.97975049,6836820.348872095,6836690.384252945,6836558.044002954,6836423.285509387,6836286.065425449,6836146.339658847,6836004.063360076,6835859.19091071,6835711.675911428,6835561.471170057,6835408.528689286,6835252.799654432,6835094.234420942,6834932.782501863,6834768.392555058,6834601.012370439,6834430.588856937,6834257.068029422,6834080.394995494,6833900.513942104,6833717.368122093,6833530.899840595,6833341.050441328,6833147.760292763,6832950.968774183,6832750.614261625,6832546.63411373,6832338.964657472,6832127.5411737785,6831912.297883118,6831693.1679308815,6831470.083372794,6831242.975160131,6831011.773124972,6830776.405965285,6830536.801229971,6830292.8853038605,6830044.583392652,6829791.81950777,6829534.516451206,6829272.595800333,6829005.977892635,6828734.581810497,6828458.325365876,6828177.125085091,6827890.896193526,6827599.55260032,6827303.006883241,6827001.170273383,6826693.952640075,6826381.262475735,6826063.006880866,6825739.091549103,6825409.420752326,6825073.897325933,6824732.422654198,6824384.89665577,6824031.21776933,6823671.282939407,6823304.987602388,6822932.225672716,6822552.889529299,6822166.870002201,6821774.05635953,6821374.336294636,6820967.595913608,6820553.719723087,6820132.590618395,6819704.08987208,6819268.097122794,6818824.490364656,6818373.145936953,6817913.938514411,6817446.741097927,6816971.425005777,6816487.859865442,6815995.9136059955,6815495.452451085,6814986.340912623,6814468.441785064,6813941.61614049,6813405.72332441,6812860.620952357,6812306.16490729,6811742.209337904,6811168.60665782,6810585.207545728,6809991.860946555,6809388.414073595,6808774.71241182,6808150.599722229,6807515.91804743,6806870.507718429,6806214.207362691,6805546.853913534,6804868.28262089,6804178.327063521,6803476.819162704,6802763.589197475,6802038.465821452,6801301.276081353,6800551.845437219,6799789.99778441,6799015.55547745,6798228.339355844,6797428.168771727,6796614.861619712,6795788.234368766,6794948.102096252,6794094.278524259,6793226.576058237,6792344.8058279855,6791448.777731162,6790538.3004793,6789613.181646402,6788673.227720275,6787718.244156578,6786748.035435737,6785762.405122777,6784761.155930085,6783744.089783326,6782711.007890444,6781661.710813915,6780595.998546285,6779513.670589114,6778414.526035346,6777298.363655242,6776164.981985908,6775014.179424534,6773845.754325377,6772659.505100591,6771455.230324971,6770232.728844708,6768991.799890142,6767732.243192716,6766453.8591060545,6765156.448731363,6763839.814047098,6762503.758043032,6761148.084858803,6759772.599926874,6758377.110120097,6756961.423903871,6755525.351492874,6754068.705012541,6752591.298665196,6751092.948900931,6749573.474593267,6748032.6972195525,6746470.441046199,6744886.533318667,6743280.804456266,6741653.088251758,6740003.222075713,6738331.0470856065,6736636.408439666,6734919.155515375,6733179.142132627,6731416.226781402,6729630.272854036,6727821.148881832,6725988.7287760675,6724132.892073227,6722253.524184359,6720350.516648438,6718423.767389642,6716473.180978302,6714498.668895451,6712500.149800801,6710477.549803848,6708430.802738098,6706359.8504379885,6704264.643018457,6702145.139156817,6700001.306376664,6697833.121333672,6695640.570102792,6693423.64846676,6691182.362205438,6688916.727385715,6686626.770651648,6684312.529514445,6681974.052641859,6679611.400146736,6677224.643874149,6674813.867686826,6672379.167748318,6669920.652803566,6667438.444456281,6664932.6774427965,6662403.499901729,6659851.073639116,6657275.574388379,6654677.192064656,6652056.131012944,6649412.610249521,6646746.86369605,6644059.140405854,6641349.704781781,6638618.836785071,6635866.83213466,6633094.002496365,6630300.675661333,6627487.195713237,6624653.923183587,6621801.235194636,6618929.525589308,6616039.20504759,6613130.701188854,6610204.45865961,6607260.939206105,6604300.621731386,6601324.002336228,6598331.5943435915,6595323.928306066,6592301.551996011,6589265.030377955,6586214.945562874,6583151.896744149,6580076.500114832,6576989.38876605,6573891.212566283,6570782.638021438,6567664.348115611,6564537.042132366,6561401.435456714,6558258.259357641,6555108.260751447,6551952.201945901,6548790.860365597,6545625.028258567,6542455.512384692,6539283.133686151,6536108.726940467,6532933.140396592,6529757.235394649,6526581.885970011,6523407.97844235,6520236.410990547,6517068.093214195,6513903.945682684,6510744.899472812,6507591.89569597,6504445.8850159645,6501307.827158708,6498178.690414887,6495059.451137018,6491951.093232059,6488854.607651093,6485770.991877405,6482701.249414485,6479646.389275412,6476607.425475204,6473585.376527665,6470581.26494838,6467596.116765408,6464630.961039409,6461686.829394758,6458764.755563347,6455865.774942774,6452990.924170468,6450141.240715483,6447317.76248955,6444521.527478933,6441753.573398767,6439014.937371278,6436306.655629471,6433629.763247645,6430985.293900166,6428374.27964977,6425797.750766662,6423256.735579569,6420752.26035979,6418285.349239346,6415857.024163971,6413468.304881874,6411120.208968915,6408813.751890741,6406549.947102428,6404329.806185876,6402154.339025277,6400024.554020647,6397941.458339488,6395906.0582063,6393919.359229704,6391982.366766694,6390096.086323425,6388261.523991788,6386479.68692093,6384751.583822623,6383078.225509392,6381460.6254640035,6379899.800438918,6378396.771084081,6376952.5626012925,6375568.205423292,6374244.735915547,6372983.197098581,6371784.639388553,6370650.121353697,6369580.710484112,6368577.483972187,6367641.529500979,6366773.94603764,6365975.844628919,6365248.349195686,6364592.59732332,6364009.741044713,6363500.947612568,6363067.400257606,6362710.2989292145,6362430.861015039,6362230.322035918,6362109.93631259,6362070.977600475,6362114.739688912,6362242.536961071,6362455.7049109405,6362755.600613501,6363143.603144553,6363621.113946328,6364189.557135266,6364850.379748288,6365605.051923843,6366455.067014236,6367401.941625604,6368447.215582051,6369592.451810543,6370839.236143125,6372189.17703327,6373643.905183058,6375205.073078154,6376874.354427553,6378653.443505211,6380544.054390781,6382547.920106823,6384666.791649995,6386902.436913816,6389256.6395008825,6391731.197422393,6394327.921683166,6397048.634750456,6399895.168905055,6402869.36447331,6405973.067939073,6409208.12993457,6412576.40310959,6416079.73987853,6419719.99004513,6423498.998304917,6427418.601625686,6431480.626506602,6435686.886116801,6440039.177314596,6444539.277548747,6449188.941643544,6453989.898469761,6458943.847503831,6464052.455277984,6469317.351724347,6474740.126416448,6480322.324711805,6486065.443799753,6491970.928658944,6498040.167929364,6504274.489704128,6510675.157246599,6517243.364638879,6523980.23236807,6530886.802857071,6537964.035947144,6545212.8043398205,6552633.889006266,6560227.974572374,6567995.644688584,6575937.377393598,6584053.540481666,6592344.386883549,6600810.050071577,6609450.539499721,6618265.736089949,6627255.3877764605,6636419.105119912,6645756.357003937,6655266.466426794,6664948.606401157,6674801.795975483,6684824.896390702,6695016.60738614,6705375.463669065,6715899.831562193,6726587.905843984,6737437.706796575,6748447.077476354,6759613.681222319,6770934.9994174875,6782408.329518521,6794030.783368839,6805799.285810377,6817710.573609015,6829761.194708552,6841947.507827915,6854265.682415867,6866711.698977289,6879281.349784567,6891970.239987125,6904773.789131641,6917687.233104769,6930705.626509427,6943823.845485002,6957036.5909808,6970338.39249113,6983723.612259364,6997186.449957061,7010720.947843051,7024320.99640598,7037980.340492438,7051692.585921165,7065451.206582351,7079249.552019207,7093080.855487382,7106938.242485821,7120814.73975086,7134703.284703342,7148596.7353365775,7162487.880530896,7176369.450778509,7190234.129300322,7204074.56353417,7217883.376972064,7231653.1813217215,7245376.588965964,7259046.225691324,7272654.743655522,7286194.834561609,7299659.243004879,7313040.779957123,7326332.336351348,7339526.896728796,7352617.552909013,7365597.517642789,7378460.13820699,7391198.909899844,7403807.489394873,7416279.707911579,7428609.584161062,7440791.337025197,7452819.3979284745,7464688.422862515,7476393.304024201,7487929.181029772,7499291.451668604,7510475.782162087,7521478.116894976,7532294.687588554,7542922.021887224,7553356.951332522,7563596.618701001,7573638.484685069,7583480.3338986,7593120.280191759,7602556.77126248,7611788.592554657,7620814.870436049,7629635.074651683,7638249.020051204,7646656.867591424,7654859.124617812,7662856.644431234,7670650.625148618,7678242.607868437,7685634.474154086,7692828.44285009,7699827.066247945,7706633.225620047,7713250.12614149,7719681.291221011,7725930.556263265,7732002.061885726,7737900.246614222,7743629.839081733,7749195.849755549,7754603.562218299,7759858.524028426,7764966.537185804,7769933.648228234,7774766.137984202,7779470.511007307,7784053.484717154,7788521.978271394,7792883.10119295,7797144.141776048,7801312.55529422,7805395.952032748,7809402.085167615,7813338.838512339,7817214.214153627,7821036.319996105,7824813.357235986,7828553.607782905,7832265.421648765,7835957.204321956,7839637.404144843,7843314.499712134,7846996.987307306,7850693.368394034,7854412.137179164,7858161.768263767,7861950.704398255,7865787.344357727,7869680.030953271,7873637.03919493,7877666.564621909,7881776.711815454,7885975.483109788,7890270.767516244,7894670.329875818,7899181.800255058,7903812.663600182,7908570.249664096,7913461.723220843,7918494.074581761,7923674.110427378,7929008.444968816,7934503.491452105,7940165.454018466,7946000.319933134,7952013.852194889,7958211.582537899,7964598.804836858,7971180.568925866,7977961.674840667,7984946.667493272,7992139.831787059,7999545.188179741,8007166.488700535,8015007.213427069,8023070.567426603,8031359.478164943,8039876.593385593,8048624.279460529,8057604.620212903,8066819.416210789,8076270.184530162,8085958.158983939,8095884.290812981,8106049.24983368,8116453.426035781,8127096.93162284,8137979.603486865,8149101.006107409,8160460.434864527,8172056.919754068,8183889.229492647,8195955.875999057,8208255.119237802,8220784.972409803,8233543.207474569,8246527.360987489,8259734.740235334,8273162.429652513,8286807.297500142,8300666.00278972,8314735.00243274,8329010.55859746,8343488.746253736,8358165.460886869,8373036.426361217,8388097.202914487,8403343.195263613,8418769.66080328,8434371.717878453,8450144.354112381,8466082.434772026,8482180.711153101,8498433.828967422,8514836.336715601,8531382.694028825,8548067.279963724,8564884.401235197,8581828.300372384,8598893.16378386,8616073.12971852,8633362.296109572,8650754.728289474,8668244.466564551,8685825.533638623,8703491.941875737,8721237.700392801,8739056.821973678,8756943.329796884,8774891.263969982,8792894.687864192,8810947.694243666,8829044.411184428,8847179.007778697,8865345.69962094,8883538.754072718,8901752.49530385,8919981.309108283,8938219.647493288,8956462.0330415,8974703.06304565,8992937.413416382,9011159.842364162,9029365.193856508,9047548.400852505,9065704.488316726,9083828.57601526,9101915.881096786,9119961.72046204,9137961.512925364,9155910.781172186,9173805.153516766]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[7568254.936651907,7568254.902605144,7568254.867924763,7568254.832598978,7568254.796615789,7568254.759962948,7568254.722628003,7568254.684598249,7568254.645860768,7568254.60640238,7568254.566209673,7568254.52526899,7568254.483566396,7568254.441087721,7568254.397818527,7568254.353744084,7568254.308849423,7568254.263119275,7568254.216538096,7568254.16909004,7568254.120758984,7568254.071528489,7568254.021381823,7568253.97030193,7568253.918271442,7568253.865272676,7568253.811287617,7568253.756297893,7568253.70028481,7568253.643229349,7568253.585112081,7568253.525913265,7568253.465612768,7568253.4041900765,7568253.341624331,7568253.277894245,7568253.212978152,7568253.146853982,7568253.07949926,7568253.01089107,7568252.9410061,7568252.869820582,7568252.797310321,7568252.723450662,7568252.648216484,7568252.571582219,7568252.493521813,7568252.414008715,7568252.333015902,7568252.250515842,7568252.166480468,7568252.080881226,7568251.9936890025,7568251.904874164,7568251.814406501,7568251.72225527,7568251.628389128,7568251.532776178,7568251.435383896,7568251.336179182,7568251.235128309,7568251.132196909,7568251.027350001,7568250.920551931,7568250.811766392,7568250.7009564,7568250.588084283,7568250.473111662,7568250.355999458,7568250.236707845,7568250.115196278,7568249.991423428,7568249.865347239,7568249.736924837,7568249.606112554,7568249.472865929,7568249.337139652,7568249.1988875875,7568249.058062738,7568248.914617212,7568248.768502259,7568248.619668194,7568248.468064423,7568248.313639397,7568248.156340635,7568247.996114645,7568247.832906961,7568247.666662099,7568247.497323541,7568247.324833711,7568247.149133986,7568246.970164624,7568246.787864783,7568246.602172491,7568246.41302462,7568246.220356874,7568246.024103743,7568245.824198515,7568245.620573235,7568245.413158679,7568245.201884335,7568244.986678377,7568244.767467654,7568244.5441776365,7568244.316732428,7568244.085054701,7568243.849065705,7568243.608685218,7568243.363831518,7568243.114421368,7568242.860369998,7568242.601591034,7568242.337996515,7568242.069496821,7568241.7960007,7568241.517415173,7568241.233645542,7568240.944595347,7568240.650166322,7568240.350258402,7568240.044769618,7568239.7335961545,7568239.416632211,7568239.093770062,7568238.764899959,7568238.429910109,7568238.088686646,7568237.741113588,7568237.387072789,7568237.026443904,7568236.659104361,7568236.284929295,7568235.903791523,7568235.515561493,7568235.120107253,7568234.717294392,7568234.306985993,7568233.889042587,7568233.463322134,7568233.029679924,7568232.587968576,7568232.13803796,7568231.679735153,7568231.212904384,7568230.737386995,7568230.253021375,7568229.759642891,7568229.25708386,7568228.745173497,7568228.223737798,7568227.6925995685,7568227.151578286,7568226.600490087,7568226.039147683,7568225.4673603065,7568224.884933635,7568224.291669746,7568223.6873670025,7568223.071820062,7568222.444819738,7568221.80615295,7568221.155602678,7568220.492947829,7568219.817963242,7568219.130419533,7568218.430083074,7568217.716715888,7568216.990075567,7568216.249915189,7568215.495983269,7568214.728023625,7568213.945775294,7568213.1489725,7568212.337344506,7568211.510615534,7568210.668504684,7568209.810725842,7568208.936987569,7568208.046992992,7568207.140439754,7568206.217019844,7568205.27641954,7568204.318319281,7568203.342393574,7568202.348310874,7568201.335733458,7568200.3043173505,7568199.253712156,7568198.1835609935,7568197.093500313,7568195.983159845,7568194.8521624105,7568193.700123827,7568192.526652781,7568191.331350681,7568190.113811531,7568188.873621776,7568187.610360186,7568186.323597708,7568185.0128973005,7568183.677813825,7568182.317893842,7568180.932675516,7568179.521688405,7568178.084453337,7568176.620482246,7568175.129277974,7568173.610334163,7568172.0631350055,7568170.487155145,7568168.881859445,7568167.246702846,7568165.581130145,7568163.884575846,7568162.156463929,7568160.396207691,7568158.603209528,7568156.776860722,7568154.916541287,7568153.021619677,7568151.091452656,7568149.1253850255,7568147.122749422,7568145.082866099,7568143.00504268,7568140.88857394,7568138.732741558,7568136.536813868,7568134.300045639,7568132.021677809,7568129.700937204,7568127.337036318,7568124.929173022,7568122.476530297,7568119.97827597,7568117.43356243,7568114.841526311,7568112.201288256,7568109.511952577,7568106.772606966,7568103.982322215,7568101.140151843,7568098.245131833,7568095.296280274,7568092.292597074,7568089.23306354,7568086.116642139,7568082.9422760615,7568079.708888927,7568076.415384374,7568073.06064574,7568069.643535628,7568066.162895601,7568062.617545695,7568059.006284127,7568055.327886808,7568051.581106976,7568047.764674744,7568043.877296741,7568039.9176555835,7568035.8844095245,7568031.776191927,7568027.591610859,7568023.329248592,7568018.9876611745,7568014.565377876,7568010.060900748,7568005.47270413,7568000.799234082,7567996.038907919,7567991.190113661,7567986.251209493,7567981.2205232065,7567976.096351668,7567970.876960227,7567965.560582144,7567960.145417995,7567954.629635073,7567949.011366792,7567943.288712029,7567937.459734539,7567931.522462263,7567925.474886691,7567919.3149622,7567913.040605357,7567906.649694241,7567900.14006773,7567893.5095247645,7567886.755823651,7567879.876681299,7567872.869772443,7567865.732728908,7567858.463138799,7567851.058545715,7567843.516447905,7567835.834297483,7567828.009499549,7567820.039411351,7567811.9213413885,7567803.65254855,7567795.230241171,7567786.65157616,7567777.913658013,7567769.013537885,7567759.948212601,7567750.7146236785,7567741.30965631,7567731.730138348,7567721.972839269,7567712.034469058,7567701.911677205,7567691.601051536,7567681.099117143,7567670.402335214,7567659.507101885,7567648.409747046,7567637.10653316,7567625.593654021,7567613.8672335185,7567601.923324387,7567589.757906879,7567577.366887476,7567564.746097572,7567551.891292086,7567538.798148079,7567525.462263382,7567511.879155139,7567498.044258354,7567483.952924412,7567469.600419576,7567454.981923463,7567440.092527451,7567424.927233133,7567409.480950699,7567393.748497263,7567377.724595202,7567361.403870503,7567344.780850963,7567327.84996449,7567310.605537287,7567293.041792024,7567275.152846023,7567256.932709362,7567238.375282931,7567219.4743565265,7567200.223606856,7567180.616595537,7567160.646767025,7567140.307446546,7567119.591838004,7567098.493021774,7567077.003952585,7567055.117457226,7567032.826232324,7567010.122842037,7566986.999715726,7566963.449145539,7566939.463284038,7566915.034141727,7566890.153584547,7566864.813331354,7566839.004951326,7566812.719861347,7566785.949323344,7566758.684441558,7566730.916159832,7566702.635258785,7566673.832352963,7566644.497887983,7566614.622137574,7566584.1952005895,7566553.2069980195,7566521.647269847,7566489.505572006,7566456.771273125,7566423.433551354,7566389.481391059,7566354.90357952,7566319.68870355,7566283.825146027,7566247.30108245,7566210.104477398,7566172.223080899,7566133.644424843,7566094.355819212,7566054.344348378,7566013.596867268,7565972.099997467,7565929.840123305,7565886.803387904,7565842.97568906,7565798.342675185,7565752.889741119,7565706.602023923,7565659.464398537,7565611.461473503,7565562.577586482,7565512.796799808,7565462.102895957,7565410.479372929,7565357.909439551,7565304.3760108175,7565249.861703004,7565194.348828863,7565137.819392674,7565080.255085243,7565021.637278835,7564961.947022042,7564901.165034603,7564839.271702113,7564776.247070694,7564712.070841585,7564646.72236569,7564580.180638002,7564512.424292011,7564443.4315940365,7564373.180437431,7564301.648336809,7564228.812422119,7564154.649432706,7564079.135711276,7564002.247197781,7563923.959423252,7563844.247503573,7563763.08613318,7563680.449578628,7563596.311672208,7563510.6458053915,7563423.424922252,7563334.621512818,7563244.207606331,7563152.154764499,7563058.434074571,7562963.01614251,7562865.871085899,7562766.968526979,7562666.27758548,7562563.766871469,7562459.404478085,7562353.15797426,7562244.994397341,7562134.880245667,7562022.781471109,7561908.663471518,7561792.491083126,7561674.22857293,7561553.83963096,7561431.287362564,7561306.534280578,7561179.542297518,7561050.272717681,7560918.686229183,7560784.7428960325,7560648.402150074,7560509.622782997,7560368.362938172,7560224.580102622,7560078.231098805,7559929.272076496,7559777.658504567,7559623.345162749,7559466.286133457,7559306.434793483,7559143.743805761,7558978.165111098,7558809.649919873,7558638.148703794,7558463.611187587,7558285.986340767,7558105.222369329,7557921.266707532,7557734.066009659,7557543.566141795,7557349.712173651,7557152.448370415,7556951.718184627,7556747.46424807,7556539.62836376,7556328.1514979405,7556112.973772143,7555894.034455287,7555671.271955877,7555444.623814239,7555214.026694825,7554979.416378606,7554740.7277555475,7554497.894817165,7554250.850649164,7553999.527424202,7553743.856394689,7553483.767885813,7553219.19128851,7552950.055052707,7552676.286680587,7552397.8127200315,7552114.558758133,7551826.449414928,7551533.408337171,7551235.358192326,7550932.2206626935,7550623.916439613,7550310.365217952,7549991.485690609,7549667.195543293,7549337.411449398,7549002.049065058,7548661.02302438,7548314.246934844,7547961.633372843,7547603.093879463,7547238.538956347,7546867.878061783,7546491.019606975,7546107.87095242,7545718.338404535,7545322.327212371,7544919.741564518,7544510.484586195,7544094.458336452,7543671.563805496,7543241.700912271,7542804.768501981,7542360.664343945,7541909.285129382,7541450.526469423,7540984.282893148,7540510.447845713,7540028.913686569,7539539.57168767,7539042.312031763,7538537.023810673,7538023.595023552,7537501.912575145,7536971.862273963,7536433.3288304,7535886.195854742,7535330.345855034,7534765.6602348145,7534192.019290592,7533609.3022092,7533017.387064786,7532416.150815587,7531805.469300282,7531185.217234082,7530555.268204276,7529915.49466542,7529265.767933933,7528605.958182207,7527935.934432032,7527255.564547435,7526564.715226747,7525863.251993915,7525151.039189014,7524427.939957803,7523693.816240396,7522948.528758874,7522191.9370038435,7521423.899219815,7520644.272389405,7519852.9122162135,7519049.67310638,7518234.408148689,7517406.969093162,7516567.206328127,7515714.968855553,7514850.104264754,7513972.458704213,7513081.8768516,7512178.2018817505,7511261.275432723,7510330.937569677,7509387.026746616,7508429.379765896,7507457.831735392,7506472.216023287,7505472.364210481,7504458.1060403725,7503429.269366213,7502385.680095743,7501327.162133172,7500253.537318473,7499164.625363927,7498060.243787797,7496940.207845265,7495804.330456468,7494652.4221317135,7493484.290893762,7492299.742197292,7491098.578845502,7489880.600903846,7488645.605610953,7487393.387286787,7486123.737238113,7484836.443661196,7483531.291542063,7482208.062554129,7480866.534953515,7479506.483472035,7478127.679208065,7476729.889515386,7475312.877890162,7473876.403856318,7472420.222849382,7470944.086099089,7469447.740510939,7467930.928547026,7466393.388106279,7464834.85240449,7463255.049854438,7461653.703946376,7460030.533129248,7458385.250693016,7456717.5646524215,7455027.177632672,7453313.7867573295,7451577.083539009,7449816.753773173,7448032.477435609,7446223.928584034,7444390.775264339,7442532.679421985,7440649.296819105,7438740.276957831,7436805.263010474,7434843.891757089,7432855.793530968,7430840.592172789,7428797.904993908,7426727.342749485,7424628.5096219815,7422501.003215809,7420344.414563544,7418158.32814457,7415942.321916559,7413695.967360616,7411418.829540515,7409110.467176831,7406770.432736421,7404398.272537934,7401993.5268738875,7399555.730149908,7397084.4110416705,7394579.092670032,7392039.292794911,7389464.52402837,7386854.294067341,7384208.105946426,7381525.458311171,7378805.845712126,7376048.758920074,7373253.685262642,7370420.108982561,7367547.511617766,7364635.3724034885,7361683.168696408,7358690.376420969,7355656.470537785,7352580.925534182,7349463.215936617,7346302.816845027,7343099.20448862,7339851.856803069,7336560.254028623,7333223.879328792,7329842.219429132,7326414.765275585,7322941.012711873,7319420.4631751785,7315852.624409518,7312237.011195954,7308573.146098873,7304860.560227428,7301098.794011165,7297287.397988839,7293425.933609392,7289513.974043919,7285551.105007444,7281536.925589307,7277471.049090885,7273353.103869238,7269182.73418542,7264959.601055907,7260683.383105766,7256353.77742202,7251970.500405679,7247533.2886208175,7243041.899639194,7238496.112878631,7233895.730433634,7229240.577896494,7224530.505167173,7219765.38725033,7214945.125037684,7210069.64607404,7205138.905305268,7200152.885806434,7195111.599488467,7190015.087781595,7184863.4222938465,7179656.705442999,7174395.071060299,7169078.684964314,7163707.745503345,7158282.484064904,7152803.165550588,7147270.088815099,7141683.587067781,7136044.028235474,7130351.81528527,7124607.386505975,7118811.21574707,7112963.812614094,7107065.72261936,7101117.527287083,7095119.844211974,7089073.327070612,7082978.665584727,7076836.585435896,7070647.848131046,7064413.250818359,7058133.626053198,7051809.841513872,7045442.799667021,7039033.43738266,7032582.725498904,7026091.668336545,7019561.303163798,7012992.69961156,7006386.959039738,6999745.213855195,6993068.626782063,6986358.390085276,6979615.724748183,6972841.879605379,6966038.13043183,6959205.778989577,6952346.1520333905,6945460.600276814,6938550.497320146,6931617.238542074,6924662.239956624,6917686.9370373385,6910692.783510568,6903681.250119892,6896653.823363777,6889612.004208595,6882557.306779256,6875491.257029784,6868415.391396107,6861331.255433564,6854240.402441549,6847144.392077783,6840044.7889648,6832943.161291186,6825841.079410216,6818740.114438464,6811641.836857104,6804547.815118469,6797459.614260587,6790378.794532329,6783306.9100318095,6776245.507360685,6769196.124296952,6762160.2884888435,6755139.516172378,6748135.310915085,6741149.162388386,6734182.545171088,6727236.917586333,6720313.720574381,6713414.376603468,6706540.288620965,6699692.839046962,6692873.388812359,6686083.276443456,6679323.817194959,6672596.302233241,6665901.997871601,6659242.144859208,6652617.957725294,6646030.624180097,6639481.304573944,6632971.131415791,6626501.2089524185,6620072.612809427,6613686.389695007,6607343.557167459,6601045.103467249,6594791.987414373,6588585.138371589,6582425.456274147,6576313.811726367,6570251.046165446,6564237.9720927095,6558275.373372481,6552364.005598564,6546504.596528317,6540697.846584171,6534944.429422295,6529244.992568117,6523600.158118222,6518010.523508092,6512476.662345053,6506999.125305683,6501578.441096871,6496215.117479557,6490909.642354176,6485662.484906621,6480474.096813564,6475344.913505758,6470275.355487937,6465265.829713765,6460316.731014239,6455428.443577798,6450601.3424803475,6445835.795263228,6441132.163557145,6436490.804749905,6431912.073695727,6427396.3244637735,6422943.912123487,6418555.19456415,6414230.534346008,6409970.300580212,6405774.870834678,6401644.633062903,6397579.987552616,6393581.348891102,6389649.147943861,6385783.833843186,6381985.875983169,6378255.766017466,6374594.019856098,6371001.179657468,6367477.815811633,6364024.528910781,6360641.951702797,6357330.751023659,6354091.6297043385,6350925.328447801,6347832.627671563,6344814.34931127,6341871.358580563,6339004.565682566,6336214.927468121,6333503.449035967,6330871.1852698745,6328319.24230783,6325848.778938224,6323461.007918007,6321157.197207771,6318938.671118654,6316806.811366011,6314763.058024783,6312808.910381484,6310945.9276778465,6309175.729741085,6307499.997495922,6305920.47335349,6304438.961472399,6303057.327887282,6301777.500500309,6300601.468931281,6299531.284222046,6298569.058391191,6297716.963835132,6296977.2325719325,6296352.155324426,6295844.080439469,6295455.412640394,6295188.611610088,6295046.190402338,6295030.713679556,6295144.795775184,6295391.098579636,6295772.329248894,6296291.237735342,6296950.61414093,6297753.28589306,6298702.1147442665,6299799.9935970735,6301049.8431560565,6302454.608409612,6304017.254944465,6305740.76509657,6307628.133942555,6309682.365136475,6311906.466597249,6314303.446052678,6316876.306446624,6319628.0412164405,6322561.629448407,6325680.03091945,6328986.18103408,6332482.985665947,6336173.315914103,6340060.00278444,6344145.831807462,6348433.537603867,6352925.798410057,6357625.2305759555,6362534.383048083,6367655.731851073,6372991.674581216,6378544.5249258755,6384316.507222855,6390309.751074004,6396526.2860274725,6402968.036343117,6409636.815855603,6416534.322949697,6423662.135662206,6431021.706924827,6438614.359962012,6446441.283857679,6454503.529304269,6462802.004547288,6471337.471538026,6480110.542306649,6489121.675567308,6498371.173566319,6507859.179183771,6517585.673298278,6527550.472423766,6537753.226626447,6548193.417729269,6558870.357810279,6569783.188000416,6580930.877585349,6592312.223415007,6603925.849623498,6615770.207661124,6627843.576639218,6640144.063987553,6652669.6064230595,6665417.97122766,6678386.7578320075,6691573.399701005,6704975.166516043,6718589.166648011,6732412.349914231,6746441.510611681,6760673.290818013,6775104.183951192,6789730.538577793,6804548.562459418,6819554.326825976,6834743.770864121,6850112.706408544,6865656.822823418,6881371.692060891,6897252.773883144,6913295.421234326,6929494.88574836,6945846.323378539,6962344.8001346495,6978985.2979133455,6995762.720407471,7012671.899080093,7029707.599189101,7046864.525848404,7064137.330111879,7081520.61506659,7099008.941921933,7116596.836081806,7134278.793187209,7152049.28511705,7169902.765935387,7187833.6777738,7205836.456637994,7223905.538128334,7242035.363064396,7260220.383004288,7278455.06564993,7296733.900130087,7315051.402153513,7333402.119025113,7351780.634518597,7370181.573599696,7388599.606994537,7407029.455598404,7425465.894720572,7443903.758161539,7462337.942119485,7480763.408923282,7499175.190589988,7517568.392205122,7535938.195124637,7554279.859997887,7572588.729611353,7590860.231553388,7609089.880700532,7627273.281526474,7645406.130235,7663484.216718725,7681503.4263456445,7699459.741575901,7717349.243411476,7735168.112681715,7752912.631167953],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[7619805.9387388155,7619805.711595476,7619805.480225021,7619805.244548778,7619805.004486643,7619804.759956954,7619804.510876614,7619804.257160918,7619803.998723604,7619803.735476809,7619803.467331036,7619803.194195107,7619802.91597616,7619802.632579602,7619802.343909075,7619802.049866459,7619801.750351745,7619801.44526313,7619801.134496873,7619800.817947328,7619800.495506847,7619800.167065812,7619799.832512576,7619799.49173339,7619799.144612382,7619798.791031531,7619798.430870612,7619798.064007219,7619797.690316585,7619797.309671675,7619796.921943068,7619796.526998935,7619796.12470504,7619795.714924573,7619795.297518219,7619794.872344063,7619794.439257559,7619793.998111474,7619793.548755821,7619793.091037822,7619792.624801858,7619792.149889418,7619791.666139071,7619791.173386308,7619790.671463626,7619790.160200382,7619789.63942276,7619789.108953705,7619788.568612855,7619788.01821652,7619787.457577593,7619786.886505443,7619786.304805943,7619785.712281322,7619785.108730129,7619784.493947197,7619783.867723489,7619783.229846119,7619782.580098232,7619781.918258943,7619781.244103233,7619780.557401911,7619779.857921518,7619779.145424265,7619778.419667934,7619777.68040579,7619776.927386499,7619776.160354071,7619775.379047742,7619774.583201905,7619773.772545981,7619772.946804429,7619772.105696483,7619771.24893622,7619770.376232394,7619769.487288314,7619768.581801774,7619767.659464962,7619766.719964316,7619765.762980475,7619764.788188091,7619763.795255795,7619762.783846037,7619761.753615008,7619760.704212457,7619759.635281678,7619758.54645929,7619757.437375137,7619756.307652226,7619755.156906501,7619753.984746789,7619752.790774637,7619751.5745841665,7619750.335761958,7619749.073886886,7619747.788530011,7619746.47925442,7619745.1456150375,7619743.787158532,7619742.403423128,7619740.99393846,7619739.558225426,7619738.095796007,7619736.6061530765,7619735.088790302,7619733.543191912,7619731.968832528,7619730.365177013,7619728.731680287,7619727.067787103,7619725.372931902,7619723.646538601,7619721.888020395,7619720.0967795765,7619718.272207301,7619716.41368341,7619714.520576207,7619712.592242265,7619710.628026148,7619708.627260244,7619706.589264573,7619704.513346404,7619702.398800234,7619700.244907334,7619698.050935693,7619695.816139642,7619693.539759619,7619691.221021999,7619688.859138704,7619686.453307065,7619684.002709411,7619681.506512914,7619678.963869249,7619676.373914295,7619673.735767882,7619671.048533426,7619668.311297756,7619665.523130651,7619662.683084583,7619659.790194441,7619656.843477146,7619653.841931345,7619650.784537066,7619647.670255362,7619644.49802796,7619641.266776922,7619637.975404272,7619634.622791588,7619631.207799685,7619627.729268178,7619624.186015114,7619620.576836537,7619616.900506141,7619613.155774815,7619609.341370201,7619605.455996306,7619601.498333025,7619597.467035729,7619593.360734777,7619589.178035082,7619584.917515577,7619580.577728837,7619576.157200512,7619571.654428809,7619567.067884031,7619562.396008123,7619557.637213936,7619552.789884933,7619547.852374455,7619542.82300529,7619537.700069026,7619532.481825489,7619527.166502167,7619521.752293648,7619516.237360911,7619510.619830794,7619504.8977952935,7619499.069310987,7619493.132398298,7619487.085040881,7619480.925184963,7619474.650738538,7619468.259570784,7619461.749511246,7619455.118349206,7619448.363832789,7619441.483668357,7619434.475519634,7619427.337006916,7619420.065706345,7619412.659149003,7619405.114820101,7619397.430158181,7619389.602554166,7619381.629350537,7619373.507840435,7619365.235266676,7619356.808820961,7619348.225642752,7619339.482818431,7619330.577380269,7619321.50630546,7619312.266515017,7619302.854872838,7619293.268184571,7619283.503196564,7619273.556594732,7619263.425003561,7619253.104984787,7619242.593036374,7619231.885591251,7619220.979016191,7619209.869610509,7619198.553604826,7619187.027159855,7619175.286365043,7619163.327237277,7619151.145719557,7619138.737679608,7619126.098908485,7619113.225119171,7619100.11194513,7619086.754938836,7619073.149570266,7619059.291225378,7619045.175204594,7619030.796721142,7619016.150899527,7619001.232773845,7618986.037286115,7618970.559284635,7618954.793522136,7618938.734654119,7618922.377237052,7618905.715726491,7618888.744475222,7618871.457731452,7618853.849636743,7618835.9142241515,7618817.645416239,7618799.037022884,7618780.0827394,7618760.776144294,7618741.110697148,7618721.079736466,7618700.676477377,7618679.89400939,7618658.725294112,7618637.163162867,7618615.200314283,7618592.829311874,7618570.042581541,7618546.832409031,7618523.190937411,7618499.110164375,7618474.581939601,7618449.597962036,7618424.149777142,7618398.228774039,7618371.826182652,7618344.933070779,7618317.540341132,7618289.638728315,7618261.218795686,7618232.270932283,7618202.785349584,7618172.752078275,7618142.160964889,7618111.001668532,7618079.2636573315,7618046.9362050295,7618014.008387352,7617980.46907848,7617946.306947215,7617911.510453431,7617876.067844051,7617839.967149272,7617803.196178545,7617765.742516623,7617727.593519359,7617688.736309611,7617649.157772961,7617608.844553355,7617567.783048753,7617525.9594065985,7617483.359519309,7617439.969019593,7617395.773275722,7617350.75738673,7617304.906177524,7617258.204193894,7617210.635697441,7617162.184660423,7617112.834760494,7617062.569375368,7617011.371577369,7616959.224127895,7616906.109471799,7616852.009731647,7616796.906701912,7616740.781842978,7616683.616275167,7616625.39077253,7616566.085756689,7616505.681290371,7616444.157070978,7616381.492424053,7616317.666296499,7616252.657249767,7616186.443453044,7616119.002675981,7616050.312281777,7615980.349219638,7615909.090017493,7615836.5107744,7615762.587152878,7615687.294371043,7615610.607194733,7615532.499929327,7615452.946411589,7615371.920001311,7615289.3935727645,7615205.339506058,7615119.729678325,7615032.535454842,7614943.727679831,7614853.276667295,7614761.152191565,7614667.323477724,7614571.759191905,7614474.427431423,7614375.295714626,7614274.330970815,7614171.499529723,7614066.767111034,7613960.098813593,7613851.459104547,7613740.811808166,7613628.120094677,7613513.346468673,7613396.45275761,7613277.4000998335,7613156.14893263,7613032.658979983,7612906.889240167,7612778.797973078,7612648.342687449,7612515.480127836,7612380.166261354,7612242.356264224,7612102.004508151,7611959.0645464305,7611813.489099848,7611665.230042386,7611514.238386681,7611360.464269288,7611203.8569356715,7611044.364724965,7610881.935054575,7610716.514404481,7610548.048301283,7610376.48130207,7610201.756977971,7610023.817897573,7609842.605609968,7609658.060627626,7609470.122408972,7609278.729340756,7609083.818720148,7608885.326736492,7608683.188453013,7608477.3377879765,7608267.707495826,7608054.229147869,7607836.833112903,7607615.448537307,7607390.003325094,7607160.424117595,7606926.636272808,7606688.563844588,7606446.1295614615,7606199.254805226,7605947.859589243,7605691.862536431,7605431.180857002,7605165.730325903,7604895.425259975,7604620.178494838,7604339.901361458,7604054.503662491,7603763.893648231,7603467.977992451,7603166.66176773,7602859.848420715,7602547.439746921,7602229.335865438,7601905.435193134,7601575.634418773,7601239.828476695,7600897.910520439,7600549.771895796,7600195.302113853,7599834.388823636,7599466.917784492,7599092.772838301,7598711.835881301,7598323.986835751,7597929.10362133,7597527.062126257,7597117.736178222,7596700.997515043,7596276.71575514,7595844.75836775,7595404.990642925,7594957.275661353,7594501.474264065,7594037.445021754,7593565.044204063,7593084.12574869,7592594.541230347,7592096.139829484,7591588.76830099,7591072.270942703,7590546.489563854,7590011.263453363,7589466.429348123,7588911.8214012,7588347.271149915,7587772.607484041,7587187.656613781,7586592.24203805,7585986.184512444,7585369.302017466,7584741.40972687,7584102.319975874,7583451.842229719,7582789.783052202,7582115.946074501,7581430.131964104,7580732.138394023,7580021.760012211,7579298.788411325,7578563.012098732,7577814.216466924,7577052.183764301,7576276.693066367,7575487.520247411,7574684.437952624,7573867.215570882,7573035.619207967,7572189.4116604915,7571328.352390557,7570452.197500999,7569560.699711512,7568653.608335526,7567730.669258042,7566791.624914358,7565836.214269711,7564864.172800113,7563875.232474211,7562869.121736316,7561845.565490681,7560804.285087141,7559744.998308058,7558667.419356771,7557571.258847514,7556456.223797009,7555322.017617673,7554168.340112682,7552994.887472752,7551801.352274989,7550587.423483707,7549352.78645332,7548097.122933619,7546820.111077083,7545521.425448923,7544200.73703941,7542857.713278914,7541492.018055713,7540103.31173658,7538691.25119039,7537255.4898147555,7535795.677565925,7534311.460991972,7532802.4832694465,7531268.384243585,7529708.800472291,7528123.365273986,7526511.708779303,7524873.457987126,7523208.2368247975,7521515.6662126975,7519795.364133556,7518046.945706414,7516270.023265432,7514464.206443836,7512629.102263041,7510764.315227127,7508869.447422905,7506944.098625723,7504987.866411024,7503000.346272124,7500981.131744151,7498929.814534383,7496845.984659288,7494729.230588182,7492579.139394,7490395.29691115,7488177.287900666,7485924.696222961,7483637.105018259,7481314.096894904,7478955.254125762,7476560.158852922,7474128.393300848,7471659.539998185,7469153.182008384,7466608.9031693535,7464026.288342373,7461404.923670287,7458744.396845368,7456044.297386868,7453304.2169284895,7450523.749515977,7447702.491914872,7444840.043928797,7441936.008728184,7438989.993189749,7436001.60824695,7432970.469251185,7429896.196344294,7426778.414842286,7423616.755630263,7420410.855569011,7417160.357912894,7413864.912739557,7410524.177391208,7407137.816927595,7403705.504590934,7400226.92228244,7396701.761050795,7393129.721592391,7389510.514763423,7385843.862103654,7382129.4963719,7378367.162093221,7374556.6161175715,7370697.6281898385,7366789.9815312885,7362833.473431995,7358827.915854276,7354773.136046872,7350668.97716955,7346515.298928053,7342311.978218922,7338058.909783957,7333756.006874038,7329403.201921812,7325000.447222952,7320547.715625499,7316045.001226793,7311492.320077667,7306889.710893081,7302237.235768973,7297534.98090449,7292783.057329052,7287981.601633624,7283130.776705489,7278230.772465707,7273281.806608637,7268284.125342617,7263238.004131066,7258143.748432979,7253001.694442168,7247812.209824071,7242575.694449361,7237292.581123208,7231963.336309264,7226588.460847331,7221168.490663553,7215703.9974720655,7210195.589467025,7204643.912003743,7199049.648267816,7193413.519931123,7187736.287793306,7182018.75240768,7176261.754690182,7170466.176510203,7164632.941262016,7158763.01441554,7152857.404045107,7146917.161335112,7140943.381061145,7134937.202045443,7128899.807585425,7122832.425854018,7116736.330270735,7110612.839842127,7104463.319470699,7098289.180231023,7092091.879612041,7085872.921724614,7079633.857473175,7073376.284690811,7067101.84823669,7060812.240055236,7054509.199196198,7048194.511795032,7041870.0110130785,7035537.576936887,7029199.136436545,7022856.662982485,7016512.17642069,7010167.742706205,7003825.473594918,6997487.526293731,6991156.1030694125,6984833.450816424,6978521.860584259,6972223.667064819,6965941.248040611,6959677.023794628,6953433.456482819,6947213.0494703865,6941018.346633084,6934851.931624886,6928716.427113631,6922614.493986228,6916548.830525206,6910522.171558578,6904537.287585002,6898596.98387646,6892704.099560715,6886861.506685968,6881072.109270265,6875338.842338237,6869664.670947971,6864052.589210809,6858505.619307031,6853026.810500421,6847619.238154784,6842286.002755623,6837030.22894016,6831855.064538927,6826763.679632326,6821759.2656254005,6816845.034344231,6812024.217157286,6807300.064125092,6802675.843181601,6798154.839350522,6793740.353999934,6789435.704138405,6785244.221755791,6781169.253211807,6777214.1586753735,6773382.311617685,6769677.098361757,6766101.917691198,6762660.180520693,6759355.309630658,6756190.7394683035,6753169.916017191,6750296.296737212,6747573.350576716,6745004.558058336,6742593.411439853,6740343.4149512015,6738258.085108595,6736340.9511064,6734595.555287269,6733025.453690745,6731634.216680342,6730425.429648864,6729402.693801453,6728569.627015647,6727929.864777478,6727487.061192357,6727244.890069272,6727207.046076613,6727377.2459675865,6727759.229873089,6728356.762659517,6729173.63534887,6730213.666598218,6731480.704235387,6732978.626847483,6734711.34541871,6736682.8050136445,6738896.9865020225,6741357.908320825,6744069.628269309,6747036.245332442,6750261.901528,6753750.783772507,6757507.125760942,6761535.209855088,6765839.368975245,6770423.988489837,6775293.5080974735,6780452.423695791,6785905.2892314,6791656.718525149,6797711.387066846,6804074.033773559,6810749.462705514,6817742.544733631,6825058.219152698,6832701.495234123,6840677.453712312,6848991.248198579,6857648.106516689,6866653.331953975,6876012.30442223,6885730.4815223785,6895813.399507245,6906266.674136618,6917096.001419003,6928307.158234494,6939906.002833373,6951898.475205057,6964290.597312259,6977088.473185257,6990298.288871429,7003926.312235265,7017978.892604308,7032462.460256655,7047383.525745791,7062748.679058815,7078564.58860426,7094838.000025963,7111575.734839706,7128784.688889527,7146471.830620961,7164644.199168643,7183308.902256016,7202473.113905215,7222144.071955442,7242329.075388518,7263035.481460509,7284270.702638801,7306042.203344196,7328357.496498041,7351224.139874687,7374649.732259975,7398641.909416825,7423208.33985927,7448356.720436853,7474094.771731441,7500430.233269088,7527370.858549888,7554924.409899117,7583098.653143435,7611901.352116244,7641340.262996662,7671423.128487066,7702157.671834429,7733551.590701129,7765612.550891199,7798348.179938469,7831766.060563267,7865873.724004849,7900678.643236948,7936188.22607419,7972409.808177466,8009350.645966672,8047017.909449412,8085418.67497467,8124559.917920613,8164448.50532597,8205091.188474639,8246494.595443359,8288665.2236225745,8331609.432220589,8375333.434761435,8419843.291586874,8465144.902373126,8511243.99867303,8558146.136494169,8605856.688923888,8654380.838811802,8703723.571520582,8753889.667755723,8804883.696484871,8856710.00795732,8909372.726834064,8962875.745438766,9017222.717139807,9072417.049873417,9128461.899817685,9185360.165227074,9243114.48043678,9301727.21004615,9361200.443289878,9421535.988605704,9482735.368406778,9544799.814066682,9607730.26112482,9671527.34471932,9736191.395254457,9801722.43430916,9868120.170792682,9935383.997353366,10003512.987045683,10072505.890260713,10142361.131924318,10213076.808967564,10284650.688072544,10357080.203697294,10430362.456382327,10504494.21134122,10579471.897337291,10655291.605847552,10731949.090515222,10809439.766891273,10887758.712464998,10966900.666983573,11046860.033059586,11127630.877065476,11209206.930313313,11291581.590517705,11374747.923539665,11458698.665408352,11543426.224617546,11628922.684693297,11715179.807028601,11802189.033981018,11889941.492228243,11978427.996376835,12067639.052818704,12157564.863829669,12248195.331904324,12339520.064320859,12431528.377929509,12524209.304157997,12617551.594226994,12711543.724568712,12806173.902441224,12901430.071731197,12997299.918937566,13093770.879328381,13190830.143263323,13288464.662673801,13386661.157692991,13485406.123427834,13584685.836865032,13684486.363903273,13784793.566503616,13885593.109950272,13986870.47021399,14088610.941410191,14190799.643344373,14293421.529137064,14396461.392920934,14499903.877602806,14603733.482683193,14707934.57212661,14812491.382275434,14917388.029800814,15022608.519684019,15128136.753221715,15233956.536049232,15340051.586175459,15446405.542023763,15553001.97047318,15659824.374894466,15766856.20317563,15874080.855731888,15981481.693495141,16089042.0458781,16196745.218708668,16304574.502129931,16412513.178461632,16520544.530018926,16628651.84688445,16736818.434629947,16845027.6219835,16953262.76843896,17061507.271803953,17169744.57568302,17277958.176892657,17386131.632804897,17494248.568616204,17602292.684538662,17710247.762910213,17818097.67522102,17925826.389052793,18033417.974928156,18140856.613067035,18248126.60004699,18355212.355364725,18462098.427895565,18568769.50224795,18675210.405010093,18781406.110885542,18887341.748714916,18993002.60738044,19098374.141590532,19203441.97754118,19308191.918451004,19412609.949967228,19516682.24543904,19620395.171055637,19723735.29084558,19826689.371534627,19929244.387258988,20031387.52413074,20133106.184652817,20234387.99198034,20335220.794025518,20435592.66740347,20535491.921215948,20634907.10067057,20733826.99053292,20832240.618409008,20930137.257856004,21027506.431318715,21124337.91289007,21220621.73089347,21316348.17028543,21411507.774876785,21506091.34937121,21600089.96121965,21693494.94228983,21786297.890349817,21878490.670365278,21970065.415609714,22061014.528587792,22151330.681771576,22241006.818150073,22330036.15159257,22418412.16702644,22506128.620430462,22593179.538644753,22679559.21899896,22765262.228760157,22850283.404402524,22934617.85070084,23018260.939650245,23101208.30921492,23183455.861908276,23264999.763207927,23345836.439808547,23425962.577716082,23505375.120186973,23584071.265516113,23662048.464677658,23739304.41882274,23815837.076638408,23891644.631572355,23966725.518927895,24041078.412833985,24114702.22309516,24187596.091926184,24259759.3905767,24331191.71585073,24401892.886526383,24471862.939680986,24541102.126926873,24609610.910563305,24677389.959649663,24744440.14600553,24810762.540142868,24876358.40713565,24941229.202432513,25005376.567617465,25068802.326124147,25131508.478908777,25193497.200087033,25254770.832539972,25315331.88349396,25375183.020079743,25434327.064875364,25492766.991437804,25550505.919828054,25607547.112134133,25663893.967996508,25719550.02014033,25774518.929918636,25828804.482870676,25882410.584299244,25935341.254870944,25987600.626242977,26039192.936720055,26090122.52694489,26140393.835625436,26190011.39530203,26238979.828157462,26287303.841872733,26334988.22553121,26382037.845573742,26428457.641807046,26474252.62346769,26519427.86534375,26563988.503956042,26607939.7338009,26651286.803655945,26694035.0129506,26736189.708202705],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[7378350.108001546,7378350.081824331,7378350.05515997,7378350.027999394,7378350.000333372,7378349.972152497,7378349.9434471885,7378349.9142076885,7378349.884424054,7378349.8540861625,7378349.823183693,7378349.791706147,7378349.759642822,7378349.726982815,7378349.693715024,7378349.659828133,7378349.6253106315,7378349.590150778,7378349.55433662,7378349.517855981,7378349.480696466,7378349.442845426,7378349.404290013,7378349.365017104,7378349.325013357,7378349.284265161,7378349.242758684,7378349.20047979,7378349.15741412,7378349.113547036,7378349.068863616,7378349.023348675,7378348.97698674,7378348.929762043,7378348.881658541,7378348.83265987,7378348.782749381,7378348.731910106,7378348.680124761,7378348.627375736,7378348.573645106,7378348.5189146,7378348.463165615,7378348.406379204,7378348.348536059,7378348.289616512,7378348.229600543,7378348.168467747,7378348.10619733,7378348.042768146,7378347.978158617,7378347.912346788,7378347.84531028,7378347.777026313,7378347.70747167,7378347.636622709,7378347.564455349,7378347.490945053,7378347.416066839,7378347.339795252,7378347.262104364,7378347.18296776,7378347.102358557,7378347.0202493435,7378346.936612207,7378346.851418727,7378346.764639939,7378346.676246346,7378346.586207907,7378346.4944940135,7378346.40107349,7378346.305914588,7378346.208984962,7378346.110251663,7378346.009681136,7378345.907239192,7378345.802891016,7378345.696601143,7378345.5883334465,7378345.47805112,7378345.365716695,7378345.251291978,7378345.134738085,7378345.0160154,7378344.895083573,7378344.771901512,7378344.646427332,7378344.5186184095,7378344.3884312995,7378344.255821749,7378344.120744705,7378343.983154246,7378343.843003618,7378343.700245199,7378343.554830467,7378343.406710004,7378343.255833478,7378343.102149603,7378342.945606166,7378342.786149965,7378342.623726816,7378342.458281519,7378342.289757857,7378342.118098558,7378341.943245305,7378341.765138665,7378341.583718122,7378341.398922038,7378341.210687609,7378341.018950887,7378340.82364671,7378340.62470873,7378340.422069342,7378340.2156596985,7378340.005409674,7378339.791247823,7378339.573101387,7378339.350896252,7378339.124556924,7378338.894006498,7378338.6591666555,7378338.419957611,7378338.176298096,7378337.928105343,7378337.6752950195,7378337.41778125,7378337.155476564,7378336.88829184,7378336.616136335,7378336.33891759,7378336.056541433,7378335.768911953,7378335.475931454,7378335.177500414,7378334.873517469,7378334.563879375,7378334.248480961,7378333.927215106,7378333.599972694,7378333.266642589,7378332.927111597,7378332.581264387,7378332.22898352,7378331.8701493675,7378331.504640067,7378331.132331494,7378330.753097238,7378330.366808512,7378329.973334151,7378329.572540556,7378329.164291638,7378328.74844879,7378328.324870818,7378327.893413911,7378327.453931598,7378327.00627467,7378326.55029116,7378326.085826266,7378325.612722324,7378325.130818741,7378324.639951933,7378324.139955282,7378323.630659082,7378323.11189047,7378322.583473379,7378322.045228468,7378321.49697307,7378320.9385211235,7378320.369683119,7378319.790266032,7378319.200073241,7378318.598904486,7378317.986555792,7378317.362819391,7378316.727483676,7378316.080333084,7378315.421148084,7378314.74970505,7378314.065776222,7378313.369129605,7378312.659528895,7378311.936733424,7378311.200498047,7378310.450573073,7378309.686704187,7378308.9086323455,7378308.116093712,7378307.308819553,7378306.486536157,7378305.648964737,7378304.795821331,7378303.926816727,7378303.041656331,7378302.140040112,7378301.221662469,7378300.286212131,7378299.333372076,7378298.362819388,7378297.374225182,7378296.367254476,7378295.34156609,7378294.296812514,7378293.232639805,7378292.148687467,7378291.044588336,7378289.919968437,7378288.774446879,7378287.607635724,7378286.419139841,7378285.208556796,7378283.975476706,7378282.719482101,7378281.440147791,7378280.137040718,7378278.809719811,7378277.45773585,7378276.080631291,7378274.677940142,7378273.249187799,7378271.793890867,7378270.311557032,7378268.801684881,7378267.263763718,7378265.697273431,7378264.101684284,7378262.476456763,7378260.821041393,7378259.134878537,7378257.417398238,7378255.668020022,7378253.886152676,7378252.0711940965,7378250.22253107,7378248.339539051,7378246.421581987,7378244.468012102,7378242.478169644,7378240.451382733,7378238.386967075,7378236.284225768,7378234.142449096,7378231.960914227,7378229.738885033,7378227.475611826,7378225.17033112,7378222.822265352,7378220.430622676,7378217.994596643,7378215.5133659905,7378212.986094341,7378210.411929922,7378207.790005321,7378205.119437172,7378202.399325864,7378199.628755265,7378196.806792419,7378193.932487222,7378191.0048721405,7378188.022961874,7378184.985753036,7378181.892223837,7378178.741333747,7378175.5320231505,7378172.263213022,7378168.933804536,7378165.542678765,7378162.088696265,7378158.570696746,7378154.987498667,7378151.337898873,7378147.620672207,7378143.834571089,7378139.978325144,7378136.05064079,7378132.050200795,7378127.975663894,7378123.825664318,7378119.59881139,7378115.293689048,7378110.908855424,7378106.4428423615,7378101.894154959,7378097.261271076,7378092.542640874,7378087.736686306,7378082.841800615,7378077.856347836,7378072.778662275,7378067.607047962,7378062.339778157,7378056.975094758,7378051.511207781,7378045.946294792,7378040.278500317,7378034.505935284,7378028.626676417,7378022.638765646,7378016.540209467,7378010.328978378,7378004.003006194,7377997.560189421,7377990.998386633,7377984.315417763,7377977.5090634655,7377970.577064432,7377963.517120664,7377956.326890812,7377949.003991422,7377941.545996239,7377933.950435431,7377926.214794882,7377918.336515381,7377910.312991883,7377902.14157272,7377893.819558773,7377885.344202703,7377876.712708078,7377867.92222859,7377858.969867153,7377849.852675086,7377840.567651222,7377831.111740994,7377821.481835575,7377811.674770943,7377801.68732694,7377791.516226365,7377781.158133993,7377770.6096555935,7377759.867336993,7377748.927663013,7377737.787056534,7377726.441877394,7377714.888421405,7377703.122919268,7377691.141535512,7377678.940367423,7377666.515443915,7377653.862724443,7377640.97809786,7377627.8573812805,7377614.49631892,7377600.89058093,7377587.035762179,7377572.927381089,7377558.560878388,7377543.931615888,7377529.034875233,7377513.865856626,7377498.419677572,7377482.691371539,7377466.675886708,7377450.368084596,7377433.762738731,7377416.854533311,7377399.638061813,7377382.107825624,7377364.258232621,7377346.083595775,7377327.57813173,7377308.7359593,7377289.551098109,7377270.017467021,7377250.128882724,7377229.879058195,7377209.261601199,7377188.270012753,7377166.89768562,7377145.137902705,7377122.983835542,7377100.428542697,7377077.464968171,7377054.085939836,7377030.284167802,7377006.052242803,7376981.382634579,7376956.26769024,7376930.699632606,7376904.670558583,7376878.172437484,7376851.19710938,7376823.7362834,7376795.781536106,7376767.324309769,7376738.355910709,7376708.867507601,7376678.850129806,7376648.29466567,7376617.191860835,7376585.532316589,7376553.306488146,7376520.50468299,7376487.117059229,7376453.133623893,7376418.54423129,7376383.338581393,7376347.50621818,7376311.036528013,7376273.918738074,7376236.141914728,7376197.694961988,7376158.566619954,7376118.745463315,7376078.219899806,7376036.978168783,7375995.008339741,7375952.298310939,7375908.835807999,7375864.6083825845,7375819.603411089,7375773.808093386,7375727.209451622,7375679.79432906,7375631.549388947,7375582.461113492,7375532.515802859,7375481.699574218,7375429.998360912,7375377.3979116455,7375323.883789751,7375269.441372568,7375214.055850881,7375157.712228442,7375100.395321591,7375042.089758969,7374982.779981361,7374922.450241596,7374861.084604599,7374798.666947562,7374735.180960204,7374670.610145204,7374604.937818724,7374538.147111152,7374470.220967881,7374401.14215036,7374330.893237228,7374259.456625654,7374186.814532855,7374112.948997781,7374037.841883024,7373961.474876889,7373883.829495724,7373804.887086428,7373724.6288292175,7373643.03574061,7373560.088676672,7373475.768336513,7373390.055266053,7373302.929862064,7373214.372376525,7373124.362921229,7373032.881472745,7372939.907877692,7372845.421858342,7372749.403018569,7372651.830850163,7372552.68473954,7372451.943974788,7372349.587753166,7372245.595188966,7372139.945321857,7372032.617125664,7371923.589517519,7371812.841367633,7371700.351509419,7371586.098750225,7371470.061882489,7371352.21969554,7371232.550987859,7371111.034579954,7370987.649327839,7370862.374137083,7370735.187977491,7370606.069898464,7370474.999044968,7370341.954674229,7370206.916173091,7370069.863076105,7369930.775084384,7369789.632085162,7369646.414172171,7369501.101666809,7369353.675140115,7369204.115435579,7369052.4036928415,7368898.521372226,7368742.450280203,7368584.172595771,7368423.670897768,7368260.9281931305,7368095.927946187,7367928.654108877,7367759.0911520645,7367587.224097813,7367413.038552818,7367236.520742833,7367057.657548221,7366876.436540654,7366692.846020901,7366506.875057786,7366318.513528318,7366127.752159007,7365934.582568357,7365738.9973105835,7365540.9899205845,7365340.554960099,7365137.688065148,7364932.385994729,7364724.646680763,7364514.4692793535,7364301.8542232765,7364086.803275791,7363869.319585717,7363649.407743816,7363427.073840452,7363202.325524507,7362975.172063639,7362745.624405715,7362513.695241598,7362279.399069103,7362042.752258224,7361803.773117544,7361562.481961852,7361318.901180904,7361073.055309335,7360824.9710976435,7360574.677584277,7360322.206168709,7360067.5906855175,7359810.867479383,7359552.075480965,7359291.256283641,7359028.4542209245,7358763.716444676,7358497.093003855,7358228.636923874,7357958.404286389,7357686.454309471,7357412.849428063,7357137.655374626,7356860.941259856,7356582.779653368,7356303.246664245,7356022.422021275,7355740.389152847,7355457.235266225,7355173.051426199,7354887.932632874,7354601.977898459,7354315.290322906,7354027.977168224,7353740.149931243,7353451.924414749,7353163.4207966635,7352874.763697166,7352586.082243529,7352297.510132407,7352009.185689437,7351721.251925835,7351433.856591849,7351147.152226719,7350861.296205024,7350576.450779045,7350292.783116954,7350010.465336546,7349729.67453427,7349450.592809217,7349173.40728188,7348898.310107345,7348625.49848262,7348355.174647872,7348087.545881232,7347822.82448688,7347561.227776164,7347302.978041406,7347048.302522123,7346797.433363399,7346550.607566076,7346308.06692853,7346070.057979679,7345836.831903046,7345608.644451514,7345385.75585258,7345168.430703793,7344956.937858196,7344751.550299494,7344552.545006718,7344360.2028082255,7344174.808224781,7343996.649301593,7343826.01742908,7343663.207152288,7343508.515968778,7343362.2441149065,7343224.694340404,7343096.171671196,7342976.98316041,7342867.437627591,7342767.845386107,7342678.517958809,7342599.767782005,7342531.907897858,7342475.251635342,7342430.112279921,7342396.80273214,7342375.635155389,7342366.9206130775,7342370.96869555,7342388.087137058,7342418.581423204,7342462.754389229,7342520.905809629,7342593.331979573,7342680.325288662,7342782.173787591,7342899.1607483085,7343031.564218328,7343179.656569854,7343343.70404445,7343523.966293977,7343720.695918601,7343934.138002666,7344164.529649278,7344412.099514509,7344677.067342052,7344959.64349932,7345260.028515914,7345578.412625424,7345914.97531157,7346269.884859691,7346643.297914593,7347035.359045805,7347446.200321268,7347875.940890533,7348324.68657849,7348792.529490687,7349279.547631284,7349785.8045347,7350311.348911914,7350856.214312529,7351420.418803489,7352003.964665482,7352606.838107969,7353229.009003714,7353870.430643744,7354531.039513541,7355210.755091328,7355909.4796691155,7356627.0981973475,7357363.47815371,7358118.469436757,7358891.904284912,7359683.597221348,7360493.345025165,7361320.92672925,7362166.103645129,7363028.619415092,7363908.200091666,7364804.554244662,7365717.37309574,7366646.330680497,7367591.084037959,7368551.273427263,7369526.522571327,7370516.43892713,7371520.6139822,7372538.623576889,7373570.028251807,7374614.37361986,7375671.190762201,7376739.996647327,7377820.294572504,7378911.574626688,7380013.314173969,7381124.9783565495,7382246.0206162585,7383375.883233444,7384513.997882179,7385659.7862005355,7386812.660374737,7387972.023735975,7389137.271368487,7390307.790727737,7391482.962267233,7392662.160072702,7393844.752502222,7395030.102830958,7396217.569899078,7397406.508761512,7398596.271338115,7399786.207062939,7400975.663531149,7402163.987142321,7403350.523738744,7404534.619237438,7405715.620254594,7406892.874721227,7408065.732488761,7409233.545923436,7410395.670488374,7411551.465312182,7412700.293743126,7413841.523887791,7414974.529133333,7416098.688652443,7417213.387890141,7418318.019031678,7419411.981450785,7420494.682137641,7421565.536105965,7422623.966778667,7423669.4063516455,7424701.296135296,7425719.086873376,7426722.239039001,7427710.223107528,7428682.519806176,7429638.620340356,7430578.026596611,7431500.251322312,7432404.8182821255,7433291.262391473,7434159.129827232,7435007.978115888,7435837.376199574,7436646.904480307,7437436.154842971,7438204.730657439,7438952.2467604885,7439678.3294180585,7440382.616268468,7441064.756247374,7441724.4094950855,7442361.247247095,7442974.951708586,7443565.215913746,7444131.743570794,7444674.2488935795,7445192.456420699,7445686.100823085,7446154.926701016,7446598.688371565,7447017.149647476,7447410.0836085025,7447777.272366235,7448118.506823484,7448433.5864292625,7448722.318930421,7448984.520121039,7449220.013590604,7449428.630472107,7449610.209191046,7449764.595216524,7449891.640815405,7449991.20481068,7450063.152345031,7450107.354650741,7450123.688826916,7450112.037625096,7450072.289244332,7450004.337136676,7449908.079824162,7449783.420728289,7449630.268012949,7449448.5344418725,7449238.137251502,7448998.998040299,7448731.042675458,7448434.201217935,7448108.407866799,7447753.600923771,7447369.72277895,7446956.719918578,7446514.542955807,7446043.146685342,7445542.490162869,7445012.536810153,7444453.254546693,7443864.615948798,7443246.598436976,7442599.1844924735,7441922.361903827,7441216.124044294,7440480.470180972,7439715.405816451,7438920.943063834,7438097.101055905,7437243.906389281,7436361.393604288,7435449.605701384,7434508.594694845,7433538.422204459,7432539.160085951,7431510.8911008295,7430453.709626302,7429367.722405917,7428253.049341519,7427109.8243271,7425938.196125045,7424738.329285283,7423510.405107775,7422254.622648683,7420971.199770604,7419660.374237077,7418322.4048515875,7416957.572641183,7415566.182084738,7414148.562385843,7412705.068790161,7411236.083947069,7409742.019315209,7408223.316611561,7406680.44930346,7405113.924142915,7403524.282742416,7401912.103191324,7400278.0017117895,7398622.634352967,7396946.698722203,7395250.935751673,7393536.131498782,7391803.118978535,7390052.780025812,7388286.047185397,7386503.905627368,7384707.395085304,7382897.611814535,7381075.710567513,7379242.90658313,7377400.47758664,7375549.765796621,7373692.179935209,7371829.197237621,7369962.365456746,7368093.304858444,7366223.710202862,7364355.35270695,7362490.081983138,7360629.827948848,7358776.6027014,7356932.502352581,7355099.708816995,7353280.491548067,7351477.209215419,7349692.311317107,7347928.339720044,7346187.930121771,7344473.813426526,7342788.817028463,7341135.865994699,7339517.984140715,7337938.294990556,7336400.02261418,7334906.492334149,7333461.131293891,7332067.468879617,7330729.136988023,7329449.870131866,7328233.505375512,7327083.98209264,7326005.341538332,7325001.7262278665,7324077.379114689,7323236.642560202,7322483.957088168,7321823.859916803,7321260.983261871,7320800.052404401,7320445.883516985,7320203.381243015,7320077.536023633,7320073.421167578,7320196.18965973,7320451.0707045775,7320843.366001516,7321378.445749454,7322061.744378929,7322898.756010604,7323895.029639827,7325056.16404768,7326387.80243982,7327895.626815279,7329585.352068287,7331462.719827161,7333533.492035214,7335803.444279712,7338278.358875885,7340964.017714045,7343866.19487895,7346990.649051589,7350343.115704676,7353929.299104197,7357754.864130427,7361825.42793293,7366146.551435044,7370723.7307044435,7375562.388207322,7380667.863964727,7386045.406630468,7391700.164510907,7397637.176547787,7403861.3632859,7410377.517848225,7417190.296941638,7424304.211916905,7431723.61990709,7439452.715068814,7447495.519951124,7455855.87701681,7464537.440341081,7473543.667512449,7482877.811760469,7492542.914334712,7502541.797158926,7512877.055783837,7523551.0526613835,7534565.910762465,7545923.507559373,7557625.4693932,7569673.1662453255,7582067.706931035,7594809.934731996,7607900.4234829815,7621339.474126801,7635127.111749916,7649263.083109619,7663746.85466205,7678577.611098663,7693754.25439702,7709275.403390045,7725139.393856194,7741344.279131072,7757887.831239485,7774767.54254491,7791980.627911845,7809524.027374677,7827394.409305103,7845588.174068451,7864101.458157785,7882930.1387931155,7902069.838971603,7921515.932953392,7941263.552166308,7961307.591511611,7981642.716051837,8002263.368060818,8023163.7744150795,8044337.9543050295,8065779.727243698,8087482.721350206,8109440.381884687,8131645.980011036,8154092.621763581,8176773.257193649,8199680.689671945,8222807.585322671,8246146.48256553,8269689.801741871,8293429.854801648,8317358.855028223,8341468.926778448,8365752.115216146,8390200.396017533,8414805.685027927,8439559.84784974,8464454.709342524,8489482.063016668,8514633.68030315,8539901.319682682,8565276.735658407,8590751.687557338,8616317.948146582,8641967.312051378,8667691.603962941,8693482.686625063,8719332.468589377,8745232.911730176,8771176.038510537,8797153.938992562,8823158.777585357,8849182.799525306,8875218.337084098,8901257.815500781,8927293.758634945,8953318.794338958,8979325.659547955,9005307.205086963,9031256.40019533,9057166.336769221,9083030.233323663,9108841.438676143,9134593.435354447,9160279.84273181,9185894.419893125,9211431.068236278,9236883.833813207,9262246.909415651,9287514.63641088,9312681.506333144,9337742.162236722,9362691.399816902],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[13571818.382614976,13571818.364733297,13571818.346518867,13571818.327965477,13571818.309066834,13571818.289816504,13571818.270207945,13571818.2502345,13571818.229889361,13571818.209165629,13571818.18805625,13571818.16655405,13571818.14465172,13571818.122341817,13571818.099616753,13571818.076468812,13571818.052890113,13571818.028872646,13571818.004408248,13571817.979488604,13571817.95410524,13571817.928249529,13571817.901912682,13571817.875085745,13571817.847759604,13571817.819924964,13571817.79157237,13571817.762692181,13571817.73327458,13571817.70330957,13571817.672786966,13571817.641696388,13571817.610027274,13571817.577768851,13571817.544910168,13571817.511440048,13571817.477347117,13571817.442619784,13571817.40724625,13571817.371214485,13571817.33451225,13571817.297127066,13571817.259046229,13571817.220256794,13571817.180745583,13571817.140499158,13571817.099503849,13571817.057745716,13571817.015210574,13571816.971883962,13571816.92775115,13571816.882797156,13571816.837006683,13571816.790364178,13571816.742853794,13571816.694459379,13571816.645164484,13571816.59495236,13571816.543805948,13571816.49170786,13571816.438640393,13571816.384585517,13571816.329524858,13571816.273439707,13571816.216311006,13571816.158119341,13571816.098844942,13571816.03846766,13571815.976966986,13571815.914322017,13571815.85051147,13571815.785513664,13571815.719306514,13571815.65186752,13571815.583173765,13571815.51320192,13571815.441928199,13571815.369328395,13571815.29537784,13571815.220051406,13571815.143323498,13571815.065168057,13571814.985558527,13571814.904467855,13571814.821868505,13571814.737732405,13571814.652030982,13571814.564735113,13571814.47581515,13571814.385240884,13571814.292981548,13571814.199005801,13571814.103281725,13571814.005776798,13571813.906457908,13571813.805291317,13571813.70224266,13571813.59727694,13571813.490358505,13571813.381451042,13571813.270517562,13571813.15752039,13571813.042421145,13571812.925180746,13571812.805759382,13571812.684116486,13571812.560210753,13571812.434000116,13571812.305441704,13571812.174491875,13571812.041106159,13571811.905239265,13571811.766845062,13571811.625876565,13571811.482285911,13571811.336024348,13571811.187042218,13571811.035288949,13571810.880713018,13571810.723261956,13571810.562882308,13571810.399519641,13571810.233118497,13571810.063622398,13571809.89097381,13571809.715114137,13571809.535983697,13571809.353521688,13571809.167666193,13571808.978354136,13571808.785521276,13571808.589102179,13571808.389030192,13571808.185237434,13571807.977654748,13571807.766211713,13571807.550836587,13571807.331456307,13571807.107996449,13571806.880381202,13571806.648533367,13571806.412374292,13571806.171823876,13571805.926800534,13571805.67722116,13571805.423001122,13571805.164054198,13571804.900292583,13571804.631626837,13571804.357965862,13571804.079216877,13571803.795285368,13571803.506075086,13571803.211487979,13571802.911424192,13571802.605782013,13571802.294457853,13571801.977346191,13571801.654339558,13571801.325328493,13571800.990201507,13571800.648845049,13571800.301143454,13571799.946978936,13571799.586231496,13571799.218778942,13571798.844496805,13571798.46325832,13571798.07493436,13571797.679393426,13571797.276501572,13571796.866122382,13571796.44811691,13571796.022343637,13571795.588658433,13571795.146914497,13571794.696962312,13571794.238649605,13571793.771821266,13571793.296319343,13571792.811982943,13571792.318648212,13571791.816148259,13571791.304313112,13571790.78296966,13571790.25194159,13571789.711049328,13571789.160109999,13571788.59893733,13571788.027341621,13571787.445129672,13571786.8521047,13571786.248066306,13571785.63281039,13571785.006129077,13571784.367810668,13571783.717639556,13571783.055396147,13571782.38085681,13571781.693793792,13571780.993975127,13571780.281164592,13571779.555121593,13571778.815601125,13571778.06235365,13571777.295125045,13571776.5136565,13571775.717684455,13571774.906940483,13571774.081151225,13571773.240038298,13571772.383318182,13571771.510702165,13571770.621896205,13571769.716600876,13571768.794511234,13571767.85531674,13571766.898701144,13571765.924342398,13571764.93191254,13571763.921077577,13571762.8914974,13571761.842825659,13571760.774709648,13571759.686790189,13571758.578701543,13571757.45007124,13571756.300520003,13571755.129661618,13571753.937102776,13571752.722442992,13571751.485274438,13571750.22518184,13571748.941742321,13571747.634525284,13571746.30309225,13571744.946996745,13571743.565784138,13571742.158991497,13571740.72614745,13571739.26677202,13571737.780376494,13571736.266463231,13571734.72452554,13571733.154047498,13571731.554503795,13571729.925359553,13571728.266070182,13571726.57608118,13571724.854827974,13571723.101735737,13571721.316219209,13571719.497682504,13571717.645518932,13571715.759110812,13571713.837829268,13571711.881034022,13571709.888073232,13571707.85828325,13571705.790988429,13571703.685500916,13571701.541120436,13571699.357134081,13571697.132816073,13571694.867427554,13571692.560216356,13571690.210416768,13571687.817249304,13571685.379920458,13571682.89762246,13571680.369533047,13571677.79481519,13571675.172616856,13571672.50207074,13571669.78229401,13571667.01238804,13571664.191438125,13571661.318513237,13571658.392665716,13571655.412931003,13571652.37832735,13571649.287855523,13571646.140498517,13571642.935221246,13571639.670970239,13571636.346673345,13571632.96123939,13571629.513557892,13571626.002498718,13571622.426911755,13571618.78562659,13571615.077452159,13571611.301176427,13571607.455566006,13571603.53936585,13571599.551298857,13571595.490065519,13571591.354343582,13571587.142787633,13571582.854028763,13571578.486674156,13571574.039306726,13571569.510484708,13571564.898741279,13571560.202584138,13571555.420495117,13571550.550929766,13571545.59231693,13571540.54305833,13571535.401528146,13571530.166072583,13571524.83500942,13571519.406627584,13571513.87918671,13571508.250916667,13571502.520017125,13571496.68465708,13571490.742974393,13571484.69307532,13571478.533034043,13571472.260892175,13571465.87465829,13571459.372307418,13571452.751780584,13571446.010984264,13571439.147789914,13571432.16003347,13571425.045514805,13571417.801997246,13571410.427207042,13571402.918832831,13571395.274525156,13571387.491895868,13571379.568517674,13571371.501923522,13571363.289606139,13571354.92901742,13571346.417567952,13571337.752626404,13571328.931519024,13571319.95152908,13571310.809896301,13571301.503816333,13571292.030440181,13571282.38687366,13571272.57017685,13571262.57736351,13571252.405400576,13571242.051207567,13571231.511656055,13571220.78356911,13571209.863720758,13571198.748835435,13571187.435587445,13571175.920600427,13571164.200446816,13571152.271647332,13571140.130670438,13571127.773931837,13571115.19779396,13571102.398565467,13571089.372500757,13571076.115799459,13571062.624605998,13571048.895009115,13571034.923041398,13571020.704678867,13571006.235840535,13570991.512388004,13570976.530125063,13570961.284797322,13570945.772091826,13570929.987636734,13570913.927001003,13570897.58569405,13570880.959165521,13570864.042805009,13570846.831941824,13570829.321844807,13570811.507722156,13570793.384721268,13570774.947928647,13570756.192369826,13570737.11300931,13570717.704750601,13570697.962436207,13570677.880847739,13570657.454706032,13570636.678671302,13570615.547343379,13570594.055261953,13570572.196906922,13570549.966698743,13570527.358998895,13570504.368110335,13570480.988278106,13570457.213689921,13570433.038476901,13570408.456714308,13570383.462422425,13570358.049567468,13570332.212062592,13570305.943769015,13570279.238497183,13570252.090008065,13570224.492014553,13570196.438182924,13570167.922134452,13570138.937447121,13570109.47765744,13570079.536262408,13570049.106721554,13570018.182459194,13569986.75686674,13569954.823305171,13569922.375107734,13569889.40558266,13569855.908016162,13569821.875675533,13569787.30181243,13569752.17966637,13569716.502468346,13569680.26344471,13569643.455821203,13569606.07282722,13569568.10770028,13569529.553690722,13569490.404066635,13569450.65211903,13569410.291167246,13569369.31456462,13569327.715704465,13569285.488026222,13569242.625022035,13569199.120243479,13569154.967308706,13569110.159909843,13569064.691820744,13569018.556905057,13568971.74912467,13568924.262548484,13568876.091361575,13568827.229874728,13568777.672534391,13568727.413933,13568676.448819753,13568624.772111809,13568572.378905926,13568519.26449057,13568465.424358496,13568410.854219802,13568355.550015518,13568299.507931685,13568242.72441397,13568185.196182845,13568126.920249302,13568067.893931178,13568008.114870047,13567947.581048734,13567886.290809477,13567824.242872674,13567761.436356371,13567697.870796353,13567633.546166975,13567568.462902702,13567502.621920345,13567436.024642086,13567368.673019245,13567300.569556817,13567231.717338862,13567162.120054636,13567091.782025633,13567020.708233431,13566948.904348416,13566876.37675943,13566803.132604294,13566729.179801246,13566654.527081372,13566579.184021937,13566503.161080731,13566426.469631393,13566349.12199976,13566271.131501205,13566192.512479065,13566113.280344076,13566033.45161493,13565953.043959847,13565872.076239318,13565790.568549901,13565708.54226917,13565626.020101763,13565543.026126629,13565459.585845338,13565375.726231623,13565291.475782055,13565206.86456788,13565121.92428803,13565036.688323325,13564951.191791827,13564865.471605359,13564779.566527238,13564693.517231124,13564607.366361033,13564521.158592537,13564434.940695055,13564348.761595285,13564262.67244175,13564176.72667042,13564090.980071405,13564005.490856694,13563920.319728887,13563835.52995096,13563751.187416919,13563667.360723395,13563584.121242126,13563501.543193234,13563419.703719294,13563338.68296015,13563258.56412835,13563179.433585236,13563101.380917555,13563024.499014536,13562948.884145398,13562874.636037117,13562801.857952513,13562730.656768383,13562661.143053755,13562593.431148086,13562527.639239253,13562463.889441358,13562402.307872063,13562343.0247295,13562286.174368482,13562231.89537601,13562180.330645809,13562131.627451861,13562085.93752073,13562043.417102475,13562004.227040129,13561968.532837383,13561936.504724493,13561908.317722062,13561884.151702637,13561864.191449847,13561848.626714924,13561837.652270403,13561831.467960749,13561830.278749766,13561834.294764532,13561843.731335605,13561858.809033344,13561879.75370006,13561906.796477795,13561940.17383144,13561980.12756704,13562026.904844943,13562080.758187626,13562141.945481904,13562210.72997531,13562287.380266367,13562372.170288531,13562465.379287547,13562567.291791968,13562678.19757663,13562798.391618786,13562928.17404673,13563067.850080635,13563217.729965407,13563378.12889533,13563549.366930293,13563731.768903393,13563925.664319761,13564131.387246367,13564349.276192678,13564579.67398204,13564822.92761355,13565079.388114408,13565349.41038252,13565633.353019385,13565931.578153053,13566244.451251205,13566572.340924269,13566915.61871856,13567274.658899445,13567649.838224594,13568041.53570733,13568450.132370172,13568876.010988697,13569319.555825794,13569781.152356561,13570261.186983932,13570760.046745317,13571278.119010488,13571815.791170962,13572373.450321225,13572951.482932113,13573550.274516694,13574170.209289145,13574811.669816913,13575475.036666768,13576160.688045127,13576868.999433273,13577600.343217922,13578355.08831783,13579133.599806963,13579936.238534939,13580763.360745398,13581615.317692949,13582492.455259498,13583395.113570655,13584323.626612961,13585278.321852846,13586259.519857936,13587267.533921745,13588302.669692406,13589365.22480645,13590455.488528382,13591573.741397008,13592720.254879342,13593895.291032985,13595099.102177903,13596331.930578422,13597594.008136362,13598885.55609615,13600206.78476281,13601557.893233636,13602939.069144381,13604350.488430828,13605792.315106438,13607264.701056918,13608767.78585241,13610301.696578003,13611866.547683248,13613462.440851316,13615089.464888368,13616747.695633737,13618437.195891375,13620158.01538307,13621910.190723866,13623693.745419962,13625508.689889533,13627355.02150662,13629232.724668354,13631141.770885646,13633082.118897447,13635053.714808572,13637056.492251106,13639090.37256929,13641155.265027717,13643251.067042692,13645377.664436389,13647534.931713613,13649722.732360642,13651940.919165846,13654189.33456145,13656467.810986025,13658776.171266956,13661114.229022365,13663481.789081674,13665878.647924114,13668304.594134351,13670759.408874381,13673242.866370814,13675754.734416617,13678294.77488635,13680862.744263921,13683458.394181797,13686081.47197066,13688731.721218424,13691408.882337479,13694112.693139134,13696842.88941405,13699599.205517584,13702381.37495888,13705189.13099258,13708022.20721197,13710880.338142484,13713763.259834388,13716670.7104535,13719602.430868896,13722558.165236443,13725537.661577119,13728540.672349032,13731566.95501214,13734616.272584612,13737688.3941899,13740783.095593547,13743900.15972881,13747039.377210231,13750200.546834324,13753383.476066487,13756587.981513547,13759813.889380982,13763061.035914354,13766329.267824158,13769618.442693582,13772928.429368593,13776259.108329834,13779610.372045917,13782982.125307655,13786374.285542892,13789786.783111611,13793219.561581073,13796672.577980679,13800145.803036517,13803639.221385317,13807152.831767833,13810686.647201572,13814240.695132837,13817815.017568225,13821409.671185564,13825024.727424474,13828660.272556756,13832316.407736689,13835993.249031674,13839690.92743333,13843409.58884948,13847149.394077301,13850910.518758113,13854693.153314134,13858497.502867714,13862323.7871435,13866172.24035405,13870043.111069387,13873936.66207112,13877853.170191633,13881792.92613898,13885756.23430813,13889743.41257909,13893754.792102724,13897790.717074769,13901851.544498833,13905937.643939037,13910049.39726298,13914187.198375773,13918351.452945847,13922542.578123227,13926761.002251053,13931007.164571041,13935281.514923606,13939584.513443423,13943916.630251095,13948278.345141696,13952670.14727091,13957092.534839433,13961546.014776405,13966031.102422543,13970548.321213651,13975098.20236521,13979681.284558712,13984298.113630362,13988949.242262827,13993635.229680628,13998356.64134979,14003114.048682343,14007908.028746227,14012739.163981188,14017608.041921167,14022515.254923696,14027461.399906814,14032447.078093931,14037472.894767141,14042539.459029343,14047647.383575622,14052797.284474218,14057989.780957501,14063225.495223166,14068505.052246066,14073829.07960089,14079198.207295926,14084613.067618197,14090074.294990059,14095582.52583757,14101138.39847063,14106742.55297513,14112395.631117111,14118098.276259085,14123851.133288449,14129654.848558145,14135510.06983942,14141417.44628674,14147377.628414748,14153391.268087225,14159459.018517876,14165581.534282872,14171759.47134496,14177993.487088919,14184284.240368217,14190632.391562613,14197038.602646405,14203503.537267108,14210027.860834207,14216612.240617692,14223257.345856007,14229963.847873023,14236732.420203691,14243563.73872788,14250458.481812045,14257417.330458147,14264440.96845947,14271530.082562711,14278685.362635864,14285907.501841318,14293197.196813626,14300555.147841284,14307982.059051946,14315478.638600398,14323045.598858623,14330683.65660728,14338393.533227842,14346175.954894707,14354031.652766459,14361961.363175549,14369965.827815542,14378045.793925134,14386202.014468085,14394435.248308143,14402746.260378161,14411135.821842408,14419604.71025117,14428153.709686691,14436783.610899495,14445495.21143404,14454289.315742796,14463166.735287596,14472128.288627358,14481174.801491022,14490307.106834676,14499526.044881824,14508832.463145657,14518227.216432272,14527711.166823743,14537285.183639936,14546950.143377945,14556706.929628082,14566556.43296532,14576499.550815037,14586537.18729208,14596670.25301197,14606899.664873255,14617226.34580992,14627651.224512862,14638175.235119417,14648799.31686995,14659524.413730556,14670351.47398097,14681281.449766813,14692315.29661529,14703453.9729136,14714698.439349247,14726049.658311611,14737508.593254074,14749076.208016172,14760753.466105202,14772541.329936843,14784440.760034418,14796452.71418645,14808578.146562316,14820818.00678581,14833173.238966575,14845644.780689407,14858233.561961563,14870940.504118277,14883766.518686807,14896712.506209422,14909779.355025839,14922967.940015823,14936279.121302558,14949713.742917834,14963272.631429868,14976956.594534952,14990766.41961419,15004702.872256558,15018766.69474988,15032958.604541281,15047279.292668825,15061729.422166279,15076309.626442889,15091020.507640386,15105862.634969411,15120836.543027744,15135942.730102835,15151181.656461287,15166553.742627978,15182059.367657717,15197698.867402384,15213472.532776672,15229380.608025562,15245423.288996834,15261600.72142204,15277912.999209315,15294360.16275166,15310942.197254255,15327659.03108454,15344510.534148758,15361496.5162988,15378616.725773167,15395870.847675921,15413258.502497477,15430779.24468122,15448432.561239718,15466217.870424546,15484134.520453475,15502181.78829891,15520358.878541341,15538664.922291514,15557098.976185001,15575660.021452682,15594346.963070657,15613158.628992908,15632093.769469963,15651151.056456674,15670329.083112031,15689626.363393864,15709041.331750983,15728572.342915298,15748217.67179611,15767975.513478633,15787843.98332864,15807821.117204782,15827904.87178002,15848093.124973347,15868383.67649262,15888774.24848927,15909262.486325204,15929845.959452085,15950522.162402865,15971288.515895126,15992142.368045643,16013080.995695151,16034101.605842149,16055201.337184256,16076377.261765365,16097626.386726556,16118945.65615853,16140331.953052979,16161782.10135015,16183292.868079517,16204860.965590348,16226483.053868612,16248155.742936542,16269875.595330928,16291639.128655983,16313442.818206515,16335283.099656891,16357156.371811181,16379058.999409653,16400987.315986747,16422937.626775466,16444906.211653039,16466889.328122681,16488883.21432609,16510884.09208139,16532888.16994111,16554891.646264778,16576890.712300763,16598881.555271903,16620860.361459568,16642823.319280827,16664766.622353395,16686686.47254321,16708579.082989434,16730440.68110189,16752267.511526031,16774055.83907059,16795801.951593287,16817502.162840102,16839152.815233674,16860750.28260675,16882290.972876593,16903771.330656555,16925187.8398012,16946537.025881495,16967815.458586928,16989019.75405142,17010146.577100415,17031192.643416334,17052154.72162026,17073029.635267586,17093814.26475572,17114505.54914226,17135100.48787208,17155596.142412093,17175989.637792774,17196278.16405548,17216458.97760518,17236529.402468007,17256486.831453674,17276328.727222603,17296052.623258144,17315656.124744218,17335136.909348976,17354492.727915373,17373721.405059412,17392820.839677397,17411789.005363222,17430623.95073732,17449323.799688697,17467886.751531724,17486311.081079606],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","showlegend":true,"type":"scattergl","x":[0.001,0.0010186101701559753,0.0010375666787451859,0.0010568759711848039,0.001076544612842316,0.0010965792912678099,0.0011169868184678225,0.0011377741332214914,0.0011589483034398105,0.0011805165285688056,0.0012024861420374122,0.0012248646137509307,0.0012476595526308698,0.0012708787092020582,0.001294529978227916,0.0013186214013947485,0.0013431611700460153,0.001368157627967472,0.0013936192742241421,0.0014195547660501016,0.0014459729217920197,0.0014728827239075019,0.0015002933220192183,0.0015282140360258693,0.001556654359271062,0.001585623961771137,0.0016151326935030914,0.0016451905877536625,0.0016758078645307671,0.001706994934038408,0.0017387624002162504,0.0017711210643450886,0.0018040819287193828,0.0018376562003881705,0.0018718552949655793,0.001906690840512252,0.0019421746814890265,0.001978318882784164,0.0020151357338155586,0.002052637752709252,0.00209083769055575,0.002129748535745521,0.002169383518385184,0.002209756114795903,0.002250880052095462,0.002292769312865649,0.002335438139906479,0.0023789010410788934,0.0024231727942376005,0.0024682684522556926,0.0025142033481427967,0.002560993100258459,0.002608653617622548,0.0026572011053245066,0.0027066520700332413,0.0027570233256095826,0.0028083319988231725,0.002860595535175742,0.0029138317048327885,0.0029680586086656023,0.0030232946844057766,0.0030795587129142264,0.0031368698245668766,0.0031952475057592136,0.003254711605531848,0.0033152823423194234,0.0033769803108250913,0.0034398264890229246,0.003503842245290676,0.0035690493456752297,0.0036354699612933176,0.003703126675869927,0.0037720424934169976,0.003842240846055061,0.003913745601980384,0.003986581073580439,0.004060772025700365,0.004136343684063274,0.004213321743847289,0.004291732378422158,0.004371602248248502,0.004452958509942656,0.004535828825510187,0.004620241371751313,0.004706224849841282,0.004793808495089107,0.00488302208687788,0.004973895958790063,0.005066461008921269,0.005160748710385908,0.005256791122018419,0.005354620899273608,0.005454271305329836,0.005555776222398878,0.005659170163246243,0.005764488282925874,0.005871766390733255,0.005981040962380944,0.006092349152400711,0.0062057288067765,0.0063212184758124484,0.006438857427240419,0.0065586856595714355,0.006680743915695614,0.006805073696735207,0.006931717276155407,0.0070607177141377726,0.007192118872221193,0.00732596542821523,0.007462302891391108,0.00760117761795533,0.007742636826811269,0.007886728615614156,0.008033501977124734,0.008183006815867389,0.008335293965098196,0.008490415204088747,0.008648423275731726,0.00880937190447399,0.00897331581458352,0.009140310748756233,0.009310413487069076,0.009483681866285927,0.009660174799522647,0.009839952296278227,0.010023075482838654,0.010209606623060466,0.010399609139541197,0.0105931476351837,0.010790287915161841,0.010991097009294973,0.011195643194838782,0.011403996019700324,0.011616226326085019,0.011832406274583786,0.012052609368708425,0.012276910479883591,0.012505385872903908,0.012738113231864785,0.012975171686575875,0.013216641839466052,0.013462605792989104,0.013713147177539449,0.013968351179887397,0.014228304572143526,0.014493095741262165,0.014762814719093903,0.015037553212997377,0.015317404637020799,0.015602464143663687,0.01589282865622978,0.016188596901781985,0.016489869444710648,0.01679674872092653,0.017109339072690143,0.01742774678408919,0.017752080117176352,0.018082449348779516,0.01841896680799711,0.018761746914391204,0.01911090621689138,0.019466563433422623,0.019828839491270712,0.020197857568198783,0.020573743134329126,0.02095662399480433,0.021346630333242442,0.0217438947560008,0.022148552337263594,0.022560740664968604,0.02298059988758851,0.023408272761782933,0.023843904700937203,0.024287643824604518,0.024739641008868128,0.025200049937640922,0.025669027154919505,0.02614673211801092,0.02663332725174982,0.027128978003724658,0.027633852900531698,0.0281481236050758,0.028671964974937698,0.029205555121827466,0.029749075472144407,0.030302710828663964,0.03086664943337273,0.031441083031472646,0.03202620693657652,0.0326222200971167,0.033229325163989715,0.03384772855945981,0.03447764054734464,0.03511927530450729,0.03577285099367873,0.03643858983763545,0.03711671819475765,0.03780746663599349,0.03851107002325571,0.03922776758927719,0.039957803018952694,0.040701424532194365,0.04145888496832911,0.042230441872066746,0.04301635758106795,0.043816899315141926,0.04463233926710395,0.04546295469532399,0.04630902801799739,0.04717084690917017,0.04804870439655132,0.048942898961145294,0.049853734638738934,0.05078152112327673,0.05172657387216019,0.052689214213506745,0.05366976945540476,0.054668572997201806,0.05568596444286412,0.05672228971644543,0.05777790117970504,0.058853157751914506,0.05994842503189409,0.061064075422320396,0.062200488256347115,0.0633580499265825,0.06453715401646702,0.06573820143409585,0.06696160054853219,0.06820776732865685,0.06947712548460236,0.0707701066118189,0.07208715033782136,0.07342870447166762,0.07479522515621821,0.07618717702322995,0.07760503335133571,0.07904927622696424,0.08052039670825474,0.08201889499202203,0.08354528058382867,0.08510007247122246,0.08668379930019779,0.08829699955494087,0.08994022174092044,0.09161402457138516,0.0933189771573324,0.09505565920101196,0.09682466119303124,0.0986265846131282,0.10046204213468131,0.10233165783302449,0.10423606739764012,0.10617591834830001,0.10815187025522881,0.1101645949633657,0.11221477682079803,0.11430311291144786,0.11643031329208768,0.11859710123376695,0.12080421346773289,0.12305240043592616,0.12534242654613995,0.12767507043192658,0.13005112521734086,0.13247139878661174,0.13493671405883065,0.13744790926775366,0.14000583824680976,0.14261137071941282,0.14526539259467813,0.14796880626863962,0.15072253093107554,0.15352750287804226,0.1563846758302246,0.1592950212572123,0.16225952870780871,0.16527920614648955,0.16835508029612023,0.17148819698705392,0.17467962151272456,0.17793043899185773,0.18124175473742377,0.18461469463245475,0.18805040551285815,0.1915500555573528,0.19511483468466165,0.19874595495809838,0.2024446509976804,0.20621218039991424,0.21004982416539153,0.21395888713434216,0.2179406984302956,0.2219966119119955,0.22612800663372773,0.23033628731421313,0.23462288481422625,0.23898925662310502,0.24343688735431104,0.24796728925021577,0.25258200269627845,0.2572825967447932,0.26207066964838527,0.2669478494034321,0.2719157943036019,0.27697619350368907,0.28213076759394706,0.28738126918510665,0.2927294835042816,0.29817722900196736,0.30372635797033115,0.30937875717301366,0.31513634848664795,0.32100108955431716,0.3269749744511768,0.33306003436245885,0.3392583382740992,0.34557199367621394,0.3520031472796679,0.3585539857459817,0.36522673643081754,0.3720236681413066,0.3789470919074668,0.3859993617679767,0.393182875570577,0.40050007578736113,0.4079534503452449,0.41554553347188755,0.4232789065573549,0.43115619903182284,0.4391800892596086,0.4473533054498463,0.4556786265841064,0.46415888336127775,0.47279695916003905,0.4815957910192351,0.49055837063650454,0.4996877453854884,0.508987019351968,0.5184593543892912,0.5281079711934331,0.5379361503980703,0.5479472336900287,0.5581446249454961,0.5685317913873753,0.5791122647641759,0.58988964255085,0.6008675891719687,0.6120498372476697,0.6234401888627864,0.6350425168595962,0.6468607661546327,0.658898955079995,0.6711611767496279,0.6836516004510238,0.6963744730628222,0.7093341204987996,0.7225349491787214,0.7359814475265763,0.7496781874966877,0.7636298261282242,0.7778411071286491,0.7923168624866254,0.8070620141149499,0.822081575524054,0.8373806535266489,0.8529644499741025,0.8688382635251184,0.8850074914473438,0.9014776314524917,0.9182542835656282,0.9353431520292387,0.952750047242729,0.9704808877380307,0.9885417021919574,1.0069386314760271,1.025677930744422,1.0447659715608042,1.0642092440647246,1.0840143591783309,1.1041880508541602,1.124737178364752,1.1456687286348715,1.1669898186171475,1.1887076977119033,1.2108297502320393,1.233363497913776,1.2563166024741201,1.2796968682159415,1.3035122446815088,1.3277708293554291,1.3524808704178755,1.3776507695490536,1.4032890847858732,1.429404533431761,1.4560059950206485,1.4831025143361045,1.510703304486654,1.5388177500383464,1.567455410205595,1.5966260221014252,1.6263395040481923,1.6566059589499136,1.6874356777273758,1.7188391428171457,1.750827031735725,1.783410220710008,1.8165997883753267,1.8504070195423021,1.8848434090337953,1.9199206655932848,1.955650715865949,1.9920457084538692,2.029118018046678,2.066880249629082,2.105345242766706,2.1445260759716676,2.184436071149426,2.2250887981283696,2.266498079273693,2.30867799418717,2.3516428844943484,2.395407358720877,2.43998629725955,2.4853948574297986,2.5316484786313556,2.578762887593801,2.6267541037238358,2.675638444552045,2.7254325312810277,2.776153294436801,2.8278179796253413,2.8804441533962977,2.934049709215787,2.988652873550383,3.044272212064303,3.1009266359319265,3.158635408267819,3.2174181506763717,3.277294849923382,3.338285864731761,3.400411932703706,3.4636941773717345,3.528154115380883,3.593813663804626,3.6606951475969023,3.7288213071828338,3.798215306190736,3.8689007393279757,3.940901640403448,4.014242490499322,4.08894822629486,4.165044248545185,4.242556430717777,4.321511127789762,4.401935185208875,4.483855948021186,4.5673012701687465,4.652299523960189,4.738879609717651,4.827070965603183,4.916903577628026,5.008407989848212,5.101615314749834,5.196557243827657,5.293266058360562,5.3917746403875,5.49211648388779,5.594325706169378,5.698437059469142,5.804485942768978,5.912508413831875,6.0225412014619275,6.134621717992506,6.248788072006894,6.365079081295571,6.483534286054721,6.604193962330306,6.727099135712336,6.852291595284065,6.9798139078306605,7.109709432312432,7.242022334607316,7.376797602527731,7.51408106111697,7.653919388230148,7.796360130405229,7.94145171902934,8.089243486805938,8.23978568452852,8.393129498166365,8.549327066268376,8.708431497690723,8.870496889654403,9.03557834613893,9.20373199661822,9.375015015145289,9.549485639791966,9.727203192450537,9.908228099003798,10.092621909870484,10.280447320933098,10.471768194855203,10.666649582795388,10.865157746525373,11.067360180959746,11.273325637104872,11.483124145435111,11.696827039703846,11.914506981197748,12.136237983442417,12.36209543736769,12.59215613694151,12.826498305280598,13.0652016212472,13.30834724654076,13.556017853293689,13.808297652180924,14.065272421052365,14.327029534098294,14.593657991557576,14.865248449978571,15.14189325304352,15.423686462966273,15.710723892474489,16.00310313738702,16.30092360979741,16.604286571875296,16.913295170296472,17.22805447131392,17.54867149648152,17.875255259042355,18.207916800994624,18.546769230846976,18.891927762076644,19.24350975230332,19.601634743191855,19.966424501097933,20.3380030584698,20.716496756020668,21.102034285685967,21.494746734379806,21.894767628566207,22.30223297965936,22.717281330269028,23.14005380130654,23.570694139967276,24.009348768606518,24.456166834524442,24.911300260677912,25.374903797335715,25.847135074695636,26.3281546564802,26.81812609453013,27.317215984413792,27.825594022071257,28.343433061513092,28.870909173592345,29.408201705870606,29.955493343598164,30.51297017182871,31.080821738690638,31.659241119835205,32.24842498408439,32.84857366030047,33.45989120549975,34.08258547423452,34.7168681892656,35.36295501355039,36.021065623570735,36.69142378402494,37.37425742391064,38.06979871402284,38.77828414589453,39.49995461220647,40.23505548869293,40.983836717572615,41.74655289253135,42.52346334528678,43.31483223376403,44.1209286319119,44.942026621191424,45.77840538376616,46.630349297427266,47.498148032285044,48.38209664925957,49.282495700405136,50.199651331100796,51.13387538414321,52.08548550577665,53.05480525369574,54.04216420705915,55.04789807854968,56.07234882852027,57.11586478126435,58.17880074344935,59.261518124755526,60.364385060758636,61.48777653810017,62.63207452198692,63.79766808606282,64.98495354469888,66.19433458774388,67.42622241778335,68.68103588995308,69.95920165435375,71.26115430111746,72.58733650817246,73.93819919175873,75.31420165974376,76.71581176779303,78.14350607844543,79.59777002314978,81.07909806731695,82.58799387844272,84.12497049736119,85.69055051268347,87.2852662384837,88.90965989529167,90.56428379445295,92.24970052592174,93.9664831495469,95.71521538991855,97.49649183484097,99.310918137498,101.15911122238298,103.04169949505875,104.95932305582267,106.91263391734772,108.90229622637305,110.92898648952227,112.99339380332216,115.09622008850312,117.23818032865998,119.42000281335325,121.6424293857368,123.90621569479157,126.21213145225461,128.56096069432965,130.95350204826676,133.39056900390588,135.8729901902709,138.401609657313,140.97728716289677,143.60089846512608,146.273335620113,148.99550728528536,151.7683390283404,154.59277364194784,157.46977146430868,160.400310705682,163.38538778098604,166.42601764859018,169.52323415541213,172.6780903884356,175.89165903277325,179.16503273638995,182.49932448161505,185.89566796356883,189.35521797562953,192.87915080207776,196.46866461804444,200.1249798969035,203.84933982524643,207.64301072557748,211.50728248687946,215.44346900318823,219.45290862033113,223.53696459097966,227.697025538168,231.93450592744276,236.2508465477945,240.64751500154216,245.126006203334,249.68784288843267,254.33457613046482,259.0677858688006,263.8890814457513,268.80010215376075,273.80251779278575,278.89802923804393,284.0883690183301,289.37530190509534,294.7606255124859,300.2461709085549,305.83380323784314,311.52542235554847,317.32296347349796,323.2283978181381,329.2437333007769,335.3710152002929,341.6123268585525,347.9697903887695,354.44556739704353,361.04185971733375,367.7609101601031,374.60500327489893,381.57646612712523,388.6776690892668,395.91102664684587,403.2789982193705,410.78408899656426,418.42885079015844,426.2158829015325,434.14783300550926,442.2273980505897,450.45732517594536,458.84041264547614,467.37951079924636,476.0775230226368,484.9374067335233,493.96217438783157,503.1548945038057,512.5186927053333,522.0567527846976,531.7723177850967,541.6686911033147,551.7492376129129,562.0173848083188,572.4766239702178,583.1305113526219,593.9826693920351,605.0367879391224,616.2966255132942,627.76601058065,639.4488428556937,651.3490946272796,663.4708121092351,675.818116816111,688.3952069645496,701.2063589007176,714.2559285543119,727.5483529196233,741.088151564157,754.8799281653431,768.9283720758306,783.2382599179205,797.8144572076629,812.6619200091945,827.7856966198473,843.1909292866251,858.8828559546258,874.8668120479914,891.1482322840202,907.7326525210224,924.6257116405734,941.833153464796,959.3608287093147,977.2146969725725,995.4008287621518,1013.9254075588143,1032.7947319189525,1052.0152176161591,1071.5933998226712,1091.535935331391,1111.8496048192699,1132.5413151528126,1153.6181017364786,1175.0871309048075,1196.9557023590428,1219.2312516491095,1241.9213527017846,1265.0337203959039,1288.576213185518,1312.5568357718428,1336.9837418249451,1361.8652367560828,1387.209780541621,1413.0259905995338,1439.3226447194065,1466.108684046983,1493.3932161242533,1521.1855179861047,1549.4950393146316,1578.3314056521165,1607.704421673822,1637.624074521689,1668.100537200059,1699.144172034626,1730.765534195724,1762.9753752872039,1795.7846470020968,1829.2045048462937,1863.2463119315598,1897.9216428390996,1933.2422875550433,1969.2202554791734,2005.867779508234,2043.1973201952705,2081.2215699863373,2119.953457536069,2159.406152103568,2199.593068030075,2240.527869300018,2282.2244741868963,2324.6970599856454,2367.9600678330785,2412.028207618007,2456.91646298279,2502.6400964179165,2549.2146544514203,2596.6559729348724,2644.9801824277197,2694.203713681882,2744.3433032283624,2795.4159990678595,2847.4391664672476,2900.4304938639916,2954.40799888038,3009.3900344497183,3065.39529505653,3122.442823092858,3180.55201533292,3239.7426295281953,3300.034791125282,3361.4490001087684,3424.0061379714257,3487.7274748141776,3552.6346765781395,3618.7498124112767,3686.0953621721615,3754.694224073337,3824.5697224669993,3895.745615775501,3968.2461045694777,4042.09583979631,4117.319931161679,4193.943955667186,4271.993966306776,4351.496500925045,4432.4785912404,4514.967772036101,4598.992090522438,4684.5801158730455,4771.7609489387405,4860.564232142139,4951.020159556351,5043.159487171359,5137.013543351339,5232.6142394866565,5329.994080844093,5429.186177618943,5530.224256192901,5633.142670601352,5737.9764142141275,5844.761131633638,5953.53313081437,6064.329395408062,6177.187597338489,6292.146109610338,6409.244019356457,6528.521141127847,6650.018030431118,6773.775997517745,6899.837121430011,7028.244264308352,7159.041085964888,7292.272058728313,7427.982482564911,7566.218500481047,7707.027114212304,7850.4562002045095,7996.554525892347,8145.371766280737,8296.958520834914,8451.366330684721,8608.647696149244,8768.856094587427,8932.04599858096,9098.272894455567,9267.593301146882,9440.064789417604,9615.746001432095,9794.696670695386,9976.977642363212,10162.650893929951,10351.779556301763,10544.427935261685,10740.661533334323,10940.547072057436,11144.152514667881,11351.547089209991,11562.801312073754,11777.98701197118,11997.177354358855,12220.446866314887,12447.871461879062,12679.52846786434,12915.496650148827,13155.856240457053,13400.688963639506,13650.078065460139,13904.108340900697,14162.866162991973,14426.439512181589,14694.918006248172,14968.392930772556,15246.95727017573,15530.705739334584,15819.734815786014,16114.142772530198,16414.029711444666,16719.497597319885,17030.650292528426,17347.593592339326,17670.435260889466,17999.285067824763,18334.254825622887,18675.45842761074,19023.01188668946,19377.03337477989,19737.643263002556,20104.96416260497,20479.120966650833,20860.24089248505,21248.45352498883,21643.890860640204,22046.687352394074,22456.979955397717,22874.90817355704,23300.614106969246,23734.242500238663,24175.940791691282,24625.85916350544,25084.15059277541,25550.97090352507,26026.478819690044,26510.836019085364,27004.2071883777,27506.760079080675,28018.665564591953,28540.097698292375,29071.233772725755,29612.254379880374,30163.343472591972,30724.688427090034,31296.480106707506,31878.912926776426,32472.18492073132,33076.49780744242,33692.05705980267,34319.07197459043,34957.75574363272,35608.325526292814,36271.00252330648,36946.01205199302,37633.58362286533,38333.951017665975,39047.35236885564,39774.03024058037,40514.23171114647,41268.20845702952,42036.21683844709,42818.51798652415,43615.377892080054,44427.06749606883,45253.86278170167,46096.04486828429,46953.900106800626,47827.72017727485,48717.80218794631,49624.44877628914,50547.96821191235,51488.674501374975,52446.88749495119,53422.932995383526,54417.14286865888,55429.855156846636,56461.414193036726,57512.170718416135,58582.48200152536,59672.711959733104,60783.231282972236,61914.41755977848,63066.65540567406,64240.33659394191,65435.86018883229,66653.63268124907,67894.06812696112,69157.58828738525,70444.62277299038,71755.6091893692,73090.99328602903,74451.22910795143,75836.7791499719,77248.114514034,78685.71506936844,80150.06961565396,81641.67604921472,83161.04153230961,84708.68266557403,86285.12566366886,87890.90653419963,89526.57125996401,91192.67598459298,92889.78720164497,94618.48194721992,96379.34799615796,98172.9840618884,100000],"xaxis":"x","y":[15428296.394183319,15428296.441180348,15428296.489052014,15428296.537814565,15428296.587484581,15428296.638078976,15428296.689614914,15428296.742109952,15428296.795581924,15428296.850049002,15428296.90552972,15428296.962042939,15428297.01960787,15428297.078244073,15428297.137971513,15428297.198810467,15428297.260781633,15428297.323906101,15428297.388205297,15428297.453701114,15428297.520415798,15428297.588372065,15428297.657592973,15428297.728102084,15428297.799923379,15428297.87308127,15428297.947600612,15428298.02350677,15428298.100825539,15428298.17958319,15428298.259806538,15428298.34152283,15428298.424759872,15428298.509545933,15428298.595909877,15428298.683881031,15428298.773489334,15428298.864765236,15428298.957739776,15428299.052444572,15428299.148911808,15428299.247174302,15428299.347265454,15428299.44921928,15428299.553070461,15428299.658854319,15428299.766606778,15428299.876364512,15428299.98816482,15428300.102045733,15428300.21804594,15428300.336204894,15428300.456562782,15428300.5791605,15428300.70403975,15428300.831242979,15428300.960813446,15428301.092795197,15428301.227233097,15428301.364172868,15428301.503661063,15428301.645745113,15428301.79047331,15428301.937894873,15428302.088059919,15428302.2410195,15428302.396825623,15428302.555531275,15428302.717190377,15428302.881857928,15428303.049589906,15428303.220443323,15428303.394476285,15428303.571747936,15428303.752318569,15428303.936249565,15428304.123603461,15428304.314443955,15428304.508835921,15428304.706845473,15428304.908539891,15428305.113987794,15428305.323258998,15428305.53642466,15428305.753557246,15428305.974730596,15428306.200019885,15428306.42950171,15428306.663254093,15428306.90135652,15428307.143889917,15428307.390936757,15428307.642581,15428307.898908244,15428308.160005596,15428308.425961846,15428308.69686739,15428308.972814351,15428309.253896538,15428309.540209496,15428309.831850577,15428310.128918933,15428310.431515547,15428310.739743292,15428311.053706966,15428311.3735133,15428311.699271018,15428312.03109087,15428312.369085664,15428312.713370282,15428313.064061796,15428313.421279414,15428313.785144594,15428314.15578102,15428314.533314683,15428314.917873953,15428315.309589546,15428315.70859464,15428316.11502486,15428316.529018397,15428316.950715978,15428317.380260944,15428317.817799345,15428318.263479916,15428318.717454152,15428319.179876402,15428319.650903843,15428320.130696608,15428320.619417801,15428321.117233561,15428321.624313107,15428322.140828814,15428322.666956266,15428323.202874338,15428323.748765187,15428324.304814374,15428324.871210929,15428325.448147386,15428326.035819858,15428326.634428116,15428327.244175643,15428327.86526971,15428328.497921443,15428329.14234588,15428329.798762089,15428330.467393195,15428331.148466473,15428331.84221343,15428332.548869895,15428333.268676043,15428334.001876574,15428334.748720672,15428335.50946223,15428336.284359802,15428337.073676797,15428337.87768148,15428338.696647149,15428339.530852161,15428340.380580068,15428341.246119678,15428342.127765166,15428343.0258162,15428343.940578021,15428344.872361537,15428345.821483454,15428346.788266333,15428347.773038773,15428348.776135487,15428349.797897357,15428350.838671684,15428351.89881218,15428352.978679141,15428354.078639582,15428355.199067311,15428356.340343134,15428357.502854899,15428358.686997682,15428359.893173924,15428361.121793522,15428362.37327401,15428363.64804067,15428364.946526745,15428366.269173471,15428367.61643032,15428368.988755131,15428370.386614226,15428371.810482625,15428373.260844193,15428374.738191748,15428376.243027322,15428377.775862243,15428379.337217364,15428380.927623235,15428382.547620228,15428384.197758812,15428385.878599668,15428387.590713905,15428389.334683254,15428391.111100236,15428392.920568436,15428394.763702624,15428396.641129002,15428398.553485436,15428400.501421621,15428402.485599361,15428404.50669272,15428406.565388342,15428408.662385609,15428410.79839691,15428412.974147856,15428415.190377584,15428417.447838917,15428419.747298736,15428422.089538135,15428424.47535271,15428426.905552877,15428429.38096412,15428431.90242723,15428434.470798641,15428437.08695072,15428439.751772005,15428442.46616758,15428445.231059335,15428448.047386302,15428450.91610493,15428453.83818948,15428456.81463228,15428459.846444141,15428462.93465459,15428466.080312327,15428469.28448554,15428472.548262216,15428475.872750616,15428479.259079522,15428482.708398737,15428486.221879387,15428489.800714372,15428493.446118737,15428497.15933009,15428500.94160905,15428504.79423961,15428508.718529645,15428512.715811267,15428516.787441405,15428520.934802087,15428525.159301085,15428529.462372247,15428533.845476098,15428538.31010023,15428542.857759874,15428547.48999839,15428552.208387792,15428557.014529236,15428561.91005366,15428566.896622216,15428571.975926911,15428577.14969116,15428582.419670343,15428587.787652424,15428593.255458526,15428598.82494358,15428604.497996934,15428610.276542973,15428616.162541784,15428622.157989819,15428628.264920564,15428634.485405225,15428640.821553387,15428647.275513813,15428653.84947508,15428660.54566634,15428667.366358088,15428674.313862937,15428681.39053634,15428688.598777415,15428695.94102979,15428703.419782346,15428711.037570106,15428718.796975061,15428726.700627074,15428734.751204714,15428742.951436197,15428751.30410026,15428759.812027128,15428768.478099465,15428777.30525329,15428786.29647903,15428795.454822462,15428804.783385789,15428814.285328617,15428823.963869087,15428833.822284874,15428843.863914324,15428854.092157595,15428864.510477742,15428875.122401899,15428885.931522463,15428896.941498285,15428908.156055909,15428919.578990778,15428931.214168547,15428943.065526342,15428955.137074076,15428967.432895808,15428979.957151055,15428992.714076255,15429005.707986094,15429018.94327499,15429032.424418567,15429046.155975098,15429060.14258705,15429074.388982642,15429088.89997736,15429103.680475611,15429118.735472338,15429134.070054661,15429149.689403573,15429165.598795645,15429181.803604808,15429198.3093041,15429215.121467466,15429232.245771661,15429249.687998071,15429267.454034623,15429285.549877767,15429303.981634432,15429322.755524056,15429341.87788057,15429361.355154617,15429381.193915542,15429401.400853625,15429421.98278228,15429442.946640233,15429464.299493939,15429486.048539707,15429508.20110624,15429530.764656924,15429553.746792339,15429577.155252691,15429600.997920392,15429625.282822616,15429650.018133923,15429675.21217891,15429700.873434955,15429727.010534965,15429753.63227021,15429780.747593122,15429808.365620276,15429836.495635316,15429865.147091955,15429894.329617085,15429924.053013843,15429954.32726484,15429985.16253533,15430016.569176534,15430048.557728967,15430081.138925832,15430114.323696483,15430148.123169972,15430182.548678547,15430217.61176137,15430253.324168174,15430289.697863044,15430326.745028228,15430364.478068061,15430402.90961291,15430442.052523168,15430481.919893408,15430522.525056517,15430563.88158792,15430606.00330991,15430648.904296003,15430692.59887545,15430737.101637645,15430782.427436875,15430828.591396894,15430875.608915709,15430923.495670425,15430972.267622184,15431021.94102111,15431072.532411437,15431124.058636636,15431176.536844678,15431229.984493418,15431284.419355918,15431339.85952605,15431396.32342402,15431453.82980214,15431512.39775054,15431572.046703065,15431632.796443261,15431694.667110411,15431757.679205736,15431821.853598598,15431887.211532922,15431953.774633586,15432021.564913057,15432090.60477799,15432160.917036023,15432232.524902625,15432305.452008128,15432379.722404763,15432455.360573841,15432532.391433116,15432610.840344146,15432690.733119842,15432772.096032135,15432854.955819659,15432939.339695686,15433025.275356112,15433112.790987499,15433201.915275395,15433292.677412583,15433385.1071076,15433479.23459336,15433575.090635745,15433672.706542589,15433772.114172509,15433873.345944084,15433976.434845032,15434081.414441533,15434188.31888773,15434297.182935316,15434408.041943274,15434520.931887737,15434635.88937194,15434752.951636428,15434872.156569246,15434993.542716341,15435117.149292132,15435243.016190106,15435371.183993664,15435501.693987036,15435634.58816634,15435769.909250773,15435907.70069399,15436048.006695531,15436190.87221246,15436336.342971064,15436484.465478819,15436635.287036275,15436788.855749344,15436945.220541462,15437104.431166073,15437266.538219163,15437431.593151903,15437599.648283502,15437770.75681413,15437944.972837973,15438122.351356441,15438302.94829145,15438486.82049891,15438674.025782255,15438864.622906145,15439058.671610221,15439256.232623098,15439457.367676286,15439662.139518416,15439870.611929435,15440082.849734947,15440298.918820681,15440518.886147024,15440742.819763621,15440970.788824195,15441202.863601191,15441439.115500811,15441679.61707792,15441924.442051038,15442173.665317504,15442427.362968579,15442685.61230466,15442948.491850572,15443216.08137078,15443488.461884804,15443765.715682516,15444047.926339502,15444335.178732507,15444627.559054723,15444925.15483125,15445228.054934386,15445536.349599004,15445850.130437886,15446169.490456838,15446494.524070064,15446825.327115161,15447161.996868223,15447504.632058805,15447853.332884802,15448208.2010271,15448569.33966429,15448936.853487037,15449310.84871248,15449691.43309829,15450078.71595664,15450472.80816795,15450873.82219439,15451281.87209313,15451697.073529417,15452119.543789221,15452549.401791774,15452986.768101636,15453431.764940511,15453884.516198669,15454345.147446023,15454813.785942733,15455290.560649484,15455775.602237307,15456269.043096803,15456771.017346997,15457281.660843646,15457801.111186987,15458329.507728871,15458866.991579376,15459413.705612723,15459969.794472545,15460535.40457648,15461110.684120027,15461695.783079695,15462290.853215322,15462896.04807153,15463511.522978492,15464137.435051657,15464773.943190651,15465421.208077148,15466079.3921719,15466748.65971053,15467429.176698472,15468121.11090471,15468824.631854376,15469539.910820248,15470267.120812934,15471006.436569924,15471758.034543283,15472522.092886,15473298.79143705,15474088.311704945,15474890.836849961,15475706.551664727,15476535.64255343,15477378.297509372,15478234.70609091,15479105.059395796,15479989.550033804,15480888.37209756,15481801.72113178,15482729.794100445,15483672.789352404,15484630.90658484,15485604.346804988,15486593.312289808,15487598.006543696,15488618.63425415,15489655.401245346,15490708.514429688,15491778.18175714,15492864.612162465,15493968.015510267,15495088.602537766,15496226.584795443,15497382.174585273,15498555.584896872,15499747.0293411,15500956.72208161,15502184.877763866,15503431.71144193,15504697.438502857,15505982.274588784,15507286.435516587,15508610.13719531,15509953.595541112,15511317.026389977,15512700.645408114,15514104.667999936,15515529.309213918,15516974.783646068,15518441.305341246,15519929.087692298,15521438.343336971,15522969.28405284,15524522.120650062,15526097.062862273,15527694.319235371,15529314.09701466,15530956.602029989,15532622.038579343,15534310.609310685,15536022.515102372,15537757.954942059,15539517.125804303,15541300.222526915,15543107.437686285,15544938.961471623,15546794.981558478,15548675.682981372,15550581.248006059,15552511.856001224,15554467.683309978,15556448.903121265,15558455.685341286,15560488.196465261,15562546.599449491,15564631.053584214,15566741.714367045,15568878.733377649,15571042.258153483,15573232.432067031,15575449.394204617,15577693.279247152,15579964.21735293,15582262.33404268,15584587.750087248,15586940.58139799,15589320.938920211,15591728.928529803,15594164.65093345,15596628.20157245,15599119.670530608,15601639.142446317,15604186.696429072,15606762.40598064,15609366.33892125,15611998.557320917,15614659.11743608,15617348.069651993,15620065.458430909,15622811.322266262,15625585.693643274,15628388.599005904,15631220.058730563,15634080.087106675,15636968.692324247,15639885.876468748,15642831.635523258,15645805.959378216,15648808.831848819,15651840.230700178,15654900.127680358,15657988.488561478,15661105.273188788,15664250.435537929,15667423.923780378,15670625.680357086,15673855.642060334,15677113.740123885,15680399.900321206,15683714.043072049,15687056.083556943,15690425.931839878,15693823.492998822,15697248.667264119,15700701.350164562,15704181.432681013,15707688.801407333,15711223.338718569,15714784.922946034,15718373.428559152,15721988.726353757,15725630.683646642,15729299.164475981,15732994.029807456,15736715.137745611,15740462.343750235,15744235.500857372,15748034.459904522,15751859.0697598,15755709.177554535,15759584.628918983,15763485.26822069,15767410.938805088,15771361.483237948,15775336.743549168,15779336.561477488,15783360.778715676,15787409.237155758,15791481.779133752,15795578.247673534,15799698.486729326,15803842.341426335,15808009.658299124,15812200.285527252,15816414.073167697,15820650.873383641,15824910.540669229,15829192.932069786,15833497.90739712,15837825.32943955,15842175.064166183,15846546.980925104,15850940.95263512,15855356.855970712,15859794.571539823,15864253.984054213,15868734.98249206,15873237.460252546,15877761.31530216,15882306.45031251,15886872.772789354,15891460.195192758,15896068.635048185,15900698.015048256,15905348.263145305,15910019.312634317,15914711.102226466,15919423.576112973,15924156.68401945,15928910.381250555,15933684.628725067,15938479.393001465,15943294.64629395,15948130.366479127,15952986.537093407,15957863.14732122,15962760.191974305,15967677.671462115,15972615.591753697,15977573.964331085,15982552.80613462,15987552.139500314,15992571.99208958,15997612.396811675,16002673.391739031,16007755.02001595,16012857.32976084,16017980.37396248,16023124.210370537,16028288.901380843,16033474.513915699,16038681.119299617,16043908.793130986,16049157.615149906,16054427.669102732,16059719.042603705,16065031.826994007,16070366.117198821,16075722.011582626,16081099.61180332,16086499.022665476,16091920.351973208,16097363.71038306,16102829.211257307,16108316.970518125,16113827.106503017,16119359.739821866,16124914.993216109,16130492.991420306,16136093.861026617,16141717.730352415,16147364.729311587,16153034.989289725,16158728.643023688,16164445.824485786,16170186.668772986,16175951.312001452,16181739.891206738,16187552.544249952,16193389.409730159,16199250.626903363,16205136.335608335,16211046.676199496,16216981.78948725,16222941.816685846,16228926.899369165,16234937.179434568,16240972.799075061,16247033.900759991,16253120.627224421,16259233.121467486,16265371.52675978,16271535.986660039,16277726.645041268,16283943.646126391,16290187.134533681,16296457.255332017,16302754.154106101,16309077.977031806,16315428.870961692,16321806.983520782,16328212.463212768,16334645.459536597,16341106.123113573,16347594.60582504,16354111.060960598,16360655.643376976,16367228.50966751,16373829.818342246,16380459.730018672,16387118.407623025,16393806.016602144,16400522.725145802,16407268.704419473,16414044.128807396,16420849.176165866,16427684.028086612,16434548.870170103,16441443.892308647,16448369.28897905,16455325.25954468,16462312.008566624,16469329.746123787,16476378.688141534,16483459.056728682,16490571.080522401,16497714.995040713,16504891.043042244,16512099.47489271,16519340.548937738,16526614.531881578,16533921.699171118,16541262.335384708,16548636.734625205,16556045.200916635,16563488.048603788,16570965.602754155,16578478.199561393,16586026.18674965,16593609.923977936,16601229.783243744,16608886.149285033,16616579.419979783,16624310.006742071,16632078.33491385,16639884.844151396,16647729.988805378,16655614.23829359,16663538.077465191,16671502.00695542,16679506.543529596,16687552.22041529,16695639.587621525,16703769.21224366,16711941.678752901,16720157.589269113,16728417.563815651,16736722.240554966,16745072.27600369,16753468.345225899,16761911.142003195,16770401.37898036,16778939.787785184,16787527.11912117,16796164.142831836,16804851.64793516,16813590.442627,16822381.354252115,16831225.22924144,16840122.933014445,16849075.349845257,16858083.382691275,16867147.95298319,16876270.000375077,16885450.482453555,16894690.374404788,16903990.668638382,16913352.37436703,16922776.517141048,16932264.138336815,16941816.29459828,16951434.05723074,16961118.511546154,16970870.75615932,16980691.90223437,16990583.072680987,17000545.401300043,17010580.031878185,17020688.117231242,17030870.818196204,17041129.302571774,17051464.74400756,17061878.320842016,17072371.21488942,17082944.610176284,17093599.691627774,17104337.64370457,17115159.64899115,17126066.886736263,17137060.531346563,17148141.75083473,17159311.705223143,17170571.544904713,17181922.408962365,17193365.423448868,17204901.699628934,17216532.332185503,17228258.39739245,17240080.95125591,17252001.027626734,17264019.636286594,17276137.761010505,17288356.357608594,17300676.35195006,17313098.637972597,17325624.07568036,17338253.489133988,17350987.664436117,17363827.34771603,17376773.243117146,17389826.01079122,17402986.264903083,17416254.571650077,17429631.447300084,17443117.35625248,17456712.709126092,17470417.860878516,17484233.108960994,17498158.691513278,17512194.78560275,17526341.505512152,17540598.901080254,17554966.956099804,17569445.58677694,17584034.640256386,17598733.893216535,17613543.050538458,17628461.74405291,17643489.531369135,17658625.894789245,17673870.24031188,17689221.896728426,17704680.114815358,17720244.066625644,17735912.844882272,17751685.46247661,17767560.852074157,17783537.865829993,17799615.27521602,17815791.77096184,17832065.96311084,17848436.381192822,17864901.474514242,17881459.61256682,17898109.085555036,17914848.105042744,17931674.80471874,17948587.241281077,17965583.395439275,17982661.173033614,17999818.406270184,18017052.855070163,18034362.208531488,18051744.086500816,18069196.041253347,18086715.559277844,18104300.06316392,18121946.91358839,18139653.41139718,18157416.799779236,18175234.266528346,18193102.9463889,18211019.92348113,18228982.23380137,18246986.867792625,18265030.77298057,18283110.856669974,18301223.988696426,18319367.004228104,18337536.70661223,18355729.87026084,18373943.243570246,18392173.55186886,18410417.500387594,18428671.77724746,18446933.056458645,18465198.00092561,18483463.265452698,18501725.499744676,18519981.35139705,18538227.46887061,18556460.504445214,18574677.117147513,18592873.975647885,18611047.761121552,18629195.170069348,18647312.917093527,18665397.73762439,18683446.390593413,18701455.66104912,18719422.362711713,18737343.340463057,18755215.47276863,18773035.674028218,18790800.896852575,18808508.13426326,18826154.42181319,18843736.83962576,18861252.514350336,18878698.621032603,18896072.38489791,18913371.083046556,18930592.046059705,18947732.659515195,18964790.36541246,18981762.663506165,18998647.112548314,19015441.331438754,19032143.000284225,19048749.861366335,19065259.720018927,19081670.445415623,19097979.97126827,19114186.296437506],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Ridge en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Ridge pour tout les paramètres de GridSearchCV\n","fig1 = go.Figure([\n","    go.Scatter(name='RMSE moyenne',\n","               x=alphasridge,\n","               y=GridRidge.ScoresMean,\n","               mode='lines',\n","               marker=dict(color='red', size=2),\n","               showlegend=True),\n","    go.Scatter(name='SDup RMSE',\n","               x=alphasridge,\n","               y=GridRidge.ScoresMean + GridRidge.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               showlegend=False),\n","    go.Scatter(name='SDdown RMSE',\n","               x=alphasridge,\n","               y=GridRidge.ScoresMean - GridRidge.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               fillcolor='rgba(68, 68, 68, .3)',\n","               fill='tonexty',\n","               showlegend=False)\n","])\n","\n","fig2 = px.line(GridRidge,\n","               x=alphasridge,\n","               y=[\n","                   'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                   'ScoresSplit3', 'ScoresSplit4'\n","               ])\n","\n","fig3 = go.Figure(data=fig1.data + fig2.data)\n","fig3.update_xaxes(type='log', title='alpha')\n","fig3.update_yaxes(title='RMSE')\n","fig3.update_layout(\n","    title=\"RMSE du modèle Ridge en fonction de l'hyperparamètre alpha\")\n","fig3.show()\n","if write_data is True:\n","    fig3.write_image('./Figures/graphRMSERidge.pdf')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["      paramètre  Lasso()\n","0  lasso__alpha      1.0\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predLasso=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[3864461.410807106,4170799.7842199835,1369626.9697970713,589643.06873067,3435757.5858639437,1255690.7033888064,365704.02024243306,4434761.530617106,48572378.64044188,1728214.6354982797,1879587.860684318,13480016.945212089,982811.2782092951,1901402.2516860492,2311799.8603145354,546162.5061743706,8009114.90216568,3952994.86662563,3322836.8827657755,1105164.1859150848,5610860.806381078,2187666.476900685,5067577.8810276985,675684.1623232565,1251911.7566099467,59724125.21077279,3804626.873006498,762345.2176734419,925255.047299051,10397869.493925553,4163422.5332178036,1582349.5193769028,466938.22720027366,1069558.1726626223,2247843.810514671,2994003.271669229,10371724.084001623,1110821.673970622,2550171.85970719,1479865.6670277715,750977.1784162172,1424972.057168486,1518571.469307573,1290959.0342232776,6119503.031177308,2077182.4059904078,13100486.13013167,660760.6079221456,3065761.002163337,834884.2675459916,2070989.0218582172,4940402.376545649,16335707.53134342,1918137.3189836997,2156286.725499876,1909092.6342197075,27124386.402290925,3823500.7953706374,8729094.51693835,6489086.215278515,1177543.8621383659,3001585.1539260903,1313582.1029797215,19056742.536085147,2832583.891572291,262307.4723607681,4976377.463768037,1686306.3817719088,-519452.71570840105,593113.7619595455,1584375.783709805,-557222.5039893552,2588807.477895001,3868245.772276558,-1083857.5391469905,1345987.3460196299,5116665.339214642,8298858.969370534,6838963.350366818,1393607.2087664676,538476.9888960337,3362069.738798173,1938828.6181493872,7632779.916688456,-194086.16798215592,3396571.3286412493,1669899.709841738,485044.3267160326,-700545.1209994974,1965714.685519272,-161050.73283667397,43611.68962772796,3011983.192394961,6810508.282240062,1434191.7010145287,2055140.9424865027,1261318.5747737244,602902.4852102369,744984.0551826446,2662617.368473996,1674730.1439974587,-118568.7934695608,2724801.103391781,2330298.9911671984,855062.1659024318,851351.7666970766,3808274.5441182805,4908226.5764049515,8809290.044527553,3561998.3012744915,2070573.2170197282,2048229.9228699491,2330875.3558026752,1338111.5019130209,2611035.4606174366,5375287.761700929,1742690.1713382315,1051253.331393114,1535984.2425523596,4751356.459577816,5458947.27696217,1864975.7360197173,3509224.7679458624,5643659.75667787,1436089.9709738386,5597615.528062651,794431.2804689063,3457743.716168737,-356122.08572677895,9102617.445800086,763062.369795246,2008454.8833841286,-197733.83909393847,2383103.271513692,4330176.9387197895,2446514.147477176,1534767.9719126304,682811.5658659525,418733.9860156458,4976955.494624702,786200.0564385084,1011571.8534257459,5345459.313442956,701371.2560206687,1693082.7849445557,1252043.032277024,19053094.86497337,852476.7648643665,1920623.6754223674,15056293.961080346,641233.1479658289,908370.8642885787,928902.7184108335,33817284.26952605,1231915.0319863663,4749320.664761787,46941.73737034411,3701844.620354239,21053584.91242938,1497529.5294798845,16218582.774122577,5429703.179218246,3486976.88117577,4582042.364236796,779693.6009756539,12005356.026208702,-3364540.11292708,7505815.122843371,15476260.719894812,3168641.6208619173,59720477.53966101,4793974.197597032,1208259.999234973,6190590.640040346,1168415.609459789,821305.5503747896,5755976.817852214,12558204.067900777,8905045.42120098,296027.9186662263,2331023.2729678033,5916308.660877378,5139248.804530941,1478363.463340255,1127424.545486431,-262040.05004552752,26049.596607788,2104463.89860992,3834062.6456490266,1903988.9428526873,1650894.3505589648,1795260.057474996,5862475.444807675,-422641.1720556421,3875896.4715907415,1241425.3581345147,1067150.4236348316,4578605.623314837,21584004.469320092,776144.9081676046,390325.70751008764,2243229.751298864,941529.8469913588,-417412.893253963,4256114.109349425,1588488.61441936,2415023.5727785327,870202.0637904967,12415400.990091901,15646098.071222726,2209879.1233369987,1635435.6339781985,3230677.1988686835,3913585.2737392243,2650780.0619924553,5601263.199174434,6618830.033258903,13494492.965055346,990803.6548738636,1513824.249941685,1761230.3809421975,1567491.6720934971,4290667.908201317,1463381.3176702254,12526576.919938106,28261570.561729398,13483664.61632387,8681150.858306143,4583359.233229369,803329.3397873857,1351308.5539371555,2356687.0630104844,3325002.4574502707,5886204.894483786,1223264.9716497592,360196.86663412605,5232768.218170082,-75201.64898116654,11776638.22595393,608988.6175911897,19722.116443894338,3551772.8990118336,1381464.5873075675,51379.765070984606,1636390.4151582676,5476337.393040435,1213525.7964156219,1963840.694267482,1704087.5297397366,2885414.599004605,22172114.435599238,16652726.01985313,5544996.774507164,4424488.9422006905,1952304.2198218836,862658.722585059,9306337.704891557,1955924.8939484833,2450144.285749484,862576.5510684662,1252720.4463290367,903852.9675542857,3532272.8124342468,1302491.0183721294,1585106.4409587416,4365131.668602278,323320.3694062517,1655776.8166744981,2080830.0771021903,2346876.189336814,1698155.993880209,777476.3491417693,828586.5853947895,2387353.1775308093,12924620.351136174,-777535.6888901126,2320542.25388058,318599.20657252474,941979.7852660907,831479.3701002421,1347439.455885013,1310641.8814123594,1443380.715327194,1972474.7379559968,8167749.842898203,2585159.806783218,637597.2843297422,-1219053.971114696,3692455.6729942975,1459733.6465584429,6978941.165216001,3045999.9043204626,1857383.919012423,3441131.9378644945,1752953.575970021,1314910.8609195987,339933.28446788155,5263836.187166981,7623859.773338875,1248897.560249801,-679577.7895940142,8686952.732041918,1530890.107440751,697186.5196161687,722945.7342503367,814313.6518985459,4783483.634116598,2230207.0655269804,2391062.901933683,1872184.7930715047,29770881.806932323,2123409.6797577566,973959.6898894759,1030035.0587801165,1132564.6387370971,3527247.671080172,1178057.4057359416,1045657.8252345042,139542416.91110146,3328443.440269249,1285886.718520828,1197608.4651196445,2398394.0708315712,12658996.809562638,4871628.398084907,642460.5074821224,5305714.640673772,4345878.823874272,1629684.1195925735,1136958.0734535248,2371513.731754579,1831768.6402786747,1853179.7518466343,193456.8560126815,9995497.60373698,779792.5792793871,2127057.350869539,2284081.8921551723,5949013.736930071,1994616.4166131923,5035378.586897874,3453066.7457391955,3685314.936587785,21223784.84176655,4984489.434160771,-322726.8127346509,7550293.590465722,12083198.550339755,3528172.442531736,7349574.23627453,2713712.2911255206,9818183.733700316,4061465.2795572947,4334897.958898485,2388349.0121683027,997980.0428623813,303460.6327812434,2885910.4860030417,5859897.002636278,1242616.0401187008,1644135.1127992505,3778627.825701559,490980.8944768575,5143027.08037653,7774462.760919891,841136.3806839609,4125809.615571803,516549.697087134,666774.6594255506,3038166.9465280673,417392.0438896648,16577163.152212113,4103094.7107209517,6950166.414597098,952011.1357572521,-80748.9164253478,-184425.57540731318,3564040.502912932,453243.9208961744,11130186.680281911,238124.00126867974,602521.4349464856,1932700.4554742153,869630.5919425478,1365979.298685289,-386855.57704701275,4853210.024460545,9610335.611340243,652658.7154561258,9686796.228003673,2057135.7504986494,13139146.168319643,4487051.366589491,4255095.179547164,1046011.9823884554,3537842.2490542345,3674402.238394945,4923231.332987229,2209542.2798029976,2832663.6790909935,2938369.1402368005,707903.2281146129,6135073.610070649,1676960.0017252031,295859.1350483494,300775.5474742146,316981.674433697,1153089.765880943,4958152.529683486,356050.05348906224,2523387.6353789805,10232472.142940897,1187132.501537136,1549625.5071571495,2690223.505217759,543745.5165181069,733658.8270078879,4374610.759074368,456083.5664152298,6610023.129187881,12281184.691023175,777343.5475340136,1041652.5253095957,818779.2185542502,1864935.1353211226,595486.5024769884,1596171.6432444844,4116525.876130352,6033328.42241619,2326651.320055416,1217943.8351263977,-96012.75047571864,1643162.1621957007,10728876.239364397,18987513.976653356,2319391.338630168,4842988.691072285,4805987.943068832,-531742.8532035062,56017312.205805786,6963790.277364055,43781625.556636654,7503671.833060511,1380515.3401619806,5678813.677473845,12351370.838547703,2041299.0117274297,4167070.2043295866,3131750.339056948,749268.9100173197,724884.8028774266,14891494.968954053,8200153.343984308,1763022.0642257293,11052286.506343905,1010384.2052651504,787473.5577346226,1155692.6997584833,1162377.3208146237,2678952.92185479,7587793.944760705,3013015.398805446,1040203.8871033872,1004874.5913076126,3770549.567861099,1795555.9687504352,3605428.011817164,64306.19846996106,476866.7907389954,2457323.900296957,819027.2175361228,1507425.469609043,4609397.856819967,4631524.754691618,24780584.023847,1755032.7058534082,7627507.444450657,948601.1215480277,5721628.343149734,1108554.770541952,-375181.6118599251,14435740.081748545,6215934.52857029,408579.86732002837,5654101.351762017,-213667.82849974185,6194238.311152129,5601992.889000602,530699.9767514148,1925398.1977775425,2286853.726928411,4285790.927287322,462034.40695361514,884182.2998525035,544416.6232123841,2711623.335164302,1568235.3248626494,132919.92652476951,7293142.48316914,223499.54521460691,1941564.5550228197,1440059.6257299844,5048691.8670437755,2386223.916403217,3911286.1026914674,101479602.26268935,2809615.254449486,1914454.238016503,915639.8358587674,3609075.682928947,363664.9373929256,1240538.721669597,649011.0443443432,3088262.8097915603,1009804.0077560607,3293958.8876323914,3072657.5554708173,-34010.4913614816,669561.9977176157,249329.3166165026,7716663.793369282,1101224.1381581414,1642527.4976715078,2511649.732876404,10758947.474975362,2455543.4254936194,2101818.1638995344,1886145.6924182167,1339321.597803187,2100816.2274981374,753785.7545585136,4288329.3916731365,26427038.23153942,1254006.9234453451,7946209.2799686985,931674.8450533822,3110309.75939448,823445.256246899,2552236.161954823,62139161.68678637,-402100.11522078374,3499416.453433953,5898989.786082018,1631787.962866416,110838.36456846213,780454.7170374575,269299.2580998631,2246185.298860899,1632742.744046485,7136121.877613265,4620580.207241651,7613557.631543485,1191648.0619942264,4169712.967591349,4932563.767928093,1742052.8045109059,1752819.3389852333,969528.5968904991,6189482.81127483,-563432.7522946163,2515321.340569634,792617.8356354425,979163.6070975126,10008976.956770565,144908.59052568884,5578868.448873436,1868957.696022924,6417757.1203019,1898848.0229792618,1589316.1834952938,25975425.855698712,14404322.625256069,163423.89045266714,2478919.100359505,1797813.7636329276,2649220.1913277046,2158063.244307965,4258742.850658946,3009137.534333566,3233664.743853843,10588576.758096986,737253.5554247815,1063781.4575897376,937588.3899909914,3097051.3998611816,2881616.320663291,607541.8042369708,7086328.12012531,5831541.620772926,1445256.1850911689,35113186.43468753,1460006.515969868,998386.9414947899,13730725.830200396,1905272.8412549007,1080547.6258502444,1128696.4011964756,344083.05122264754,1649584.081503375,1471041.0641857437,2049120.4594631128,52210008.73919894,1251933.556693538,961989.6957600049,1463493.3432166635,2904768.9856790258,6300762.432910188,8987135.882251173,1025631.7240274583,4012546.5007568505,4626433.752928544,906175.0302598786,7657246.123272653,22731971.030808594,35995290.12979383,3173666.434478704,4747753.730622684,6252424.351724328,1814176.1429603002,-672174.2441619681,2089752.318755093,1558487.3832087745,5871787.852376786,695119.31735839,731888.1969784619,7997472.0155250775,-136480.15007628826,6540243.7269333135,4722940.905841258,1309492.4027179219,6174300.708914537,1076818.3778130738,2965541.439140044,2346386.659650504,1539153.095508889,40785335.99902661,-45439.48307838943,-71247.9214242748,514554.02767546964,944018.2662995004,16649078.348741349,1343048.2075239478,3565854.158569514,1483318.5546348712,978165.3408619305,10597540.734454405,56020959.87691756,24551785.938140523,11871152.428281602,2720559.6820092397,3140846.5933545204,3678103.616944893,596761.4330713279,1036285.74397939,967939.2068660865,2828672.1083018775,4456343.480207576,189510.64188633976,-239390.76325690513,816969.5289991973,2616127.4700164357,18780481.380401123,4200478.879453056,1258724.0718957512,1718470.5174960417,1522629.4127253727,368528.2132301931,4943963.280050933,22232239.102683403,4616650.3886249,1913443.7308702357,16717711.545100432,5297172.354506161,2618570.0354031366,139546064.58221325,5727615.783210777,2467095.5527612306,10829989.561746463,3230880.7487931033,50214875.5165351,624586.6391066797,6870285.283609628,10947228.024911176,1063641.2495055574,1768623.374906049,1081744.711994995,6962644.035592816,1207413.194392235,2401285.8436774397,2681307.9281446217,1983947.571524957,8022557.396001597,5808601.143652435,39147960.68689999,1798907.7285867785,870269.1052223039,2650257.693375334,3600491.35813669,3504994.3553589247,6991061.180713668,1498166.4786540489,968455.5741198896,4211987.5990466755,6336641.749062649,8710570.712046422,788136.2264221313,9715308.390259583,448240.77946560434,1156737.4369927256,2837506.6946665496,2923383.995372016,759396.8930616877,4564014.46469727,5043215.456341609,43777977.88552488,6543891.398045097,25290145.73594365,360017.26628114306,159776.2193408846,4830958.302500172,3626981.9452380976,415884.4249757384,675153.027286398,1385227.4557934671,356398.58829281246,4576308.404157777,726944.9011954558,1051017.2495744827,4414695.567400862,9602757.228850603,653071.6715464182,5004656.847396324,332838.3367861111,10328959.092368297,8545004.763558663,637585.4768540466,1412066.650847353,1920025.903058245,8004494.826801907,27057964.405028664,22831388.183102865,2014609.32186129,4073334.76652502,5387683.086289797,-482870.63173902966,812619.4943645094,577256.5105615398,449584.01277895947,30042.140588996466,1081965.4352484338,8746164.445813058,1202019.506248082,1993713.3188006072,1844174.9624168884,2067341.3507464346,1145593.9028584592,2622306.260453359,107561.85522603267,11412378.24798923,753075.1955792359,1714822.8463842592,2919841.688003077,1947053.9030093509,2543803.8039181153,1434473.082084767,1983865.6293182075,1015572.540180756,789737.724125023,2410004.583956046,32446786.142671626,3153312.7376542166,5999916.151306497,-65746.54350772593,2180705.483296049,67639060.44682038,1879809.8072617697,5821979.553415667,3303296.9444771265,481135.08217501594,648010.9727609961,-647057.4041782143,714684.926509686,579478.9166317526,5830467.214520859,4213907.087045022,1719445.2127897658,913284.6281807991,-29732.503322110046,2944111.3089661403,652368.5553499234,70777122.42926547,818658.9108272148,898888.4544560506,19270980.535801668,924216.368809534,605234.5155250276,732184.6123968349,881974.8481684567,18340947.328318257,3824394.2623696104,2363525.811176887,784612.0937007042,107190.69345667958,1524763.1315730645,1362806.499096644,9006672.473806385,2080684.1536414558,4888421.435266048,45770239.78549099,393635.2829724157,2720149.633095192,-1429400.7409668192,2471087.4629119206,6177948.38002632,964807.9030081071,812743.3970978823,490214.85600986285,4528070.614144808,2497781.648707385,1491959.8485517409,420022.8940687822,7073985.8682253035,2579244.8958259253,9387707.128468875,645544.797155431,30040986.15768657,3751239.163054587,3945575.5407123775,1164652.7409738898,901562.2080444198,312347.1179010954,2290501.3980401936,244714.06097006286,1534595.1199834514,6194618.073501575,5001808.197093528,1085613.1063602164,-1976776.4324101768,20417716.558792885,6256904.081558741,6410358.467771169,97501.3213859268,261076.5386593109,1937890.24047536,10949188.313487254,28204200.142350767,2270166.876218833,11340000.9158873,1941514.6653701393,1731862.3066100623,767399.4527579239,2923489.3591148593,586446.9811245438,2045734.5223848345,8829004.131364185,533773.8413625797,3112161.475245326,8283208.987149415,1349982.867493393,3643653.6386515307,3215491.136558729,8145441.926336557,3010613.0935883354,583405.7358692193,18119268.78122417,5079730.365386366,1010950.9515455598,159684.91589864064,3347493.450951514,2083939.3651556466,111209.52633781522,2371460.5162773896,1148803.866178561,1580241.1538363155,2950826.5502564274,5958641.990196679,12737327.277844891,7071718.574199518,6873932.954721411,1121764.9600882703,5751759.069207951,-490403.2714949241,2391000.848642592,3188633.1408718275,1241061.0902867182,-547179.9963827003,1041924.6784894047,1256368.1174408193,5906930.809007238,5544286.241891886,14000398.740805507,1401562.6981453055,4483533.4674078785,4613480.637072787,9044469.017073207,2154790.5107535054,9721785.063317075,1654803.2661893852,1803765.5113199612,-482716.2612742544,2408826.9483382986,2482073.9763014214,1332392.6574378323,9812078.344997391,8313880.496187219,5193227.166719483,2519739.964267198,2156871.1474080887,782292.7212816486,25769844.547589988,13079993.667090453,5172861.209145786,5013378.810895279,3276455.0464728717,4061632.3962650383,2482566.771471288,2897641.5821363297,712719.6002544919,3251218.0207988117,3121899.377923104,3142835.9489405253,-775314.6838524113,1454621.9047025652,4822872.904606745,626647.9258686232,2643744.386044733,31547.11799734691,9897006.595269654,2261354.7634427734,8098495.2667864105,1634514.9319319746,2097072.611470484,4972879.373169844,1044568.7103369734,2573132.2633348657,3100699.070972964,2957236.4967553504,4865002.570521892,1505992.441741531,4980486.377736259,6237620.908972982,1090851.3681829683,1313493.981987189,27181592.507664543,3645893.5775612183,727768.4030254013,4922132.847264574,1139578.8843470046,5547933.913003667,8613616.984946862,2333240.2150862366,1448872.0424570846,1176609.8196838815,-118153.73353641713,11352312.901627284,877604.870323257,3368822.546503539,1285192.4466404968,685405.7333237552,675619.1044317724,1195295.7331060087,7620201.83980616,597844.6533751944,1302275.048624588,8298408.576370003,397282.9540841982,4792940.105385578,6054241.077128073,2612479.7989046527,376173.65385030676,2708986.45965496,675356.577210818,1861287.46420934,1706553.439743517,866224.2221802487,-734879.4368832707,4313849.186456704,3682452.052096052,6350107.4816228775,12963530.80691797,-526529.2246653629,8732742.188050132,5266853.846184958,245796.96748842392,2513696.001270667,9675812.542686677,410114.65931010037,-375206.14286612626,2470929.6095810384,874093.9430583997,11924313.918839455,3372470.2176153213,-804774.8829365582,229480.96614545234,2806863.631140266,1222910.8144958077,261989.24855736573,297040.87202810636,2938236.3386290446,4058675.3052335475,351125.4183331125,5660309.244528985,4215709.618839005,1825169.851246776,185741.66614920925,664421.6111497923,4408311.624744922,4981178.569178034,7382951.169408497,44425874.94769324,1286002.4038137887,309903.1641496434,9751865.232414845,3042352.2332086805,1929815.5628449274,12267589.21805751,8306708.758859675,714623.2590261677,2947497.3215212133,476561.8067100609,1537123.1795905991,4834605.973611955,15007128.446944404,1883457.478373552,1577522.9511904945,1152874.0048837308,1137170.662579983,3584041.048404707,-326633.2350202594,690528.873294295,27351630.519360285,10395074.105181972,549459.6431181633,527330.8699580932,1976122.4090677793,5187117.068814365,5017026.482007061,4199610.877787353,4228261.544351717,1179907.8362360713,18407198.97420243,4430047.791138824,2660030.0629365137,5297868.504564298,1678996.3907104707,3851975.078722652,1562135.054320557,964078.9466278455,1345987.3460196299,8061050.85702895,11033128.883759562,6258203.695435492,2540899.7115986664,944219.7955699423,2287814.5831564227,58632717.52974023,1588095.6472695435,5052339.538155558,1656783.6897896042,2508002.0617646216,1688230.801561126,3688652.5014978833,2295742.948432989,4167630.0326678697,2059674.1785713956,815010.3402353902,6472649.844549839,6734741.515338512,7373128.787524129,1731567.4125173304,4254691.365998914,19605671.558991633,-368156.4486459368,5418465.137629503,7244542.958250347,3588663.1943951435,9088446.568695996,1361640.8461050787,1935180.9470376046,-321495.52671844,1956639.3892921314,845831.5382085259,8164102.17178642,490117.4642575213,653197.793561443,2128016.5452793646,5352867.698005346,864485.7711709025,2371885.4931421694,1354956.225048938,1384465.926008375,1663954.169147686,2518831.923225751,920748.3832556703,10218687.86219443,989938.3337546298,55220921.09211661,678688.7062827104,874494.1277872927,1444998.5697679557,4635307.329941675,648983.0794355855,23312652.910341777,1801503.2833656569,3660718.3882345734,1411169.0717716871,4401498.996502608,389609.17584122135,3910110.461888319,4180336.88397483,6342112.791776312,3137349.3280841326,2173686.1380914617,1244186.3927813794,5566016.2226407565,1351574.0857585026,530385.7791876062,-122180.18509550206,1042868.8742720184,776841.5274149701,18995542.56514668,22057919.49647261,10822253.279189304,-114301.46589175519,1528052.5657864215,917100.7121438878,6338279.804618813,1617885.9415773342,2022346.4939889717,6903948.706451163,2934836.393716834,-238684.05157832615,4711864.517581517,8068575.246092463,3145173.151921686,1580728.1125980224,9646790.391911395,555166.3884018231,57615.368081824854,11011707.833423767,5057427.710629967,1181705.0768477237,2856525.3554316545,764239.8372430797,4581956.410138297,2525078.3556024325,2406068.5763443885,1768174.9054149527,1103242.3939055076,21057232.58354116,6911656.728864857,14038847.227088578,1555024.1670210515,10348552.16201229,6842611.0214786,2933848.4274312216,2072503.2464447801,799621.469123503,21384386.185123563,2969189.1102518267,1909507.9196042996,5139024.279728817,8185070.903415834,112859959.9873399,3135398.010168731,10229519.382206451,1121009.8914573605,893161.3595923702,1887883.8938609462,1024486.4607368901,990151.7738390348,6575238.8455491485,2350523.8604485965,2294786.635143214,12860.795285363682,6639733.720162912,1868839.4880653247,5756050.149178045,616682.5660261582,755996.9481494771,1539769.447815112,6101507.419012773,-502436.1684465613,2346758.4210380944,3070039.7235173536,10227786.729096223,-48221.068404531106,887829.970964286,29723709.430686835,26528238.463792443,941613.9688099467,3069009.8843590347,16729595.626726408,1816203.5641376935,3050888.210446241,956871.9569545838,12647437.562309496,1303259.0101774428,2889062.270116387,1217164.7011076668,4426152.1210451685,688309.6802722209,89361.68505098182,2214296.871434086,2904255.383481642,4364928.208659152,1060133.786477955,526987.1855727024,2270279.323408602,4080588.014481024,1734961.2384513472,1251703.3633334409,413120.3252093238,8133646.7053848505,994148.9277346388,2161245.41999069,797534.9767775168,1490118.7556236333,2820489.9257712876,1912740.30533149,2328470.4491174947,1183077.5877881856,795361.0217171684,3552720.3355172095,320993.48819785705,8203801.01509609,2813262.925561269,1374434.5755172132,1119334.8491739857,2908606.2047237917,14724605.27914831,3692300.1726096664,32320345.43271125,1726178.175118848,4662409.153913939,4928916.096816311,4361280.537547369,2997937.4828143083,3091910.4809033433,1716532.7114661203,22303177.794265915,1135776.8714828487,9097877.99167516,12442759.30152686,1972933.5370906442,3442442.2069200845,5004662.130455919,5302066.96956199,18801240.108156737,1138666.33535016,5190764.739926147,4624288.793810617,7609909.960431703,96328.35653245123,8420674.536587447,315994.78901287797,1183508.020623386,5214081.040956851,392176.65771734854,1155294.3663293803,3223477.3520019636,5834114.885632641,64007.745401442284,23670676.423600655,1998395.9549262128,160862.35173220187,282137.6586250088,1653260.575830308,20084354.15648197,12479067.35657902,1379383.9506529844,1756467.0100970159,2116816.493760106,3897137.726560455,6680774.106239327,5754044.069625792,1972310.8551952182,-528095.1820917241,1499502.3797148825,2249832.9699726817,1211060.8655040173,2328177.9216326876,7827109.979863293,1530947.448871669,4169055.499097576,1882942.7344636396,3222693.520987084,4610652.270165997,339428.4979149676,2457927.5147741474,8000243.0533369705,2841154.365778332,2291138.9640314314,3562206.4874577313,28980330.39799875,1980299.9004131744,2320772.6287847226,414922.26503024204,14149899.146133859,2456476.191363333,819027.2175361228,2849193.7049475145,-1231530.2496518353,2153719.20147775,887626.4210398656,9335592.457132027,1718165.4793853424,3855482.5323115345,5818135.439536035],"xaxis":"x","y":[1437220,1614322,1416488.75,780410,5912066,1318947.25,281191,9146727,53332648,600528,2842760,6711975,2177075,8054334,962971,766628,3009967,2161828,2962476,860443,2075833,1121259,2709429,1406687.5,764094.5,26689658,5276777,1372475,1078936,9162048,4678999,768325.6875,572654.3125,562495.8125,2504814.5,4696639,875943.3125,436604.0938,1210262,841891,437372,1267348.5,2214604.25,1696448,4933374.5,1691021,3920913,1813864,1545519.375,1988760.875,1471914.25,5827595.5,6508595,804620.5,4236396.5,1524620,25307744,1661448,15232788,9512829,847298.8125,1489363.875,642052,22104512,1904641.75,131810,2462408.25,1261714,577642,479358,1355062.75,715064,63835192,1479728.75,592739.3125,557863.6875,5543575,2634166,2756494,433871,508057,308400.4063,1430356.75,12187231,592899,2216979,1884194.625,690750,673834,2478045.5,847767,647792.875,1810353,13458948,1072855,1606981.25,2291194,347437.6875,1423687,2045592,835258,2978555,2203026.5,1750419,1070381,614044.6875,5337334,2409429,3093378,430500,5767765,3418003,4618360,3546459,1310226,2729167.75,1639849.25,695955.8125,5061274,5454482,1498457.875,9190442,2996502,2890379,1795493,2364351,610744.6875,1424157.25,1003659,5907068,784584,725487,585851,1513700.625,4547129,362874,917100.3125,674961,619545.125,1906597,2001936.5,980356,1883132,721549.625,939840,1098965,23230284,533669.125,805751,7860942,883197.6875,784584,1049436.25,48143224,745575,1792425,603223.1875,1938613.125,15088676,1297041,13631141,2131817,1153276,4397572,960987.8125,19609472,7768294,2757855.5,7559797,1325972.75,25475474,1841968,559805,12051984,1469556.25,1314300,3375091.5,4665069,12726789,508264,9796649,1973429,17074142,7542332,531737.1875,1672142,1739693.25,3307792.5,1436773.875,843622.875,987987.3125,9997511,25478086,713660,3594908,2755829,1147582,2349950,27717026,1982967,429591,1645748.25,895901,947534,3528086.75,841821,973242,188745.7031,7564643,20311228,1593160,495481,5570472,2594119,336745.4063,2284133.75,5229981,13146645,838394.8125,2520919,3218565,1514279,9177040,1048781,6456026,24882924,6772289,10813735,4993962,628676.375,888561,892227,2842819.5,2265836,196410,541451.625,2371304.5,1325206,17812788,481206,519317,971155.625,1509492,1419453,1093724.375,8141155.5,797435,1162131,451738,1162519,5696695,6769410.5,2467851,6376820,1821047,600543,2637656,1299001.625,1857387,436848,2062419,1082076,1145535.75,3242077.75,1021754.688,1298039.875,395346.3125,1489392,1858224.625,1406987,1056099,2055534,840549.375,2126288,8819864,956598,10420096,350264.5938,507976,1565475,788976,599390,2345314,402136,2220053.25,65225380,880590,1916309,1241023.75,1093627,3879728,7442314.5,1256575.875,448676,1198291,1452804,645451.8125,2557933.75,4785692,522972.1875,919746.3125,1118392,145573,1093757.75,1493131.75,975327,2015280.125,861643.6875,1199742,699382,18760766,6471427,967492.875,2788860,2104255,3632613.75,5657046,866343,59991984,3497215.5,462192,682249.625,13140574,13349651,6856153.5,3033340,2646879,3925806,2852102.75,595978,2559486,841242,456397,618366,1555420.25,2058532.875,4383100.5,2321336,14697487,2241280.5,3059758,1260212.75,1791715.625,19594950,0,1689631,4863803,8259071,1820085,3983784,14084606,10287986,2966309,2059083.25,3040978,667871.125,869835,4704936.5,2091869,2067352.5,1199392,1878442.25,1297872,4917420,3591653,3318151.5,2105451,1391569,520206.6875,3292049,540465,6219841.5,2014500.375,5328224,421430,844000.3125,649874.875,10557910,4841347.5,4606967.5,962577,439521.0938,1275236.125,1426023,1299232,2655387.5,4252035,6320060,520011.8125,22388122,3371659.75,10769230,17225444,8787353,722951,3493093.75,1403094.75,2994822,710275.375,6454983,4328527,722891,2311532,1539191,1186835.125,2213115.5,456953,2601945,3986982.75,3537977,2339234,5417598,4169431.5,1222949.25,3646053.75,593543.3125,1266387,1951856.25,609368,5535626,9097980,758322,1509775,1417728.25,2076289.625,849745.6875,2228083.75,4471029,2334260,1773880,892086,1392299.75,847419.375,3644982.25,18558162,1162007.625,1686765.875,1253745.5,967835,56762408,11061916,65047284,4217623,928409,1742870,9734582,2046914,4817062,9329202,690304,646994,8241967,3612830,925293.3125,13214416,2019672.875,2717863.25,706505.6875,1242666.25,2154119,4567879,1128634.875,1176615,747994.8125,2477499,876569.6875,2666634,627854.125,1525624,1245187.5,688145,1820814.75,2468322.5,1885930,36378588,826488.6875,4391348,2447861,4045980,688641.8125,977150,10055690,850568,609101,5669431,2160420,12086616,5401277.5,835093.1875,694189,1378155,2625396,438557,3766069,4777648,1248599,579888.3125,528411.875,3155496,1359579,1769137,1686262.25,5917000,1004253,2564004,163945984,84980760,595327,1327591,2551022.25,585077.125,868163,507947,1331031,864751.8125,2093011,3586479.25,2892779,2946756.25,724153,3603866,2998208.25,1134195.5,2213421.5,4180298,2828950.5,1034827.313,1552645,2168281,3168809,730765,27137190,16614404,1360340,3091348.25,12015936,1445538.625,652449,1245197.5,157606480,896403,2512443,1933811.625,474811,1533245.75,1369190,1180231,3988729,1032857,836348,3185509,4192895,747326,3835202,13962749,960550,873776,1989017,686019.1875,588965,1072500,3996541.5,1777554,7655751,5689125,4728488.5,2654804,4585070.5,591249,1227772,21346100,6481807.5,489557.0938,2926166,1712279.875,1815350,952635.375,8873485,2920918,4329966,18628834,739711,767642.8125,504656,715446,317581,577023,4983934,2982450,972608,10614234,576038.1875,905926.875,20657852,896976,1965529,544172.375,638201,866242,914023,1111858,41655852,2440980,3605755.5,1093743,1516377,10340282,1015060.188,1045640,2726269,12087498,566000.6875,284867168,33729336,39403320,5974993.5,11924933,2993610,766381,1608224,1056782.125,2347893,4405368,357915,633408,4634655,8204897,2271263,2536760.75,694479,1271021,1315298,1484807,846274,1348497.125,11320378,1840259,502254,666385,807185.5,6860521,1758581,2971013.5,1162226.375,876871,40183424,48479988,30083496,6649938,1435558.125,1259959,4369513.5,493969.6875,1696978.75,777915,9143764,1052244.75,758266,1109851,696840,1608884.25,10084950,1224554,507217,4855036.5,919266.125,631195,5375518,9341427,1998767,3730953.5,59107620,2751694,3473109,59757440,3714139,5424718,6344171,1427294.375,131373880,2520483.25,6012303,12931002,5040382,9620496,267135.0938,36436388,1380283,1240875,1240924.625,1133028.875,6205886,2503948,32552428,9393108,811870,2786648.75,3913410,1084048,6298131.5,523133,439266.9063,6981428,6668568.5,7574092,1872278.625,2757588,5170809.5,2640980.25,8381744,1750362.25,781890,2100813.5,7335905.5,63811044,2330242,24603420,609257,549834,3322250,1695593,1357116,756196,980418,493000,7325166,703847.375,1356874.75,4826430,2057288,1360847,5327249.5,697824,4039667,23262244,842923,713326.875,7067404,2577115.25,2512319,25909504,1647746,19061780,2125323.5,636266,4729846.5,1601955.625,0,913899,529072,3404992,522791.0938,1187540.25,1527917,1182575,926261.1875,1725336.25,1152639,6915293,739869.6875,4904803,1172785,1230757,1063269.125,3467070,971990.3125,576174.125,1929118,918177,18576900,1988784,9230223,1163748.875,1333016.625,47952868,2074152,2660396,1552051.25,2056814,781682.5,1070740,734495,1059424,8063959,2145603.5,2217046,625988.625,2188093,3220635.25,791640,37980712,912509,728876,53466684,763693,1027230.875,948676.5,654664,16373232,3518459.5,4335178,1581036.75,1241865,1140620,7459418,52960796,819003.3125,2161240.25,59124384,2083584,906103,792015,8687690,1441665.375,449955,557193,455798.9063,2196718,4519115,1313366,5459657,861972.875,1012341,6669954,1332591.5,1507270,3893073.5,3132453,789209.8125,561473.875,519573,2715249.75,1140137,1791071.375,12783255,340270,588856.125,1970956,6582088.5,11047668,2925780,1055033,839011.3125,1605522,624180,20828130,10394330,10716391,1026921,659652.625,1987073,1172903.375,609218,2595783.5,3534690.75,2171445,980773.375,14431245,13852986,2668769,859236.625,3276819,1242311.5,696522,19459304,595936.5,165472,2108012,1848878.5,1090032,1201781.125,6584966.5,8631444,1290284,8688676,2599171.75,6843855,2712983.75,5952875,1750099.75,2856934,666355,3050455.25,1881516.125,700899,15240688,1625308.625,2502081,8227025,11208498,5885427.5,908834,1866802.375,4590246.5,1976920,948657,763364.5,1097572,3468606,2827772.75,1959777,1834290,3347567,4253577.5,3085505,0,2254220,1718845,755856.875,8386493,1738376,7369591,1886007,3075425,3724098.25,2979413.5,6881405.5,473874.0938,455153,3300549.25,422133,7606084.5,831577,519806.0938,776427,3158743,793436,9974370,28229320,3574542.25,630040,1243282,27273538,864420.875,953059.875,430473.6875,1269847.75,2471623,1372706,3182301,2556131,1440554.25,849504,8976533,3133849.5,1302636,5549121,659265,13491170,8915523,1045838.5,1921801.25,1231546,1210229,45313836,2338567.75,2322314,2327611,1372834,983687,741999.6875,3590731,3635600,4376927,18074716,2108861.75,55073120,2105912.75,1456039,843520,4673827,620154.375,2248776,21042926,547084.375,936336,1688612.5,3735308.5,5328051.5,4759256,9546165,14856199,6345009.5,601813.125,10077532,19036416,472532,653366,1856230.625,784329.5,42566736,2495819,1083156.5,362823,378714.4063,957632,488181,867184.375,1800331,1692323,3778355,1983509,2337146,1187261,618670.875,2219190,1650279.25,6042399.5,8555790,45163308,1415494,1125724,5965171,7291418,887403,5851209,3162976,1605669,161311.0938,4367202,731092,3548858.25,0,1619177.125,478690.0938,641713,572139,876304,783651.5,1852926.625,11110405,28731502,609921.6875,4857581.5,586589.1875,629036,1586388.75,1675510.875,3371455,1270582.125,20361158,1633499,3704086.5,6858533.5,832985,4202335,1906384.875,606886,743528.125,16016644,4130053,13951571,1743198,2266983,1331122.125,35891484,1386983.375,6062770.5,1904480,2674034,764832.6875,3282191,662012,1406648.125,1024033,1612797,5662234,11521702,16246106,664539,1465535,24017770,610512,3022275,7273155.5,390726,25631512,650205.8125,1350190,1740568.25,924192.6875,1139980.625,2185193,471484.8125,15473117,1922953.25,7577723,563298,1119640.25,915286.1875,960790.8125,863703,1591528.5,1358519,8835104,862945.125,36681188,571384,673907.3125,2222097.5,1681424,3005926,10444297,2356607,1754477.25,901679,2331952.75,535649.875,1870069,13924967,1835804.875,1346952,1152368.25,885865.875,2799874,776248.6875,1615855,4952487,767720,2139713,11788719,15571642,9686734,656980,985236,1834570,8442782,935442,1748629.875,2352982.75,1832808.75,932083.5,1612965,2650112,5471735,1324965,8690179,827618,1079004,19490284,13720714,5571501.5,1457118,833747,17186624,4345698,1371061.25,942089,1100499.75,14361382,6578370,5987079,1011399,6579348,3017709.25,1360400.5,2313978.75,44293.5,5118939,1497496.5,2439061,4660015,9343428,57514116,8422861,13256662,2181833.5,1076733.25,987288.8125,692127.875,671004.625,5143676.5,1073156.375,982153.875,391058,8058913.5,2277316,1377184,454296,690259,4801472,5755308,823032.375,1416470.25,1460307,9274650,1261634.5,3532966.5,13223740,8890235,1870457.75,3361743,3459543,953432.1875,4476997,1494252,7657568,597210,866126.375,648022,3494383,1258978.375,1679127,842593,2771820.5,930611.875,991283,622051,3936421.5,1961982,1160989,672915,3678939,9683979,1381407,777696,318364.8125,2470642,11007235,1517410.5,2845946,497556,549438,2520310,660508,3736960,75073888,1166069.875,705621.6875,1417778,16865978,3652708.75,14856602,97690.39844,4028075.5,14064603,1113488,1387773,1388812,1392153,8355988.5,1528642,17695420,61762380,1305578,1814947,2671115,2303288,20781390,2473302,605971.1875,2115666,4407932,804002,7753080,524144.6875,2466100,3579008.75,2188051.25,788027,2558573,7802205.5,560823.625,8987012,984657,1100575,1459786,1078535.75,33270518,5475367,872555,770175.8125,872114.5,1963481.625,45252356,2802428,3073778,979442.3125,752441,4145920.5,1294615.25,1730886,41696464,1826203,3234787.75,1188444,1501767,1819329,1049088,10105379,16238985,8305255,978463,2549892,16467486,1095744,440658,532670,7945081,444458,1122837,2454812,1247972,1130097,701422,6235947,1128311.125,6424029,2960159.75],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle Lasso() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predLasso"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["alphaslasso = np.linspace(0.1, 1, 5)\n","param_gridLasso = {'lasso__alpha': alphaslasso}\n","\n","GridLasso, \\\n","BestParametresLasso, \\\n","ScoresLasso, \\\n","SiteEnergyUse_predLasso, \\\n","figLasso = reg_modelGrid(model=Lasso(),\n","                            scaler=RobustScaler(quantile_range=(10, 90)),\n","                            X_train=BEBM_train,\n","                            X_test=BEBM_test,\n","                            y_train=SiteEnergyUse_train,\n","                            y_test=SiteEnergyUse_test,\n","                            y_test_name='SiteEnergyUse_test',\n","                            y_pred_name='SiteEnergyUse_predLasso',\n","                            score=score,\n","                            param_grid=param_gridLasso)\n","\n","print(BestParametresLasso)\n","ScoresLasso\n","figLasso.show()\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[10313307.796421101,10313307.510375062,10313307.201426847,10313306.937466053,10313306.649282433]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[13782749.976254089,13782750.019129742,13782750.052986775,13782750.09894458,13782750.141743395]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"y":[6843865.616588114,6843865.001620381,6843864.349866919,6843863.775987526,6843863.156821471]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[7568256.647129019,7568256.368123629,7568256.106804671,7568255.83412993,7568255.563095717],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[7619817.6026367815,7619816.433319062,7619815.140279415,7619814.079614754,7619812.891110589],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[7378351.434123829,7378351.253091036,7378351.072514445,7378350.891324965,7378350.709454269],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[13571819.287157621,13571819.157559726,13571819.026254151,13571818.895346178,13571818.767630154],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0.1,0.325,0.55,0.775,1],"xaxis":"x","y":[15428294.011058254,15428294.33978185,15428294.661281558,15428294.986914443,15428295.315121436],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle Lasso en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE Lasso pour tout les paramètres de GridSearchCV\n","fig1 = go.Figure([\n","    go.Scatter(name='RMSE moyenne',\n","               x=alphaslasso,\n","               y=GridLasso.ScoresMean,\n","               mode='lines',\n","               marker=dict(color='red', size=2),\n","               showlegend=True),\n","    go.Scatter(name='SDup RMSE',\n","               x=alphaslasso,\n","               y=GridLasso.ScoresMean + GridLasso.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               showlegend=False),\n","    go.Scatter(name='SDdown RMSE',\n","               x=alphaslasso,\n","               y=GridLasso.ScoresMean - GridLasso.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               fillcolor='rgba(68, 68, 68, .3)',\n","               fill='tonexty',\n","               showlegend=False)\n","])\n","\n","fig2 = px.line(GridLasso,\n","               x=alphaslasso,\n","               y=[\n","                   'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                   'ScoresSplit3', 'ScoresSplit4'\n","               ])\n","\n","fig3 = go.Figure(data=fig1.data + fig2.data)\n","fig3.update_xaxes(type='log', title='alpha')\n","fig3.update_yaxes(title='RMSE')\n","fig3.update_layout(\n","    title=\"RMSE du modèle Lasso en fonction de l'hyperparamètre alpha\")\n","fig3.show()\n","if write_data is True:\n","    fig3.write_image('./Figures/graphRMSELasso.pdf')\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.633e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.874e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.548e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.725e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.487e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.688e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.782e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.535e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.924e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.584e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.586e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.746e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.841e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.977e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.621e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.904e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.807e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.640e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.658e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.033e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.970e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.696e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.872e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.697e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.939e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.092e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.038e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.153e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.736e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.110e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.010e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.776e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.886e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.186e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.084e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.286e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.265e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.817e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.955e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.161e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.858e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.347e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.357e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.242e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.900e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.028e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.431e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.104e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.433e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.327e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.942e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.184e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.508e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.522e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.415e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.984e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.615e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.506e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.267e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.026e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.589e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.673e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.601e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.712e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.353e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.069e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.760e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.443e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.811e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.699e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.111e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.801e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.850e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.914e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.536e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.943e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.153e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.905e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.020e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.632e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.039e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.195e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.013e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.730e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.129e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.137e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.241e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.237e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.831e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.123e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.278e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.238e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.935e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.355e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.235e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.319e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.350e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.342e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.472e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.041e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.359e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.447e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.149e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.590e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.466e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.398e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.554e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.259e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.584e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.711e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.662e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.832e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.703e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.436e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.473e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.772e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.370e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.482e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.955e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.823e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.510e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.944e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.883e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.078e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.594e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.994e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.545e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.707e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.065e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.201e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.105e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.579e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.324e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.820e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.185e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.612e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.216e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.447e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.933e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.305e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.644e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.044e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.569e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.424e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.326e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.436e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.675e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.689e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.155e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.541e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.544e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.264e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.704e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.808e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.657e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.732e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.770e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.650e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.371e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.925e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.755e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.477e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.759e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.039e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.882e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.151e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.990e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.580e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.784e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.809e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.858e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.958e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.260e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.096e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.680e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.056e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.832e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.366e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.778e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.199e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.468e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.151e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.298e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.854e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.872e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.242e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.874e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.963e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.568e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.395e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.663e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.894e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.912e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.331e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.487e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.052e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.416e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.755e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.576e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.136e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.498e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.930e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.843e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.218e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.661e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.577e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.927e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.946e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.742e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.961e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.652e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.295e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.820e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.008e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.370e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.724e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.976e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.894e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.440e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.084e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.792e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.964e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.508e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.157e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.989e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.857e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.002e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.226e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.031e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.014e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.919e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.094e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.632e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.292e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.977e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.689e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.354e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.153e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.035e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.412e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.743e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.032e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.044e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.084e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.794e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.467e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.053e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.312e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.519e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.263e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.568e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.179e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.887e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.062e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.133e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.069e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.842e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.359e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.613e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.403e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.077e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.222e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.263e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.656e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.969e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.083e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.929e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.444e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.483e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.696e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.734e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.336e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.040e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.301e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.089e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.095e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.006e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.519e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.552e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.769e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.369e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.401e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.101e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.802e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.103e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.105e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.613e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.073e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.584e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.832e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.457e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.861e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.110e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.131e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.158e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.430e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.114e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.666e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.640e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.887e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.505e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.912e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.482e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.118e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.182e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.205e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.122e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.711e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.689e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.935e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.547e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.527e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.125e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.226e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.956e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.246e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.128e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.976e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.750e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.281e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.731e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.131e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.584e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.994e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.566e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.264e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.134e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.784e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.027e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.768e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.139e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.011e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.311e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.615e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.799e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.600e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.629e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.136e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.297e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.041e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.813e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.642e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.055e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.141e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.325e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.337e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.838e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.143e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.068e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.826e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.665e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.145e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.079e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.654e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.360e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.349e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.146e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.860e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.849e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.685e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.090e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.100e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.148e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.370e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.149e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.379e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.675e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.869e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.694e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.878e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.109e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.702e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.396e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.151e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.118e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.388e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.894e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.152e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.126e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.716e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.153e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.886e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.133e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.410e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.403e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.709e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.154e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.907e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.901e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.140e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.729e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.723e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.146e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.422e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.156e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.416e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.155e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.913e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.919e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.152e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.734e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.739e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.157e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.157e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.432e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.157e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.427e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.924e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.162e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.928e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.744e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.158e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.748e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.441e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.436e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.166e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.937e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.159e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.171e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.756e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.933e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.448e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.174e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.444e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.159e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.752e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.944e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.160e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.178e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.763e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.454e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.941e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.160e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.181e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.451e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.759e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.161e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.950e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.947e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.184e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.766e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.768e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.187e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.460e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.161e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.162e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.457e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.953e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.190e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.955e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.162e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.192e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.771e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.464e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.773e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.162e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.960e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.462e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.957e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.194e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.777e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.196e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.775e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.468e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.163e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.466e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.163e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.962e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.198e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.963e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.779e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.163e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.781e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.200e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.472e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.470e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.967e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.202e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.163e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.784e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.164e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.203e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.965e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.475e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.473e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.782e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.164e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.969e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.968e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.477e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.786e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.204e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.206e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.785e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.164e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.164e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.476e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.972e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.207e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.970e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.788e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.479e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.208e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.164e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.478e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.164e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.787e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.974e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.209e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.973e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.210e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.790e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.481e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.789e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.974e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.480e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.975e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.791e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.211e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.792e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.482e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.482e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.977e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.211e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.793e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.212e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.213e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.484e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.976e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.483e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.792e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.978e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.977e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.213e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.794e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.214e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.484e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.793e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.485e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.979e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.214e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.978e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.795e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.215e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.486e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.485e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.794e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.980e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.215e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.979e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.796e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.216e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.486e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.795e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.980e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.216e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.486e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.980e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.796e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.796e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.487e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.487e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.217e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.981e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.981e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.217e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.217e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.797e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.488e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.797e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.981e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.797e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.982e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.487e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.217e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.488e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.798e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.218e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.982e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.488e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.982e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.218e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.798e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.488e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.798e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.218e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.983e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.488e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.798e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.218e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.218e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.489e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.982e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.489e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.798e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.219e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.983e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.983e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.219e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.799e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.798e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.489e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.983e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.489e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.983e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.799e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.219e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.799e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.489e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.219e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.983e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.489e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.219e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.799e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.219e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.983e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.799e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.983e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.799e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.219e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.799e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.799e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.799e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.490e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.984e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.220e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.221e+17, tolerance: 1.844e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.166e+17, tolerance: 1.033e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.985e+17, tolerance: 1.797e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.491e+17, tolerance: 1.498e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.800e+17, tolerance: 1.560e+14 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"name":"stdout","output_type":"stream","text":["              paramètre   ElasticNet()\n","0     elasticnet__alpha  283309.610184\n","1  elasticnet__l1_ratio       1.000000\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predEN=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[3636160.191941049,4032041.877357489,1284986.5933610336,723952.8888553448,3156537.1602833667,1100909.4112240584,424073.49659616104,4354083.460813515,46687889.37919703,1715997.674913309,2293778.9607549664,13646860.170259804,1013180.4641617788,1684907.1598786437,2300948.370586102,622866.056368026,7580508.18081028,3893852.2691233195,3331502.2372812787,1037683.3089054709,5222415.344209952,2112183.8969892175,4709951.446328951,802077.8248708064,1029587.0126271294,59930320.18774167,3954978.1380793536,712624.4734747086,1042178.1010412558,11537756.308624389,4160536.7734101946,1541524.503823152,516604.44918184937,1172541.4250266906,2205248.160740632,3694533.772791645,9213959.003457285,952047.2399983802,2585807.0984707945,1480514.7117461376,711701.0108740532,1162380.934601401,1486763.1716042766,1150037.621578935,6318721.7915386055,1878464.920976052,13702829.456855467,625756.8173604468,3373767.561002768,865488.6197095253,1937566.5274180088,5099538.421261374,16714878.145980036,1805080.730050253,1855809.0014336715,1958497.138515578,28274646.037164275,3702735.113963711,8757355.86561072,8080930.397690274,1277938.1029886238,2744857.5329111116,1096722.7436947892,20030566.310178395,2759695.106174239,498443.32177525363,4763362.085424066,1511388.5372154615,-284283.57854041085,639702.0138492375,1488610.0968055876,-165966.6786628808,3946161.8697979595,3837868.778265941,-683180.6418889104,1087488.1176882335,5683197.100620001,8420152.374371538,6479531.928892037,1155454.965096484,587895.7619524598,3023189.1607487015,2174867.152397956,7104370.863912266,64908.71659378102,3346124.747103056,2262381.2323326766,624773.005544936,-348248.45093952306,2050691.6072172082,134414.36459682626,202750.5970453387,2762957.399883961,8321739.427550461,1389854.576686658,2110070.2524393615,1273289.704159027,454362.1586787719,708345.459685374,2408802.187362438,1617894.860897368,34864.18502726406,2683631.9624876473,2010950.7183438079,800538.1130571195,912677.558603025,3954978.1380793536,4465144.925846062,8445362.185325582,3197046.0826258273,1865536.4445668738,2567109.901316892,2503895.965792645,1497753.2877709696,2465541.18203912,5170671.263965812,1666099.6086515891,983106.6692067266,2121133.987441543,5321435.569115108,5249996.701362126,1791152.351378159,3638162.311603281,5009834.253539039,1344434.9724765208,5209856.252841036,739589.5814138516,3064436.8531716866,-241312.39300249936,10006694.94178081,800846.237663635,1803880.2286694008,64908.71659378102,2558034.647699752,4533329.374038545,2131000.8564290325,1354030.5100637516,724752.9193696158,462766.57956362935,4792975.060297683,1544665.1878842728,927426.571084013,5317471.642862621,689009.2091707706,1507602.3405527733,1100909.4112240584,20030566.310178395,794627.9524129238,1772697.3673746698,14556634.778816711,625172.5651926259,938626.8576814467,1042178.1010412558,33949671.07516895,1154408.9816283355,4577130.396785798,202454.56431062357,3443087.6052667913,21455506.232249297,1616118.4088160677,17235782.70209408,5397351.157819329,3296103.445068743,4363995.596466849,806247.1123185148,15033232.598986616,-2094021.5892147836,7315844.710681284,14477695.82719406,2984157.77856729,59930320.18774167,4535390.2551485235,1122928.722038264,5733687.249604451,1094199.220065592,764615.4178916179,6290586.356826037,11809843.459726425,9215188.677267922,368665.74055682635,4274372.644128045,5708403.536327552,6856690.217628092,1455519.049875799,928467.8578553437,-167435.38495005388,311411.0593161809,2804362.3438763246,3430312.7355514253,1966748.8568176166,1551682.5924303634,1727663.4782889967,6013271.136549249,-126100.31084476598,3933813.71885908,1506987.0025586337,1080738.324747772,4243637.333775109,22159817.508815672,983999.9571999547,460642.61558212154,2340615.4384228885,1194949.9010160067,-309648.62545101205,4366092.367039302,1597855.7224631421,2367408.193415751,811434.9717448552,12509274.944681847,15438099.665532531,2443796.2779544406,1361941.20260307,3140623.428944663,4051188.639017379,2412226.304154974,5209856.252841036,6090623.795520003,12610085.063628227,1185408.6410021218,1263777.1281533828,2710262.4160884134,1738684.6802820088,6282338.86763455,1516407.2323042122,11656148.197151018,28618224.18948367,13646860.170259804,8331159.843056876,4337584.566088099,835568.4314482848,1358617.648459602,2312185.1298965476,3189288.9967803205,5655119.744269727,1062123.9819965246,415392.94814999844,4947993.345121434,43606.90512606432,12892924.656986428,654754.4542399235,265176.66884965566,3327008.3563643857,1299669.6487114572,424829.5241930047,1606536.2709093045,5243069.820638318,1052889.3559899689,2026958.61838036,1482730.1107688192,2550622.2621669252,20576711.26578781,16833234.96555776,5056068.644005563,3857875.919654956,1665852.74447882,807741.1213422329,9076610.851351267,1992851.7625898502,2312985.1604108186,825102.2182345577,1244969.5769263282,1156441.5105686695,3248052.304008334,1531735.800256203,2225811.202127824,3950222.1797205135,329387.4942026455,1484977.506836712,1878464.920976052,2120288.690261428,1775038.0209215195,814513.4841533375,785116.2876261713,2078363.4881916647,12872257.918392126,-383278.7693306883,2325420.81969335,209736.9157651104,879494.165413171,923754.7423224754,1179865.46358011,1448007.9169841697,1197749.5522065093,1956035.7794311203,7731956.047317794,3946161.8697979595,650291.5336188921,588766.3245161669,3575020.236293046,1516407.2323042122,6974102.288004974,2701793.089894243,1873909.7796254123,3077550.022100996,1812222.4779016199,1120732.7713118345,539168.0224641645,4977451.802082347,7393753.868350295,1034850.7494508661,-395130.17644539895,8314230.302857451,1266270.4771751529,832860.2152989563,671807.4265257325,884511.9492830303,4926618.953788595,1925899.8128234297,2344506.3209194927,1796548.2519528235,30240096.998920597,2329545.315569981,997359.0790831414,887035.4729122277,2089270.0815123601,3629809.830723265,2419926.6856611925,981259.7440054154,131547254.25830708,3178037.6606508563,1121410.2809586124,1259530.1114092588,2093970.0061427439,13030262.369364295,7218667.18170378,966638.8603076299,5017160.693910536,4302538.642020284,1363788.1278043813,1064371.378064417,2150886.3881694465,1639103.4148861272,1660527.7472213367,211119.34297616198,9314616.426928744,983999.9571999547,2329545.315569981,2645851.8603764335,5452049.407166673,1702514.2097248463,4747232.575738912,3348033.129051905,3470514.444506262,22434520.607584424,4608682.099814257,-215086.05514388112,7236457.101632332,12010098.670065256,3335073.566816408,7186161.477907244,2506019.018555261,9227934.374553505,4465637.245174053,4782417.412042782,2131277.895209229,932593.2649508666,202810.94626019336,3272304.8868455086,5766022.171654563,1309809.0719622693,1926978.7045556805,4085110.802287757,447744.3137803711,4725561.379230933,7270470.699943883,1569907.7139273805,4079999.760938941,1115716.8098797584,652137.6731317688,2983911.8256134125,663101.3455988518,15967304.084923029,4051927.4090979025,7688064.496298123,889005.8301999234,121362.45610126364,145896.3866712749,3929814.567464917,2010351.250735843,11059746.786259113,843375.9450678239,477945.1046639127,2122598.2579819583,890760.4091411689,1284986.5933610336,105388.75675117085,5278923.297540054,10044770.864316354,684335.4320683293,10235157.663791683,2852609.7675545677,13796308.416091088,4578638.479466632,4724803.537351232,894053.78867721,3638952.1104697557,3473408.264394613,4347434.530088795,2344014.4150117375,2588145.9295798605,2775916.9621194582,661002.9140980621,6067945.3300383855,1842481.8765956955,466522.7346870366,312272.35026419815,330741.60227730963,1345751.343703062,5492098.42064659,256371.77709821705,2214112.490488034,9376303.728652537,1637197.051688853,1552975.4400712813,2380335.7586060376,620678.6842757326,681965.5151329436,4495065.114107302,509770.82593699824,6711592.045246022,11025300.720035767,726845.7975248045,1070004.499928416,762676.146430241,1699990.6860956487,375867.8376230481,1761370.7744318168,4764996.877154493,5707079.602860316,2010950.7183438079,1144620.2780613864,293927.44155636337,1456873.1579504632,10961983.182676006,17966414.100384433,2185547.3215203495,5016115.621661279,4887479.351369441,-238328.76301602228,55764951.60366242,7475163.813265301,45241066.70326695,7698990.137706701,1463522.999894075,5461931.368212581,12501426.42379517,1858302.3504554415,4160536.7734101946,3620191.071298483,802231.4315646186,764646.5037179366,15174395.352396397,7765108.354681328,1763648.3451071368,11165364.83360107,1031896.5803476598,792965.7197317439,1169677.2003654717,1088473.7519415275,2617481.865673281,7268501.2538750805,2843237.3857072494,976088.3534417441,1179927.6352327485,3783493.9047540464,1871416.4306036422,3296410.658456367,85562.28180325497,554773.6291963523,2327175.3986345953,766369.9968328632,1596132.2293482157,4643004.733951217,4381356.693359174,26833572.36450192,1475342.4099635747,7393753.868350295,1460630.0912246155,5389001.647090554,1078869.7408947095,-103346.16924566496,16009788.831866536,6055497.483995776,485332.6081549125,5454113.0219333265,-64561.6512370226,5733687.249604451,5962633.701506924,580877.4461874773,1786818.798353411,2245695.8226493467,4159731.089695924,386149.35831664456,828149.6448167213,756027.2157055209,2645001.0511728167,1298222.2831578357,287432.11752545484,6882062.330108149,377900.3665633821,1830752.9903484778,1374100.7343242965,4814519.519909044,2324191.116815126,3519888.607815016,90845500.11439231,3584255.5626453063,1717505.3896817854,857977.4868178961,3296410.658456367,444544.8947710013,1166044.6103965957,684335.4320683293,2949866.230256649,987908.6747301354,3006443.401850517,3489476.5136460206,281830.0814877753,586404.5324765125,325329.7840347497,7943523.151565768,1293113.9754656944,1806374.4889100625,2562834.0138479588,10301357.999737898,2605907.4973387667,1979245.7765338942,1780577.8853065611,1375107.4783895016,2804362.3438763246,817930.2957757632,4439377.035227096,26305010.794004627,1038636.9461135538,8376841.978400793,907813.3814269779,2935491.299512741,858100.9189042808,2526860.0099415416,59386425.49805092,-294134.4537599981,3482982.1008340036,5492158.769861445,1361941.20260307,10058.662670641672,663188.1385133164,285584.28110525245,2542280.924153598,1606536.2709093045,6931283.797941983,4063346.3483008216,7963797.490186296,1119686.7878436858,3765529.6595893987,4600986.274402497,1809164.3888642082,1651785.9383414276,983229.1900742194,5942685.998113872,-282899.43345931964,2199337.0888775447,1222443.7808137818,1013180.4641617788,9936384.707169034,253104.56784405746,5628739.799717296,2051645.2444252912,6662803.045323981,1609152.1407985673,1505878.847437847,27952674.780725267,14943989.47432672,248451.69552457938,2690436.3223439627,2372918.794412257,2402899.3318883525,2123419.812902367,4724803.537351232,2755477.3528186507,3146677.424296758,12226213.806334665,776374.4787462624,907443.9963867161,966330.7357011142,2911357.8398093116,2547020.7580243684,585795.4188394069,7059153.193525353,6131997.653686048,1360156.4490543974,33630589.02600865,1370683.9227018708,1017828.8629913761,18081366.865404215,1883883.1757124925,1048245.5830738549,1265285.754140751,403572.6268616072,1553898.902671937,1436694.4520918045,2020278.601829321,54599601.24686476,1176849.1228242659,998620.8408977401,1516714.445691836,2744057.5023968406,6353689.027058242,8433725.45216339,1132824.0531263875,3655699.781764024,5514572.381325514,933086.0820775134,11796678.142552285,23075754.355823833,35051580.36763334,4420775.521786576,6371111.280910168,6757673.740237627,1709964.082182729,-164971.2993168193,2204878.7757003703,1379979.8091421733,6426213.7494738065,562407.2829554742,744604.1308235447,8701625.597489139,612081.396197489,6278734.038492436,4605851.362797435,1347461.492097801,5390546.797963012,1182884.2061540328,2801681.5686777486,2127061.0530725326,1813053.59424221,38019498.64231808,-38181.706684591714,104872.98876704974,652753.9223448001,1015089.5610157284,16833234.96555776,1634518.0989280601,3812874.027614494,1480330.0192260067,917263.785779984,10371447.853096453,55764951.60366242,24514892.78536101,11902960.486917717,2787337.7237601597,2967904.836795752,4436829.354707822,639702.0138492375,1017316.9785553082,907567.4284731005,2476560.5615943484,4107395.514352092,291477.2255552274,-18265.089117859025,746309.0240056366,2235075.0915229158,19441376.68282464,3811702.7896221774,1232164.5326035349,1927065.9429231854,1430062.5679240243,473737.3354310836,4604157.133071044,22999441.614307795,4192169.381092274,3077646.9244555198,18001942.70321559,5453584.132085403,2560227.184432635,131547254.25830708,5770830.519350762,2899855.5217250953,10072686.875806905,3049815.969473901,47774834.12355718,810374.3136041951,8174948.346603405,11350392.675339913,1620512.5532244144,1491687.6979951784,1020167.6941004423,6786418.62154126,1272719.352872289,2290083.2879145606,2791339.698769298,2042565.1363314393,7508724.050913021,5729178.384378735,40872452.09609789,1727663.4782889967,777697.5009946073,2324189.2324861786,4016529.5281042587,3313187.503180871,7052904.7336672135,1235242.1337931259,848065.3511645622,4771430.029532763,6082221.197072929,9587223.49786116,821162.4148780578,9565706.819265885,324430.9707665327,1345751.343703062,2767821.577060008,3005151.4654284907,884912.4201496113,4401426.917619718,5230275.045536384,45241066.70326695,6278734.038492436,27981151.414767355,444544.8947710013,248451.69552457938,4206575.397662501,3533907.1983216237,423580.6794695142,805032.9051929042,1215695.8124855463,411791.44400744163,6395773.303135166,799461.0437626517,991971.9101730199,5597746.187037155,8568812.992370652,609011.9696811533,5513862.874633525,503217.0631260318,10562035.422749342,12044076.50692459,625172.5651926259,1476913.207603581,1636578.980038038,8016783.910405183,25130391.0033165,23942841.087921202,1995937.918702282,3618268.4606111534,5007340.904517269,-311002.7335256762,986800.5196093488,796291.0963129955,519066.7123773005,241662.47140023578,940226.9187099887,8673122.406077843,1163458.9151147602,1876741.4278611254,1939536.8847057046,1937566.5274180088,1296191.576655285,2297685.8558473634,871708.72739802,10959347.894778643,787917.7612544573,1927065.9429231854,2808975.165034719,1923499.7212806167,2246064.296470717,1457705.185509945,1859565.0234889318,861732.5976542649,1114952.0626923912,2306736.70055268,31802658.802101355,2979725.158084143,5499977.116140698,221897.3587052282,2054046.2471869958,68306411.64849098,1784672.2065757648,5597680.370508949,4199750.470471071,928058.5689379408,637731.6565615418,-265450.1735555739,625756.8173604468,626773.5374400595,5554615.92749208,3746968.061316222,2070919.7109918152,939827.3590622991,79036.78316491959,2777948.5798409004,612059.3962633167,72307355.72829698,921255.2897211639,842093.9300866202,22299580.361950338,922904.4588145884,647736.138474941,779698.9441086224,823546.885314814,17863667.481636208,3762113.7591858646,2611172.1453813952,670114.1080182334,10058.662670641672,1386074.6623064997,3782435.4037136366,9106992.770912835,1959206.6380996683,4926565.055316083,46355250.34429722,408623.7274823501,2656544.3336810116,-967704.2492722501,2172187.288418271,5390546.797963012,848065.3511645622,773388.3125978454,538675.2053375177,4112659.251175829,2701857.6633649594,1663607.1708487107,2617518.4188129497,6518741.057183927,2417274.2626322606,9493612.909787167,960328.2287968528,26123698.462407954,4227571.818180377,3993210.003761851,1178173.056291503,841170.4674859648,504907.55997984274,2245695.8226493467,309625.39454861614,1311551.230433595,6476264.511210445,4732457.1741284225,940226.9187099887,-770760.9429090922,18966499.63714204,8035253.9415764455,5889493.64109722,44991.18780815601,485084.1998920669,1652185.4979891172,10205111.412740914,26335448.43673828,2241262.290947308,11193926.813353261,1951603.3872980457,1715997.674913309,1161783.2244339692,2808975.165034719,559944.2635363014,1880804.66330401,8397372.11152388,535873.7317092319,2937128.983893349,11556811.547882808,3074906.8970206664,3619746.9119910942,2934413.3189993817,8347660.560220076,2705763.9790770616,655497.7233093728,17807518.865374636,4715342.030189976,860809.1350536095,413146.46330099786,2968950.8202639003,1826258.198212693,871708.72739802,2147377.2302869554,1917050.7074583918,1488148.36550526,3035717.1662912983,5968949.252527807,12106459.647056995,7153530.16009346,8174948.346603405,1049965.36149419,5739124.666321956,-165233.5681029586,2078363.4881916647,3089925.3321686722,1254081.682065391,1021441.0645906772,912061.3093899935,1244969.5769263282,6188944.210320178,7632291.848851159,13348948.401631642,1406268.3186534296,4237573.566437101,4185705.142887685,8390599.748712776,2120474.2940004505,9029358.829586238,1663860.184669131,1614631.6559687546,-27899.27477210434,2205494.11369451,2676458.0693136267,1745370.4514585622,10003642.959104186,8632640.207563493,4910500.763534818,2214112.490488034,2209988.905830295,815621.6392741243,26010883.329653054,12217059.380789215,5135518.3996017575,4623180.462644549,4539648.369764147,3837529.567833106,2690436.3223439627,2821382.407898031,662111.0692188488,2897474.8149731588,2946480.5044605425,2795893.928901046,-199509.71180022927,1251864.460604926,4384249.6020286335,690845.7061938599,2757406.204426791,368496.5584348962,9849424.704794705,2746768.4522028444,7693200.792697687,1364526.8978849058,2153288.3021500427,14471169.105362972,1064310.1176306705,2513684.669359594,2911357.8398093116,2615264.6442128154,4815045.816453351,1569167.1214090728,5418304.392003167,5813246.716109388,1070004.499928416,1497845.634031035,26454056.92412493,3268245.0491363723,981805.745309755,5357243.617562341,1157856.8790770802,7632291.848851159,8742748.271448106,2023786.8484929204,1442621.65929294,1006223.4088305435,-65516.19966399716,12058555.272939065,1437452.0911670523,3619716.737383667,1327866.3438577713,639670.9280229183,624198.9937400678,1119686.7878436858,8507449.764740918,839740.9739287016,1284004.3918860804,9028231.818118885,408623.7274823501,6195312.385734636,5901992.3832512805,2235075.0915229158,536291.8094363315,2812547.341539165,714225.4457221422,1699990.6860956487,3188749.514303127,825102.2182345577,-418188.35529110115,4357839.482868776,3378661.001567351,6770760.468649175,12631880.603441479,809744.386894031,8757355.86561072,4854015.026982124,394461.43294143607,3035132.9141234774,10377894.910199137,466183.39118605526,-62806.16107688518,2438054.572965089,922743.300950171,12622830.669955056,3619716.737383667,-356744.30686555477,327835.3603514007,2472682.018671595,1149329.9373247297,256754.5311802437,347693.0318334915,2688249.2754909247,4100809.666498902,615137.9086717991,5444385.578800124,4109119.9186859103,1895976.104220103,425059.13084945455,562684.3217356708,4078706.913298023,5020729.456074362,9070121.50288407,47860403.38776685,1146159.0786561817,309832.621154075,10111904.772946231,2701793.089894243,1907154.433249013,11435810.020634599,8128768.838038387,667374.8060425855,2606030.0182062597,478495.61838220153,1272180.6378193488,4206575.397662501,14040898.017083973,1784672.2065757648,1394570.5182325314,1188331.144898714,980489.8880985714,3300597.325985636,23722.284604541957,797583.0327350216,26099504.65348967,10291498.52313371,854470.7307534833,663527.3489461513,1956035.7794311203,4781338.52031053,4623180.462644549,4038690.808082209,4178841.3450354063,1105096.0787533277,18435495.56246343,4015233.946806666,3025750.1108093685,5422657.014275664,1581787.473211735,3856682.2250783737,1379979.8091421733,991448.9184389457,1087488.1176882335,8446102.286145726,10801485.382682066,6780391.831432644,2466741.6834199717,881618.1293946789,1999869.167135941,57987223.415345915,1778270.140023814,4814519.519909044,1868522.610715291,2562834.0138479588,1674626.5504039396,3312201.8689275775,2079040.9978384427,4025577.6391529003,2117827.3382848687,774419.3034752824,6474693.71357044,7017375.243421601,9347043.919807294,1458997.121931971,3850303.5263295807,19340441.309354097,-127046.83065491123,5072875.663337495,7340169.763152089,3216069.4121993324,9084367.937196774,1277414.200035658,2174867.152397956,-161994.13903796952,1758937.7746249018,681995.689740371,7731956.047317794,538582.859077452,1383207.2119790372,2004086.9204915292,5520899.646839531,944906.4033659047,2235321.955695685,1358617.648459602,1386598.565259466,1387613.4629012956,2768249.490451933,859362.6807188794,9787984.267243683,1051576.635388935,53411753.624469675,764646.5037179366,852802.1903889771,1700609.9165070534,4331931.412913428,845061.1823496711,21919603.88709713,1558454.0440225764,3710739.0639819875,1327835.2580314523,3946158.9442776293,443281.5186897968,3779382.5098181237,3722407.783722627,6270868.930710698,3227214.0462787286,2271711.648054081,1166044.6103965957,5439060.581542641,1180327.194880438,528116.6458637251,163835.04399779928,1066157.0428319816,736572.3294391157,18062694.741964873,21605291.037316967,11446956.477151778,-4536.5821944102645,1468041.7256733794,859362.6807188794,5912149.560639599,1698944.7026275,1903891.2283203993,6620138.1619548015,2594025.0043977373,-130496.88092383044,4348173.300169319,7844249.0995575115,3175769.403300019,1488610.0968055876,10193757.775346264,691262.3127921375,381992.63822567416,10674724.142219486,5348806.279741321,2419926.6856611925,2777486.8485405724,710962.2407935285,5278770.683000822,2399359.088179542,2530122.3036512635,1658880.7534819148,899466.5992082672,21455506.232249297,7623514.097050076,14131029.789345741,1272180.6378193488,10248193.561298711,6479531.928892037,2771484.3416363113,1951449.5522541613,979412.8188041041,20277595.94809035,2801681.5686777486,2519456.220870288,5562898.738728337,8324717.996587127,107002892.7701511,3620191.071298483,13999905.926905096,1050931.2046407924,1338917.720458211,1863936.383538332,1020045.1732329493,960112.4504504027,7376890.144679075,2120288.690261428,2249759.058092231,91134.14323350717,6632236.43324228,1586711.9996026373,5359928.400304455,662049.8087851023,790688.1490564239,1361510.5571290613,6487838.87954496,-244756.28137273388,2211496.620598771,3160416.614425012,10820980.31465549,-39751.593105706386,828149.6448167213,29316510.054959744,25189028.909006562,1018321.680118023,3489476.5136460206,15686231.332672011,1795969.5361300819,3011445.1365950685,984615.2951940943,11888522.47330228,1225515.6018788146,2550622.2621669252,1143881.507980862,4011540.0964040435,726507.4983108614,473933.5652728542,2176897.858900506,2565028.2787371525,3960934.3458881183,907443.9963867161,568451.2965432205,2314063.140924178,4238755.884267094,1547311.232380963,1089089.0899356673,516583.38387040957,8328991.92209561,1322288.221894998,1945077.660309638,1422736.127552526,1272180.6378193488,2471019.7859904147,1958497.138515578,2633872.6024722,1111560.3169579168,743929.8556369329,3574589.5908190366,364318.37029216904,7765108.354681328,3584255.5626453063,1377086.9004727136,1131507.1091320773,2975507.404728555,13417563.467267143,3312201.8689275775,33428722.83516201,1623066.2514610393,4966863.979220019,4600986.274402497,3960934.3458881183,2744857.5329111116,2949866.230256649,1441081.9474792532,21030155.795972005,1406780.0188497256,8475188.922932826,12222171.333356922,2123122.1609349246,3075333.7118594227,5313395.248770165,5017160.693910536,20879061.23662974,1033065.995902193,4781338.52031053,4640387.952843063,7963797.490186296,146541.8992728421,8460388.260298062,504907.55997984274,1162350.7599939734,4755680.179288011,467864.2874789825,1260299.9673161025,3439334.9198845066,5554615.92749208,307286.5634395499,21130915.76858308,1893209.2367602221,149659.98225395475,233778.0292084748,1524039.9748444427,21578107.227785587,11977607.930257302,1210155.036881613,1651785.9383414276,1993467.10058399,3681556.7345823795,8401422.547817225,5262462.535252085,2335400.7064938685,-238328.76301602228,1290742.2360925255,2542280.924153598,1272719.352872289,2009596.6102691437,9013437.347277308,1311551.230433595,3939387.492685416,1977245.244638771,3430529.425116767,4524247.4435069095,333327.2975591454,2886599.339124892,7524515.261384232,2767821.577060008,2249759.058092231,3812874.027614494,27815452.68342043,2042565.1363314393,2102773.986675291,558283.6982977348,14528766.95388588,2181329.568164761,766369.9968328632,2778903.128267875,-517856.5336347413,2119458.4851397295,918957.104287483,8593207.44329661,1790552.192612533,4186045.2645394113,6269685.016224902],"xaxis":"x","y":[1437220,1614322,1416488.75,780410,5912066,1318947.25,281191,9146727,53332648,600528,2842760,6711975,2177075,8054334,962971,766628,3009967,2161828,2962476,860443,2075833,1121259,2709429,1406687.5,764094.5,26689658,5276777,1372475,1078936,9162048,4678999,768325.6875,572654.3125,562495.8125,2504814.5,4696639,875943.3125,436604.0938,1210262,841891,437372,1267348.5,2214604.25,1696448,4933374.5,1691021,3920913,1813864,1545519.375,1988760.875,1471914.25,5827595.5,6508595,804620.5,4236396.5,1524620,25307744,1661448,15232788,9512829,847298.8125,1489363.875,642052,22104512,1904641.75,131810,2462408.25,1261714,577642,479358,1355062.75,715064,63835192,1479728.75,592739.3125,557863.6875,5543575,2634166,2756494,433871,508057,308400.4063,1430356.75,12187231,592899,2216979,1884194.625,690750,673834,2478045.5,847767,647792.875,1810353,13458948,1072855,1606981.25,2291194,347437.6875,1423687,2045592,835258,2978555,2203026.5,1750419,1070381,614044.6875,5337334,2409429,3093378,430500,5767765,3418003,4618360,3546459,1310226,2729167.75,1639849.25,695955.8125,5061274,5454482,1498457.875,9190442,2996502,2890379,1795493,2364351,610744.6875,1424157.25,1003659,5907068,784584,725487,585851,1513700.625,4547129,362874,917100.3125,674961,619545.125,1906597,2001936.5,980356,1883132,721549.625,939840,1098965,23230284,533669.125,805751,7860942,883197.6875,784584,1049436.25,48143224,745575,1792425,603223.1875,1938613.125,15088676,1297041,13631141,2131817,1153276,4397572,960987.8125,19609472,7768294,2757855.5,7559797,1325972.75,25475474,1841968,559805,12051984,1469556.25,1314300,3375091.5,4665069,12726789,508264,9796649,1973429,17074142,7542332,531737.1875,1672142,1739693.25,3307792.5,1436773.875,843622.875,987987.3125,9997511,25478086,713660,3594908,2755829,1147582,2349950,27717026,1982967,429591,1645748.25,895901,947534,3528086.75,841821,973242,188745.7031,7564643,20311228,1593160,495481,5570472,2594119,336745.4063,2284133.75,5229981,13146645,838394.8125,2520919,3218565,1514279,9177040,1048781,6456026,24882924,6772289,10813735,4993962,628676.375,888561,892227,2842819.5,2265836,196410,541451.625,2371304.5,1325206,17812788,481206,519317,971155.625,1509492,1419453,1093724.375,8141155.5,797435,1162131,451738,1162519,5696695,6769410.5,2467851,6376820,1821047,600543,2637656,1299001.625,1857387,436848,2062419,1082076,1145535.75,3242077.75,1021754.688,1298039.875,395346.3125,1489392,1858224.625,1406987,1056099,2055534,840549.375,2126288,8819864,956598,10420096,350264.5938,507976,1565475,788976,599390,2345314,402136,2220053.25,65225380,880590,1916309,1241023.75,1093627,3879728,7442314.5,1256575.875,448676,1198291,1452804,645451.8125,2557933.75,4785692,522972.1875,919746.3125,1118392,145573,1093757.75,1493131.75,975327,2015280.125,861643.6875,1199742,699382,18760766,6471427,967492.875,2788860,2104255,3632613.75,5657046,866343,59991984,3497215.5,462192,682249.625,13140574,13349651,6856153.5,3033340,2646879,3925806,2852102.75,595978,2559486,841242,456397,618366,1555420.25,2058532.875,4383100.5,2321336,14697487,2241280.5,3059758,1260212.75,1791715.625,19594950,0,1689631,4863803,8259071,1820085,3983784,14084606,10287986,2966309,2059083.25,3040978,667871.125,869835,4704936.5,2091869,2067352.5,1199392,1878442.25,1297872,4917420,3591653,3318151.5,2105451,1391569,520206.6875,3292049,540465,6219841.5,2014500.375,5328224,421430,844000.3125,649874.875,10557910,4841347.5,4606967.5,962577,439521.0938,1275236.125,1426023,1299232,2655387.5,4252035,6320060,520011.8125,22388122,3371659.75,10769230,17225444,8787353,722951,3493093.75,1403094.75,2994822,710275.375,6454983,4328527,722891,2311532,1539191,1186835.125,2213115.5,456953,2601945,3986982.75,3537977,2339234,5417598,4169431.5,1222949.25,3646053.75,593543.3125,1266387,1951856.25,609368,5535626,9097980,758322,1509775,1417728.25,2076289.625,849745.6875,2228083.75,4471029,2334260,1773880,892086,1392299.75,847419.375,3644982.25,18558162,1162007.625,1686765.875,1253745.5,967835,56762408,11061916,65047284,4217623,928409,1742870,9734582,2046914,4817062,9329202,690304,646994,8241967,3612830,925293.3125,13214416,2019672.875,2717863.25,706505.6875,1242666.25,2154119,4567879,1128634.875,1176615,747994.8125,2477499,876569.6875,2666634,627854.125,1525624,1245187.5,688145,1820814.75,2468322.5,1885930,36378588,826488.6875,4391348,2447861,4045980,688641.8125,977150,10055690,850568,609101,5669431,2160420,12086616,5401277.5,835093.1875,694189,1378155,2625396,438557,3766069,4777648,1248599,579888.3125,528411.875,3155496,1359579,1769137,1686262.25,5917000,1004253,2564004,163945984,84980760,595327,1327591,2551022.25,585077.125,868163,507947,1331031,864751.8125,2093011,3586479.25,2892779,2946756.25,724153,3603866,2998208.25,1134195.5,2213421.5,4180298,2828950.5,1034827.313,1552645,2168281,3168809,730765,27137190,16614404,1360340,3091348.25,12015936,1445538.625,652449,1245197.5,157606480,896403,2512443,1933811.625,474811,1533245.75,1369190,1180231,3988729,1032857,836348,3185509,4192895,747326,3835202,13962749,960550,873776,1989017,686019.1875,588965,1072500,3996541.5,1777554,7655751,5689125,4728488.5,2654804,4585070.5,591249,1227772,21346100,6481807.5,489557.0938,2926166,1712279.875,1815350,952635.375,8873485,2920918,4329966,18628834,739711,767642.8125,504656,715446,317581,577023,4983934,2982450,972608,10614234,576038.1875,905926.875,20657852,896976,1965529,544172.375,638201,866242,914023,1111858,41655852,2440980,3605755.5,1093743,1516377,10340282,1015060.188,1045640,2726269,12087498,566000.6875,284867168,33729336,39403320,5974993.5,11924933,2993610,766381,1608224,1056782.125,2347893,4405368,357915,633408,4634655,8204897,2271263,2536760.75,694479,1271021,1315298,1484807,846274,1348497.125,11320378,1840259,502254,666385,807185.5,6860521,1758581,2971013.5,1162226.375,876871,40183424,48479988,30083496,6649938,1435558.125,1259959,4369513.5,493969.6875,1696978.75,777915,9143764,1052244.75,758266,1109851,696840,1608884.25,10084950,1224554,507217,4855036.5,919266.125,631195,5375518,9341427,1998767,3730953.5,59107620,2751694,3473109,59757440,3714139,5424718,6344171,1427294.375,131373880,2520483.25,6012303,12931002,5040382,9620496,267135.0938,36436388,1380283,1240875,1240924.625,1133028.875,6205886,2503948,32552428,9393108,811870,2786648.75,3913410,1084048,6298131.5,523133,439266.9063,6981428,6668568.5,7574092,1872278.625,2757588,5170809.5,2640980.25,8381744,1750362.25,781890,2100813.5,7335905.5,63811044,2330242,24603420,609257,549834,3322250,1695593,1357116,756196,980418,493000,7325166,703847.375,1356874.75,4826430,2057288,1360847,5327249.5,697824,4039667,23262244,842923,713326.875,7067404,2577115.25,2512319,25909504,1647746,19061780,2125323.5,636266,4729846.5,1601955.625,0,913899,529072,3404992,522791.0938,1187540.25,1527917,1182575,926261.1875,1725336.25,1152639,6915293,739869.6875,4904803,1172785,1230757,1063269.125,3467070,971990.3125,576174.125,1929118,918177,18576900,1988784,9230223,1163748.875,1333016.625,47952868,2074152,2660396,1552051.25,2056814,781682.5,1070740,734495,1059424,8063959,2145603.5,2217046,625988.625,2188093,3220635.25,791640,37980712,912509,728876,53466684,763693,1027230.875,948676.5,654664,16373232,3518459.5,4335178,1581036.75,1241865,1140620,7459418,52960796,819003.3125,2161240.25,59124384,2083584,906103,792015,8687690,1441665.375,449955,557193,455798.9063,2196718,4519115,1313366,5459657,861972.875,1012341,6669954,1332591.5,1507270,3893073.5,3132453,789209.8125,561473.875,519573,2715249.75,1140137,1791071.375,12783255,340270,588856.125,1970956,6582088.5,11047668,2925780,1055033,839011.3125,1605522,624180,20828130,10394330,10716391,1026921,659652.625,1987073,1172903.375,609218,2595783.5,3534690.75,2171445,980773.375,14431245,13852986,2668769,859236.625,3276819,1242311.5,696522,19459304,595936.5,165472,2108012,1848878.5,1090032,1201781.125,6584966.5,8631444,1290284,8688676,2599171.75,6843855,2712983.75,5952875,1750099.75,2856934,666355,3050455.25,1881516.125,700899,15240688,1625308.625,2502081,8227025,11208498,5885427.5,908834,1866802.375,4590246.5,1976920,948657,763364.5,1097572,3468606,2827772.75,1959777,1834290,3347567,4253577.5,3085505,0,2254220,1718845,755856.875,8386493,1738376,7369591,1886007,3075425,3724098.25,2979413.5,6881405.5,473874.0938,455153,3300549.25,422133,7606084.5,831577,519806.0938,776427,3158743,793436,9974370,28229320,3574542.25,630040,1243282,27273538,864420.875,953059.875,430473.6875,1269847.75,2471623,1372706,3182301,2556131,1440554.25,849504,8976533,3133849.5,1302636,5549121,659265,13491170,8915523,1045838.5,1921801.25,1231546,1210229,45313836,2338567.75,2322314,2327611,1372834,983687,741999.6875,3590731,3635600,4376927,18074716,2108861.75,55073120,2105912.75,1456039,843520,4673827,620154.375,2248776,21042926,547084.375,936336,1688612.5,3735308.5,5328051.5,4759256,9546165,14856199,6345009.5,601813.125,10077532,19036416,472532,653366,1856230.625,784329.5,42566736,2495819,1083156.5,362823,378714.4063,957632,488181,867184.375,1800331,1692323,3778355,1983509,2337146,1187261,618670.875,2219190,1650279.25,6042399.5,8555790,45163308,1415494,1125724,5965171,7291418,887403,5851209,3162976,1605669,161311.0938,4367202,731092,3548858.25,0,1619177.125,478690.0938,641713,572139,876304,783651.5,1852926.625,11110405,28731502,609921.6875,4857581.5,586589.1875,629036,1586388.75,1675510.875,3371455,1270582.125,20361158,1633499,3704086.5,6858533.5,832985,4202335,1906384.875,606886,743528.125,16016644,4130053,13951571,1743198,2266983,1331122.125,35891484,1386983.375,6062770.5,1904480,2674034,764832.6875,3282191,662012,1406648.125,1024033,1612797,5662234,11521702,16246106,664539,1465535,24017770,610512,3022275,7273155.5,390726,25631512,650205.8125,1350190,1740568.25,924192.6875,1139980.625,2185193,471484.8125,15473117,1922953.25,7577723,563298,1119640.25,915286.1875,960790.8125,863703,1591528.5,1358519,8835104,862945.125,36681188,571384,673907.3125,2222097.5,1681424,3005926,10444297,2356607,1754477.25,901679,2331952.75,535649.875,1870069,13924967,1835804.875,1346952,1152368.25,885865.875,2799874,776248.6875,1615855,4952487,767720,2139713,11788719,15571642,9686734,656980,985236,1834570,8442782,935442,1748629.875,2352982.75,1832808.75,932083.5,1612965,2650112,5471735,1324965,8690179,827618,1079004,19490284,13720714,5571501.5,1457118,833747,17186624,4345698,1371061.25,942089,1100499.75,14361382,6578370,5987079,1011399,6579348,3017709.25,1360400.5,2313978.75,44293.5,5118939,1497496.5,2439061,4660015,9343428,57514116,8422861,13256662,2181833.5,1076733.25,987288.8125,692127.875,671004.625,5143676.5,1073156.375,982153.875,391058,8058913.5,2277316,1377184,454296,690259,4801472,5755308,823032.375,1416470.25,1460307,9274650,1261634.5,3532966.5,13223740,8890235,1870457.75,3361743,3459543,953432.1875,4476997,1494252,7657568,597210,866126.375,648022,3494383,1258978.375,1679127,842593,2771820.5,930611.875,991283,622051,3936421.5,1961982,1160989,672915,3678939,9683979,1381407,777696,318364.8125,2470642,11007235,1517410.5,2845946,497556,549438,2520310,660508,3736960,75073888,1166069.875,705621.6875,1417778,16865978,3652708.75,14856602,97690.39844,4028075.5,14064603,1113488,1387773,1388812,1392153,8355988.5,1528642,17695420,61762380,1305578,1814947,2671115,2303288,20781390,2473302,605971.1875,2115666,4407932,804002,7753080,524144.6875,2466100,3579008.75,2188051.25,788027,2558573,7802205.5,560823.625,8987012,984657,1100575,1459786,1078535.75,33270518,5475367,872555,770175.8125,872114.5,1963481.625,45252356,2802428,3073778,979442.3125,752441,4145920.5,1294615.25,1730886,41696464,1826203,3234787.75,1188444,1501767,1819329,1049088,10105379,16238985,8305255,978463,2549892,16467486,1095744,440658,532670,7945081,444458,1122837,2454812,1247972,1130097,701422,6235947,1128311.125,6424029,2960159.75],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle ElasticNet() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predEN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["alphasEN = np.logspace(0, 7, 200)\n","l1ratioEN = np.linspace(0, 1, 6)\n","param_gridEN = {\n","    'elasticnet__alpha': alphasEN,\n","    'elasticnet__l1_ratio': l1ratioEN\n","}\n","\n","GridEN, \\\n","BestParametresEN, \\\n","ScoresEN, \\\n","SiteEnergyUse_predEN, \\\n","figEN = reg_modelGrid(model=ElasticNet(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=SiteEnergyUse_train,\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predEN',\n","                         score=score,\n","                         param_grid=param_gridEN)\n","\n","print(BestParametresEN)\n","ScoresEN\n","figEN.show()\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"y":[10313306.649282433,10313306.5431364,10313306.423581222,10313306.283292819,10313306.146680614,10313305.979687562,10313305.817740763,10313305.635418719,10313305.444803568,10313305.230251506,10313304.997107716,10313304.732653502,10313304.45643,10313304.167547207,10313303.842462316,10313303.489194745,10313303.087826263,10313302.845171517,10313302.423069134,10313301.947630713,10313301.409625912,10313300.846502196,10313300.234595101,10313299.595192393,10313298.875150273,10313298.119686488,10313297.272157546,10313296.386130635,10313295.225229234,10313294.126884459,10313292.978221955,10313291.685581028,10313290.333408402,10313289.452287976,10313287.856632411,10313286.19032359,10313284.162579352,10313282.188107818,10313279.964525994,10313277.425121535,10313274.78971845,10313271.926028866,10313268.933013134,10313265.563263528,10313261.7533281,10313257.76216551,10313253.425150901,10313248.89084303,10313243.786993947,10313238.449747987,10313234.478782993,10313228.380077885,10313221.51194612,10313213.423486784,10313205.572903197,10313196.732152255,10313187.493440842,10313177.090860616,10313166.218332222,10313153.980058838,10313140.682631623,10313124.416261632,10313108.55971371,10313091.976186486,10313073.318668298,10313053.045645807,10313031.84446973,10313018.05778439,10312994.015537787,10312964.751628647,10312936.162063578,10312904.081630314,10312866.246390399,10312829.585506855,10312788.317205675,10312745.174204737,10312692.802004816,10312641.72073448,10312584.23846499,10312521.821509827,10312456.555469487,10312383.139426706,10312306.38779582,10312212.738790771,10312060.539039578,10311960.417668942,10311844.370817143,10311569.833794245,10311427.99886184,10311282.78876974,10311172.660694772,10310963.93619741,10310757.381870518,10310508.16091517,10310252.739536751,10309954.166054923,10309629.577945175,10309290.901121631,10308917.848652085,10308542.19560958,10308100.931580031,10307665.262297247,10307214.506389666,10306683.672910681,10306140.272807648,10305479.800601209,10304798.376689091,10304063.913113138,10303277.994468985,10302400.971690983,10301467.293672372,10300456.659371044,10299366.63656182,10298166.009077266,10296902.886019917,10295539.51339614,10294054.734773377,10292466.932458352,10290755.286621878,10288975.333110696,10286660.797651038,10284574.39373269,10282341.571813298,10279948.273387656,10277375.276333,10274617.464795755,10271578.081935402,10268108.43337343,10264108.296694076,10259853.666118499,10255313.071188385,10250667.928301055,10245641.05496744,10240334.501098696,10234594.796810772,10228409.911576027,10221685.238643548,10214503.390702557,10206922.410854394,10198946.551937083,10190484.823966375,10181576.073712412,10172357.623822102,10163001.297227088,10153418.18952387,10145404.358674468,10139193.4056913,10132804.712364161,10126172.522570081,10119472.138746122,10112645.65000182,10105882.027091866,10099205.530777456,10092850.495545806,10086929.317652838,10084421.485144742,10085641.466396907,10088291.336071905,10092792.961210463,10099548.802588481,10109072.745498706,10120716.524828285,10134665.524876859,10153125.070700713,10176923.507192275,10206992.569378696,10244389.909931362,10287436.342580438,10316953.578577787,10346803.404904556,10381377.194105161,10419254.727967173,10464333.887506966,10515642.09864012,10573943.801478788,10620714.407407925,10658442.897261336,10702109.294882417,10752460.150255367,10809458.280187255,10871663.425048528,10943232.835028991,11023578.497573808,11113682.080457378,11214618.00921096,11327551.276972093,11453774.69160723,11594710.972228266,11751887.517876035,11927041.40538815,12122047.544659417,12338997.085918732,12580241.371332439,12848364.227096915,13146213.570999296,13477018.817113906,13844388.430124924,14252282.320019176,14705148.294256482,15143836.092895832]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"y":[13782750.141743395,13782750.164144307,13782750.180137139,13782750.171648726,13782750.198450752,13782750.214096222,13782750.2460296,13782750.268168742,13782750.306498343,13782750.33345679,13782750.362423187,13782750.36306728,13782750.393896936,13782750.44885264,13782750.486466594,13782750.526836976,13782750.591610089,13782750.679600107,13782750.768267244,13782750.831399199,13782750.835719487,13782750.903271789,13782750.975799121,13782751.10314187,13782751.191537008,13782751.34393843,13782751.451548103,13782751.634934084,13782751.55326665,13782751.674903464,13782751.892445587,13782752.040908452,13782752.30212652,13782752.626314908,13782752.833586441,13782753.1872768,13782753.067381702,13782753.458250392,13782753.7283479,13782754.153087746,13782754.478233945,13782754.827571578,13782755.433143126,13782755.858768806,13782755.875062382,13782756.329136016,13782756.816508403,13782757.685801921,13782758.280903671,13782759.32507062,13782760.513557928,13782761.80925811,13782762.739441672,13782762.256245835,13782763.808265597,13782764.88051448,13782766.745162446,13782768.05087999,13782770.290136099,13782771.884019343,13782773.599172281,13782773.43125391,13782775.243971802,13782778.444275336,13782780.660565142,13782783.042763663,13782787.206123602,13782792.368592706,13782797.587313196,13782795.959670365,13782801.728726644,13782805.781008907,13782809.541957326,13782817.036502743,13782822.341695879,13782831.328648778,13782828.326369029,13782838.331005838,13782845.27054545,13782852.816793647,13782865.86398457,13782875.090520881,13782890.863083374,13782900.609116107,13782883.308355313,13782901.931354217,13782914.030331401,13782549.503028333,13782537.918572132,13782543.034690797,13782643.639346855,13782750.18353137,13782723.073308978,13782676.435821993,13782634.298834963,13782574.235965136,13782454.22562124,13782385.43146596,13782309.131419959,13782232.51950163,13782142.127809605,13782053.749485377,13781962.636180729,13781855.801908325,13781746.805211458,13781614.607899172,13781479.194007672,13781334.57116118,13781181.153950423,13781010.978218542,13780832.195935996,13780640.955681901,13780436.83959179,13780214.235338807,13779984.9059246,13779740.530796442,13779478.75060932,13779205.027326386,13778916.66802303,13778630.067177672,13778252.613680953,13777942.17385697,13777622.095113922,13777296.070617719,13776961.826898292,13776624.365210172,13776270.274189593,13775905.99577791,13775475.091513393,13775058.313803738,13774659.784090977,13774328.015763558,13774026.553931877,13773792.464524075,13773613.433047364,13773514.71783844,13773507.307787966,13773637.676048834,13773962.158472575,13774437.391594168,13775048.919196941,13775782.521380465,13776672.802553881,13777811.77105795,13779155.055192232,13780993.59534969,13783363.30985671,13786027.497285992,13789003.222698161,13792357.985877875,13796117.898783794,13800362.563131046,13805141.688956968,13810571.981981639,13816694.473779196,13824163.220695267,13833077.954419784,13843053.532332413,13854286.726266436,13866984.75511814,13881355.671616932,13894685.275633257,13906136.354353257,13919734.52354005,13935983.458495297,13955603.66333436,13979536.015075432,14008358.095072556,14036843.082924042,14068400.255359558,14105270.77235173,14145987.366340686,14194116.875998596,14241294.572259592,14293979.629414203,14341567.778213574,14382901.246918231,14431831.666986784,14489452.332205988,14556856.814026851,14635744.260712767,14728255.345510866,14837225.636582218,14965659.649637643,15117046.939611126,15295302.789909344,15504833.485881329,15750560.613330789,16037679.79696815,16371856.296423152,16758929.11264513,17204964.140922394,17716192.672742452,18298936.328736704,18959557.473862752,19704675.801527284,20541192.340460517,21476279.24522423,22517598.980522167,23500489.584045537]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"y":[6843863.156821471,6843862.922128491,6843862.667025305,6843862.394936911,6843862.094910476,6843861.745278902,6843861.389451924,6843861.0026686955,6843860.583108792,6843860.127046222,6843859.631792245,6843859.102239723,6843858.518963063,6843857.886241775,6843857.198458038,6843856.451552514,6843855.584042438,6843855.010742927,6843854.077871025,6843853.063862228,6843851.9835323375,6843850.789732603,6843849.493391082,6843848.087242916,6843846.558763537,6843844.895434545,6843843.092766989,6843841.137327185,6843838.897191817,6843836.578865454,6843834.063998324,6843831.330253605,6843828.364690284,6843826.278261043,6843822.879678382,6843819.1933703795,6843815.257777001,6843810.9179652445,6843806.200704088,6843800.697155323,6843795.101202954,6843789.024486154,6843782.432883142,6843775.267758251,6843767.631593818,6843759.1951950025,6843750.033793399,6843740.095884139,6843729.293084223,6843717.574425355,6843708.444008058,6843694.95089766,6843680.284450566,6843664.590727733,6843647.337540798,6843628.583790031,6843608.241719239,6843586.130841242,6843562.146528345,6843536.076098332,6843507.766090965,6843475.401269354,6843441.875455617,6843405.508097636,6843365.976771456,6843323.04852795,6843276.482815857,6843243.746976074,6843190.443762379,6843133.543586928,6843070.595400512,6843002.38225172,6842922.950823471,6842842.134510966,6842754.292715471,6842659.019760696,6842557.277640604,6842445.110463124,6842323.206384532,6842190.826226006,6842047.246954404,6841891.188332532,6841721.912508269,6841524.868465434,6841237.769723843,6841018.903983667,6840774.711302886,6840590.164560158,6840318.079151549,6840022.542848684,6839701.68204269,6839177.68886345,6838791.690432056,6838339.886008346,6837871.180238539,6837334.096144709,6836804.930269109,6836196.370777303,6835526.565884212,6834851.87171753,6834059.735350458,6833276.775109117,6832466.376598604,6831511.543913037,6830533.7404038375,6829344.993303245,6828117.55937051,6826793.255065095,6825374.834987547,6823790.965163423,6822102.391408747,6820272.363060187,6818296.433531851,6816117.782815726,6813820.866115236,6811338.495995837,6808630.718937434,6805728.837590319,6802593.9052207265,6799320.599043719,6795068.981621122,6791206.613608408,6787061.048512675,6782600.476157593,6777788.72576771,6772610.56438134,6766885.889681211,6760310.870968951,6752741.501874758,6744649.01843326,6735966.358285793,6727007.8408385515,6717255.556003004,6706876.537673317,6695576.160574181,6683305.105313613,6669863.169499129,6655369.10535628,6639882.663236213,6623455.712279998,6605920.728735808,6587369.626044359,6568042.445090323,6548190.823396226,6527681.323855508,6509815.121999247,6495023.501525891,6479581.927442331,6463341.822442001,6446586.291614369,6429173.401219847,6411401.491052687,6393269.372597944,6375129.009109974,6357164.161526481,6344679.749594217,6338204.97837403,6333529.139811397,6331299.19615449,6332112.850058825,6336789.819380479,6346747.774023313,6363194.695400462,6386515.617861374,6417863.5558892535,6458381.475423032,6509243.804787291,6566514.590088319,6597064.074231531,6625206.554449553,6657483.615858591,6692522.08959366,6734550.899015335,6789989.625020647,6853907.973543374,6899861.036602275,6933984.547604442,6972386.9227780495,7015467.968304746,7062059.746347658,7107582.589384289,7158210.324547116,7209931.358565398,7261704.511277112,7312189.0788107915,7359799.764034841,7402715.89733313,7438861.331125743,7466095.238783919,7482226.514353149,7485165.976673705,7473030.030915069,7444290.069922426,7397792.125457126,7332869.66813584,7249361.83270053,7147584.519789332,7028285.394814121,6892697.607990799,6787182.601746125]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[7568255.563095717,7568255.465895162,7568255.360874381,7568255.24740728,7568255.124817892,7568254.9267577445,7568254.77786806,7568254.616974053,7568254.443112289,7568254.255242395,7568254.052240986,7568253.83289507,7568253.595894953,7568253.339826614,7568253.063163347,7568252.764256935,7568252.281442786,7568251.918417506,7568251.52612253,7568251.102208642,7568250.644139137,7568250.149174871,7568249.614358226,7568249.036495819,7568248.412139813,7568247.737567762,7568247.008760898,7568246.221380724,7568244.946681252,7568243.990193013,7568242.956611769,7568241.839752695,7568240.632937466,7568239.328955232,7568237.9200202925,7568236.397726722,7568234.752999238,7568232.976040229,7568231.056272818,7568227.948981337,7568225.616963006,7568223.096991189,7568220.3739890745,7568217.431677315,7568214.252478815,7568210.817415915,7568207.105999766,7568203.096110744,7568198.763869824,7568194.083499618,7568186.509225483,7568180.823983165,7568174.6805924,7568168.042315671,7568160.869487748,7568153.119284368,7568144.745472661,7568135.698142298,7568125.923415653,7568115.36313539,7568103.954527944,7568085.493191591,7568071.635843706,7568056.662228118,7568040.4829162005,7568023.001370868,7568004.113383262,7567983.706471828,7567961.659233694,7567937.840646322,7567912.109315706,7567884.31266737,7567839.324799491,7567805.564815703,7567769.08803282,7567729.677304415,7567687.098309946,7567641.098227956,7567591.404300767,7567537.722269848,7567479.73475648,7567417.099476074,7567349.447335614,7567239.873146202,7567157.721985277,7567068.976215451,7566973.101690777,7566869.53197142,7566757.685711399,7566636.889820996,7566506.437039073,7565902.79154188,7565716.837022346,7565478.077336851,7565250.377940071,7565008.463343529,7564757.632451756,7564462.014348638,7564119.697107412,7563779.923919068,7563399.598452177,7563096.102051639,7562766.388611577,7562414.182696779,7562097.236069542,7561651.956340984,7561245.50038893,7560790.7548315935,7560293.251256541,7559733.755663492,7559157.516082823,7558510.03222976,7557840.292503269,7557094.953063121,7556270.5747824935,7555408.495824728,7554460.850691244,7553473.344466162,7552405.882885076,7551249.780528589,7550038.535228676,7548713.030620699,7547300.069309512,7545795.872936767,7544152.056530708,7542399.297054485,7540043.976803378,7536314.7828607615,7530900.427028925,7525066.999168927,7518779.552475389,7511993.238511013,7504920.528340403,7497507.521251301,7488767.100387456,7479285.812618019,7469083.928350855,7458103.161314543,7446265.502295678,7433708.924147468,7419932.892288,7405095.815893684,7389342.145202646,7372142.255565212,7353657.981457465,7334018.741349432,7312697.904931913,7290073.665791427,7265594.39769724,7239654.898308254,7211713.802791971,7182161.628115774,7150504.635229593,7117120.8120776005,7081787.393897455,7058272.812826419,7048920.657693401,7038881.464078373,7028178.666619446,7016727.247313027,7004550.615651096,6991581.721973531,6977838.611283475,6963297.593849968,6947929.715352311,6931784.962429385,6914871.296767176,6890685.377008176,6845459.899423005,6799507.302626506,6752893.098038068,6706499.281575494,6661191.239721581,6627873.247805189,6595877.952370781,6562762.119619571,6528721.516725831,6494044.58194957,6459110.576848337,6424327.776668783,6392742.642294089,6367328.851338824,6340731.926024051,6313057.685904728,6284389.571002135,6254929.845506357,6224944.05825142,6194722.0322127445,6164727.947123229,6135493.26352075,6107708.134188623,6082269.380978069,6060253.696429858,6043011.503016382,6032162.013022952,6029677.324281,6037953.791301836,6059684.802940105,6098126.180293149,6157063.131441341],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[7619812.891110589,7619812.434214012,7619811.937537335,7619811.397613569,7619810.810673252,7619810.172617998,7619809.478991859,7619808.724949993,7619807.905224799,7619807.014088895,7619806.045315025,7619804.992132298,7619803.847178795,7619802.602449796,7619801.249241713,7619799.7780909855,7619798.178707611,7619797.319608638,7619795.508326221,7619793.539342175,7619791.398912148,7619789.072092767,7619786.54263687,7619783.792879676,7619780.803614947,7619777.553960384,7619774.021211208,7619770.180680953,7619766.005528336,7619761.466569048,7619756.532070945,7619751.1675315155,7619745.335435721,7619742.196062595,7619735.590776666,7619728.410449413,7619720.604970207,7619712.11985762,7619702.89587788,7619692.868629931,7619681.968094372,7619670.1181429,7619657.236004937,7619643.2316876985,7619628.007345503,7619611.456594146,7619593.463765192,7619573.903095365,7619552.63784488,7619529.51933906,7619517.080656732,7619490.898050229,7619462.436294782,7619431.496827306,7619397.863774282,7619361.3024422545,7619321.5576768,7619278.352078444,7619231.384063232,7619180.325754336,7619124.820689978,7619064.481332018,7618998.886357518,7618927.577714882,7618850.057423755,7618765.784097034,7618674.169160449,7618624.904862928,7618521.155485437,7618408.379208345,7618285.790379976,7618152.534999641,7618007.684783342,7617850.230717086,7617679.076054,7617493.0287079,7617290.792992121,7617070.960648109,7616832.001103889,7616572.250897595,7616289.902195982,7615982.990332443,7615649.380282918,7615286.751991862,7614640.339348392,7614208.360301306,7613721.099190761,7613208.7231499795,7612662.454048117,7612071.543157174,7611423.157519573,7610700.956826192,7609958.770665071,7609112.215945198,7608223.065330901,7607189.902527723,7606177.600085902,7604983.526374785,7603681.865425989,7602377.554229389,7600812.672467034,7599214.631009339,7597586.919743119,7595605.47800716,7593550.297616598,7591066.003502275,7588460.681752927,7585671.806004723,7582694.931195436,7579358.104526634,7575785.360227427,7571943.316516964,7567794.7846190175,7563201.651791438,7558366.687099258,7553107.206626531,7547364.960788662,7541184.338730427,7534500.575424251,7527589.823752716,7518078.820887049,7509846.034189062,7500991.24145304,7491432.357325299,7481118.478866772,7469979.473947941,7458002.382308776,7445125.298144137,7431226.62229973,7416363.77266584,7400349.01622225,7383983.428262793,7365621.589385677,7345836.875324546,7324798.460495418,7302257.623217618,7278361.670283688,7252956.784118931,7226003.711098418,7197552.8075598255,7167459.353090686,7135939.974534236,7102933.511840409,7068630.391881886,7034079.022981781,7008026.595359412,6992113.134808509,6975994.70793167,6959821.311037865,6944002.137258217,6928699.669931409,6914369.432523183,6901513.102263118,6890744.50895239,6882814.957383387,6878637.953586965,6879316.676597291,6886174.22858545,6900786.046603724,6925013.536466871,6961037.425615346,7011388.725508671,7078974.664844363,7167096.523994531,7279456.077204501,7420147.842206877,7593635.509082773,7797400.839122416,7948269.692257728,8094243.788169918,8262868.420302203,8457135.950934105,8680390.082408875,8935786.044802815,9227629.973108243,9466296.067210067,9669126.352882992,9896186.519058798,10150208.783166155,10434167.818182133,10751289.842281613,11105061.101794608,11499236.155828506,11937846.529955616,12425210.439032959,12965944.367573794,13564977.327441989,14227568.582472945,14959329.542893473,15766250.402776742,16654731.940911846,17631622.75122939,18704262.033311382,19880527.971539948,21168891.671465244,22578476.605617,24119123.544331655,25801461.002679776,27636981.313135985,29318013.089378543],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[7378350.709454269,7378350.641678937,7378350.568198209,7378350.488531789,7378350.402158897,7378350.308514935,7378350.206987771,7378350.096913736,7378349.977573266,7378349.848186229,7378349.707906816,7378349.551153494,7378349.385882012,7378349.20669814,7378349.012430674,7378348.8018098045,7378348.573458825,7378348.325885118,7378348.057470408,7378347.766460182,7378347.450952199,7378347.108884097,7378346.738019875,7378346.335935287,7378345.900001988,7378345.412891292,7378344.899295484,7378344.342466658,7378343.738765472,7378343.08424618,7378342.374630913,7378341.605281696,7378340.7711701365,7378339.866844564,7378338.886394421,7378337.823411621,7378336.670948706,7378335.421473404,7378334.06681951,7378332.553199188,7378330.957232801,7378329.226932658,7378327.35099226,7378325.317153679,7378323.112127484,7378320.721505979,7378318.129669172,7378315.31968284,7378312.273187998,7378308.970281113,7378305.389384285,7378301.507104492,7378297.298081029,7378292.5954713095,7378287.636862926,7378282.260983557,7378276.432730054,7378270.1140483,7378263.263685487,7378255.836921728,7378247.785279146,7378239.056206695,7378229.592738878,7378219.333125537,7378208.210431091,7378196.152100305,7378183.079487871,7378168.907348972,7378153.543287351,7378136.887157745,7378118.286048735,7378098.667121908,7378077.398932724,7378054.343032681,7378029.349386344,7378002.255407866,7377972.884918434,7377941.047018492,7377906.534867834,7377869.124366424,7377828.57272814,7377784.616939129,7377736.972091865,7377685.329585434,7377629.355181909,7377568.68690796,7377502.932790344,7377431.6684129825,7377354.434282898,7377270.732991296,7377180.026155611,7377078.633462038,7376970.808321037,7376851.614233295,7376725.411244869,7376577.477326834,7376412.038634653,7376233.931596138,7376041.303530218,7375838.406410374,7375611.113757357,7375369.721548516,7375111.990054162,7374830.124104191,7374527.727365012,7374197.348061613,7373846.998607677,7373464.315034232,7373056.296632365,7372615.029467627,7372142.196081204,7371624.085147726,7371038.034483162,7370416.372517829,7369796.217154758,7369133.531735331,7368424.139120267,7367667.159598561,7366860.329119818,7366002.102464831,7365091.087132016,7364126.175122241,7363106.946015908,7362033.280367437,7360906.927010697,7359728.881282314,7358503.61995562,7357235.830128184,7355932.065857083,7354603.736944241,7353292.737655493,7352114.198028781,7351083.806274188,7350151.606320678,7349311.229452611,7348111.233854293,7345838.1130859535,7343038.379026001,7340001.7238850035,7336703.728284911,7333335.068476254,7329792.113476775,7326682.203098855,7325054.881305179,7323311.290694833,7321445.342894158,7319450.877763441,7317322.162623285,7315054.695595872,7312642.016840827,7310081.512619181,7307369.696926191,7304505.879446517,7301490.810305523,7298327.709186864,7295012.853479247,7289441.197602489,7283544.540734858,7277564.547048118,7271436.426856719,7265345.4559334265,7259309.99823342,7253403.336431253,7247796.918101107,7242727.672673774,7238285.844678371,7234834.016700192,7232600.277910012,7231993.957255954,7233478.885176941,7231037.4907216905,7219327.432033224,7207756.378191912,7196531.445203053,7185941.686942078,7176310.375742191,7168072.442935595,7161705.642740758,7157876.87662645,7153472.360402754,7134597.440667104,7115090.174671456,7095075.568890514,7074721.825284584,7054251.0402607815,7033952.530810963,7014214.176042114,6995458.253801613,6978289.305350073,6963420.592770405,6951768.558496138,6944342.547644809,6942524.541010563,6947778.8959990945,6962020.4007314835,6987496.589844323,7026666.622998695,7082554.701666766,7158628.94765641,7258578.16410593],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[13571818.767630154,13571818.731349707,13571818.670092223,13571818.602717623,13571818.556035977,13571818.4772444,13571818.42266441,13571818.330521436,13571818.266707009,13571818.1589505,13571818.040433416,13571817.958324192,13571817.8197242,13571817.723722827,13571817.561637338,13571817.383366812,13571817.259843953,13571817.05136549,13571816.906943794,13571816.663139515,13571816.494283963,13571816.209169118,13571815.895584414,13571815.678323144,13571815.311604043,13571815.05758645,13571814.628731702,13571814.33174119,13571813.830225216,13571813.27863266,13571812.896509213,13571812.251461137,13571811.804696009,13571811.050363174,13571810.220717968,13571809.645899152,13571808.67569913,13571808.003649566,13571806.869092135,13571806.083373679,13571804.756628422,13571803.297439178,13571802.2865546,13571800.580205318,13571799.39836918,13571797.403021924,13571795.208531072,13571793.68809068,13571791.121973801,13571789.344473556,13571786.343854213,13571784.265880482,13571780.75726366,13571776.898612194,13571774.225572899,13571769.713817481,13571766.589120861,13571761.313885696,13571757.661353124,13571751.493668135,13571744.711111255,13571740.013510574,13571732.084004156,13571726.593496542,13571717.323589435,13571707.130380139,13571700.070541713,13571688.155439643,13571679.905407205,13571665.978660084,13571656.338687314,13571640.062193653,13571622.167698495,13571609.77873334,13571588.868709303,13571574.396962207,13571549.966674479,13571533.064759603,13571504.526257308,13571473.160714164,13571451.45873399,13571414.830215389,13571389.497868055,13571346.733878184,13571299.751358313,13571267.259724643,13571212.433531852,13571174.53828041,13571110.582946124,13571066.404694308,13571254.243312368,13571467.037307322,13571323.041073136,13571087.314452339,13570829.26252711,13570501.897398425,13570159.266403018,13570022.206405438,13569874.441710485,13569714.772649804,13569542.049651941,13569356.13181422,13569154.33860217,13568938.72667034,13568704.30469283,13568453.670413999,13568182.769534668,13567891.645700132,13567578.486494359,13567242.357331593,13566882.595316473,13566497.167867089,13566082.50050822,13565639.151764924,13565168.515027583,13564664.253461054,13564127.681184914,13563558.020327432,13562954.508093318,13562324.763306526,13561674.097881336,13560992.94707567,13560277.062626608,13559536.334601736,13558765.862785585,13557971.605311714,13557164.343272299,13556343.46501597,13555524.313857308,13554717.315646784,13553935.500448074,13553203.044315156,13552538.708317157,13551969.4825707,13551537.322645044,13551279.360919705,13551250.6571678,13551510.374212619,13552155.051380139,13553018.810255947,13554075.858768199,13555221.254889565,13556420.562269133,13557789.20736141,13559234.773291294,13560826.958166195,13562582.52040121,13564521.491901431,13566668.072586395,13569042.730250472,13571672.931276198,13574589.86638258,13577829.071550379,13581482.862075755,13585485.45184124,13589949.710239282,13594945.053701185,13600532.242906306,13606790.91960561,13613859.72659339,13621738.071250249,13630617.365865363,13640625.75428897,13651967.826561438,13664735.409549233,13679200.71782425,13695644.821461719,13714272.188109888,13735504.658175068,13759626.17157349,13786722.594286881,13810798.904164264,13837461.192652404,13866924.571989166,13899502.025704244,13926776.526335778,13943107.416789452,13961080.233872095,13980853.515009692,14002629.477500709,14026633.997765537,14053121.659223152,14082379.288424179,14114730.888482587,14150579.02999384,14190266.963099321,14234297.37533445,14283247.551355649,14337627.181057649,14398218.760028865,14465683.330397207,14541017.022169312,14625077.870122077,14719134.455298457,14824388.526674893,14942184.541938255,15074311.810352674,15222522.673040837,15388873.38762699,15575591.941293105],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,1.0843659686896103,1.1758495540521567,1.2750512407130132,1.3826221737646558,1.4992684327860455,1.6257556664437942,1.7629141180959478,1.9116440753857022,2.0729217795953714,2.247805833548725,2.4374441501222206,2.643081486974105,2.866067616948251,3.107866187782013,3.370064329271928,3.6543830709572562,3.9626886387014784,4.297004704320841,4.659525668664681,5.05263106533568,5.478901179593942,5.941133984965034,6.442363508721372,6.985879746785248,7.575250258771915,8.214343584919426,8.907354638610439,9.658832241158704,10.473708979594498,11.357333583431052,12.315506032928255,13.354515629298987,14.481182276745338,15.702901247293775,17.027691722258997,18.464249428955434,20.022003718155844,21.711179456945043,23.54286414322418,25.529080682395165,27.68286630392065,30.01835813575589,32.55088599835058,35.2970730273065,38.27494478516313,41.504047578504746,45.005576757004974,48.80251583654431,52.91978735958442,57.384416483023955,62.22570836730228,67.47544053110693,73.16807143427197,79.34096665797492,86.03464416684504,93.29304026284686,101.1637979766207,109.69857978923841,118.95340673703195,128.9890261253308,139.87131026472386,151.67168884709224,164.46761779946644,178.34308769319094,193.38917504552302,209.70464013232328,227.39657523579274,246.5811075822604,267.3841615839947,289.94228538828753,314.40354715915,340.9285069746811,369.6912707195028,400.8806328898465,434.7013158125022,471.3753134116724,511.14334834401654,554.2664520663108,601.0276782070382,651.733960488242,706.7181273927491,766.3410868007454,830.9941949353396,901.1018251665018,977.1241535346502,1059.5601792776158,1148.9510001873086,1245.883364295008,1350.9935211980264,1464.9713983072863,1588.5651294280526,1722.5859653987857,1867.9135990207828,2025.5019392306665,2196.385372416547,2381.6855519761584,2582.6187606826747,2800.503894183631,3036.771118035457,3292.9712550971512,3570.7859649004627,3872.038781812553,4198.70708444391,4552.935074866948,4937.047852839004,5353.566677410724,5805.225516094895,6294.988990221887,6826.071834272386,7401.959996915644,8026.4335222571735,8703.591361485165,9437.878277775382,10234.114021054527,11097.524964120721,12033.778407775904,13049.019780144015,14149.91297434576,15343.684089300132,16638.168860761274,18041.864093920718,19563.98343517065,21214.517849106276,23004.301197729168,24945.081352303165,27049.59730463137,29331.662783900425,31806.25692794119,34489.6226040576,37399.37302478794,40554.60735840828,43976.036093027215,47686.116977144746,51709.202428967554,56071.69938205458,60802.24261649427,65931.88271333542,71494.28986597576,77525.97488629464,84066.52885618317,91158.88299750819,98849.59046625586,107189.13192051287,116232.24686798519,126038.29296797274,136671.63564620074,148202.0705798857,160705.28182616385,174263.33860096507,188965.23396912077,204907.4689815846,222194.6860939524,240940.35602395269,261267.52255633264,283309.6101839324,307211.2998861759,333129.478793467,361234.26997094305,391710.1490809261,424757.1552536894,460592.2041145104,499450.511585514,541587.1378079476,587278.6613189477,636824.9944718586,690551.352016233,748810.3857590015,811984.4993184009,880488.3581643464,954771.6114208066,1035321.8432956616,1122667.7735108135,1217382.727739662,1320088.4008314167,1431458.9375234786,1552225.357427048,1683180.353330955,1825183.4943190424,1979166.8678535575,2146141.1978584058,2327202.4789604074,2523539.170434766,2736439.997074672,2967302.4081888665,3217641.7502507353,3489101.2134067737,3783462.617131925,4102658.1058271904,4448782.831127585,4824108.704165374,5231099.308056259,5672426.068491978,6150985.788580504,6669919.663030115,7232633.896483534,7842822.0613376815,8504489.341802686,9221978.823334321,10000000],"xaxis":"x","y":[15428295.315121436,15428295.442544175,15428295.58120396,15428295.680193823,15428295.839717055,15428296.013302736,15428296.202191709,15428296.407734383,15428296.631400475,15428296.874789506,15428297.139642337,15428297.328762455,15428297.633470032,15428297.965038648,15428298.325838504,15428298.718449188,15428299.145678142,15428299.610580834,15428300.11648272,15428300.667003052,15428301.05984211,15428301.693190139,15428302.382376129,15428303.132328037,15428303.94839057,15428304.836426554,15428305.80278844,15428306.854383653,15428307.604945892,15428308.81478139,15428310.131286934,15428311.5638781,15428313.122802682,15428314.819214303,15428316.665252706,15428318.674131045,15428320.108279485,15428322.419518273,15428324.934567632,15428327.67142354,15428330.64967364,15428333.890638405,15428337.4175248,15428341.255593637,15428343.996319521,15428348.412289582,15428353.217789311,15428358.447235527,15428364.138093233,15428370.33114659,15428377.070794247,15428384.405371057,15428392.387498729,15428398.084207442,15428407.268818142,15428417.264233612,15428428.142203838,15428439.976148343,15428452.859143613,15428466.8808146,15428482.141549798,15428493.03706728,15428510.59962429,15428529.714367354,15428550.518981012,15428573.160280686,15428597.78977535,15428624.614798583,15428653.814275252,15428674.672470735,15428708.285886157,15428744.831168998,15428784.655737944,15428828.010235468,15428875.203845909,15428926.512641294,15428963.2671291,15429022.433018245,15429086.725795146,15429156.849301098,15429233.108932845,15429316.160170503,15429406.641400648,15429505.005352167,15429575.527323999,15429688.805195354,15429812.286881981,15429164.707156435,15429254.837320667,15429368.373184932,15429499.439447235,15429670.261849612,15429817.452270996,15430011.58260817,15430235.580640797,15430493.089678101,15430641.352150545,15430752.826883148,15430871.93548632,15431000.320839273,15431139.223571641,15431289.725062514,15431452.894937295,15431629.853074942,15431821.798294263,15432030.024687175,15432255.933161259,15432501.043995006,15432767.006766222,15433055.611465564,15433368.80065392,15433708.695093682,15434077.570695434,15434477.91624901,15434912.43603549,15435384.079333058,15435896.042081794,15436451.79916918,15437055.137586925,15437710.195500815,15438421.447126105,15439193.781655772,15440032.539661428,15440943.521707045,15441933.056471242,15443008.066382317,15444176.087336943,15445522.7907181,15446958.054427335,15448516.5061667,15450208.54914071,15452045.732387535,15454040.64251978,15456207.020026248,15458559.871073332,15461115.527270492,15463891.824329443,15466908.25484069,15470186.065612737,15473748.489437256,15477620.947208732,15481831.209767802,15486409.696699468,15491389.750021761,15496807.879193973,15502704.155603142,15509122.590551428,15516111.533572987,15523724.135933034,15532018.911072839,15541060.333390336,15550919.511511607,15561674.965397673,15573413.484317763,15586231.075955242,15600234.0955918,15615583.74639017,15632324.204054542,15650644.626175413,15670707.075712398,15692692.159043409,15710684.812560437,15722485.25753623,15735466.490996515,15749768.661181552,15765543.479754595,15782963.905644951,15802223.030751698,15823539.685777182,15847160.876975918,15873364.367176965,15902512.071128774,15934870.54456005,15951095.183400378,15960767.369268592,15971426.948132012,15983186.756972814,15997529.49679086,16014250.9996262,16032693.968181897,16053053.202234298,16075562.388116913,16100469.548701793,16128053.472659377,16158659.96576508,16192662.677870026,16230440.520966182,16272558.44129838,16319463.61295575,16371824.007843986,16430345.759303272,16495733.727572074,16569088.71578831,16651368.309630692,16743605.243101902,16847259.02388896,16963886.38163976,17095188.419768404,17243131.64256988,17409934.13826025],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle EN pour le paramètre l1=1.0 en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"alpha"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre l1 ratio\n","for i in BestParametresEN['ElasticNet()'][BestParametresEN['paramètre'] ==\n","                                          'elasticnet__l1_ratio']:\n","    fig1 = go.Figure([\n","        go.Scatter(name='RMSE moyenne',\n","                   x=alphasEN,\n","                   y=GridEN.ScoresMean.where(\n","                       GridEN.elasticnet__l1_ratio == i).dropna(),\n","                   mode='lines',\n","                   marker=dict(color='red', size=2),\n","                   showlegend=True),\n","        go.Scatter(\n","            name='SDup RMSE',\n","            x=alphasEN,\n","            y=GridEN.ScoresMean.where(\n","                GridEN.elasticnet__l1_ratio == i).dropna() +\n","            GridEN.ScoresSD.where(GridEN.elasticnet__l1_ratio == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            showlegend=False),\n","        go.Scatter(\n","            name='SDdown RMSE',\n","            x=alphasEN,\n","            y=GridEN.ScoresMean.where(\n","                GridEN.elasticnet__l1_ratio == i).dropna() -\n","            GridEN.ScoresSD.where(GridEN.elasticnet__l1_ratio == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            fillcolor='rgba(68, 68, 68, .3)',\n","            fill='tonexty',\n","            showlegend=False)\n","    ])\n","\n","    fig2 = px.line(GridEN.where(GridEN.elasticnet__l1_ratio == i).dropna(),\n","                   x=alphasEN,\n","                   y=[\n","                       'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                       'ScoresSplit3', 'ScoresSplit4'\n","                   ])\n","\n","    fig3 = go.Figure(data=fig1.data + fig2.data)\n","    fig3.update_xaxes(type='log', title='alpha')\n","    fig3.update_yaxes(title='RMSE')\n","    fig3.update_layout(\n","        title=\n","        \"RMSE du modèle EN pour le paramètre l1={:.2} en fonction de l'hyperparamètre alpha\"\n","        .format(i))\n","    fig3.show()\n","    if write_data is True:\n","        fig3.write_image('./Figures/graphRMSEEN{:.2}.pdf'.format(i))\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          paramètre  KNeighborsRegressor(n_jobs=-1)\n","0  kneighborsregressor__n_neighbors                              18\n"]},{"name":"stderr","output_type":"stream","text":["/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning:\n","\n","\n","5 fits failed out of a total of 250.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","5 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/pipeline.py\", line 394, in fit\n","    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/neighbors/_regression.py\", line 213, in fit\n","    return self._fit(X, y)\n","  File \"/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/neighbors/_base.py\", line 566, in _fit\n","    raise ValueError(\"Expected n_neighbors > 0. Got %d\" % self.n_neighbors)\n","ValueError: Expected n_neighbors > 0. Got 0\n","\n","\n","/home/lancelot/Documents/FormationDataScientist/P4/.env/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning:\n","\n","One or more of the test scores are non-finite: [               nan -14604618.26841321 -14112352.57947439\n"," -14016357.45170158 -14129203.51789925 -14038495.5703273\n"," -13945944.10280748 -13929957.66759034 -13916602.70614032\n"," -13757234.53718702 -13910239.90120853 -13915428.79210892\n"," -13985429.56359803 -14064730.97643084 -14131629.53225077\n"," -14174790.34053594 -14190766.3547747  -14201296.1657718\n"," -14221211.77953116 -14203249.09689116 -14250248.33933494\n"," -14275813.52074705 -14306595.52971122 -14341483.350632\n"," -14386407.77541222 -14447815.55219859 -14463848.35939736\n"," -14499187.86362449 -14517035.60115878 -14539570.40098339\n"," -14560494.69783062 -14540310.09541588 -14569618.13415278\n"," -14598848.93317671 -14615410.55882208 -14644652.68364638\n"," -14681308.17152314 -14709360.83718143 -14736130.67572005\n"," -14764648.9066251  -14788901.205982   -14813551.75885176\n"," -14843267.44478049 -14870205.17579794 -14890654.54858837\n"," -14919539.8437549  -14940720.95916583 -14966411.35308953\n"," -14984087.43010396 -14998612.71244393]\n","\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predkNN=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[2048818.3333333333,2586660.9444444445,883167.0902777778,989944.7222222222,2525538.184027778,811076.7855933333,572152.5555555555,3583823.277777778,49532375.777777776,1604610.6666666667,3461573.9305555555,12506979.222222222,2654171.107638889,1596018.180588889,1887991.2777777778,746984.8333333334,6881779.770833333,2262992.222222222,5278627.611111111,810381.5,3189347.5,2061044.111111111,3681867.8333333335,1246425.4340277778,1275400.0338611112,44203696.88888889,6617735.666666667,887258.7222222222,1190801.7777777778,11140628.555555556,2671219.722222222,1469051.2083333333,553677.9079944444,1846248.2508666667,1531790.6493055555,4060359,3260945.1831611115,952289.7986166668,3589410.277777778,1378384.8333333333,661652.0555555555,1264586.321188889,1219068.2534722222,1134599.111111111,6160546.597222222,2358542.6666666665,6339313.111111111,2205550.388888889,2870885.6354444446,971116.6805555555,2877198.215277778,4342290.527777778,15722394.666666666,1831291.691,2888113.3541666665,1450996.5555555555,20046542.833333332,2391052.277777778,9542570.611111112,20783537.055555556,1456914.0625555557,2339172.230468889,814270.2777777778,20733580.722222224,1699607.1562777779,1155986.388888889,4341420.625866666,1993313.388888889,842847.7222222222,711467.3888888889,1238969.6215277778,1166075.8333333333,15415046.833333334,2287664.6875,1924754.4739611112,1255402.9479222223,5128812.388888889,9733192.611111112,7789478.111111111,932906.3055611112,614672.9444444445,3333295.0707466668,1830815.5381944445,4189091.8333333335,1116295.2881944445,1823842.111111111,3586410.090277778,1016366.3888888889,1049187.5555555555,1488045.4444722224,1140642.0555555555,773625.0173666667,2261718.611111111,15429676.416666666,1242189.9444444445,1472072.5625277779,964288.9444444445,1513686.4132,2882416.8333333335,6679685.319444444,1499511.7222222222,2598189.5555555555,4867558.243055556,3001368.121527778,791607.2222222222,852048.0763888889,9210872.979166666,3884211.017361111,4360636.722222222,4058149.277777778,2007618.5,2780992.611111111,2504875.611111111,1508976.5,1980945,2859275.236111111,1708536.7673888889,836699.5625,5936411.166666667,7130846.833333333,2948925.625,2287310.28125,2548883.0555555555,3126627.9479166665,814944.2777777778,3189347.5,759011.6250055556,1516730.1093833335,1134089.2777777778,12273650.527777778,844202.0555555555,967461.1666666666,981013.1111111111,3082494.9791666665,3361311.222222222,2136801.4930555555,1465400.870661111,796982.9444444445,559887.3281333332,2751609.277777778,2430737.451388889,850039.7222222222,3916725.774333333,1276332.107638889,2108118.9444444445,889756.0555555555,22500769.611111112,809321.8073,1812232.0555555555,7255447.833333333,851505.48785,915140.7222222222,1387252.5798888889,37409726,1078436.2083333333,2812180,1379220.9930555555,4872815.59375,17207172.194444444,1858033.2222222222,16381822.305555556,3949500.309055555,2522778.8333333335,1905549.2777777778,1747835.201388889,27188939.722222224,3379313.5,3965004.298611111,8496740.541666666,2212777.8541666665,41990333.55555555,1880478,888088.8888888889,6870699.666666667,1010242.6493055555,736570.8888888889,4640961.111111111,5788431.555555556,7753533.666666667,615006.6666666666,14187559.638888888,3029133.111111111,20541038.861111112,2100566.111111111,1302837.9574666666,1134089.2777777778,1124153.6267666668,4391425.423611111,4760760.774305556,1472958.5486388889,1351917.8402777778,2345065.5,9624738.722222222,1157510.388888889,2827979.777777778,1998186.9409722222,2859037.5555555555,4182230.722222222,20298376.611111112,1299779.9444444445,516827.94444444444,2890593.6458333335,1627496,1134089.2777777778,4078422.9244777774,1946517,1738082.388888889,810483.5781333334,8714683.805555556,15547725.666666666,1797578.9444444445,1126157.1406333332,2634876.888888889,3380306,2509617.7408855557,3694604.6744777774,4555801.833333333,7510504.944444444,1846248.2508666667,1210153.6666666667,3959186.111111111,1554288.5555555555,9910932.277777778,1480236.1354166667,5169833.670138889,23065500.611111112,12834806.027777778,3985312.111111111,2041161.5,983875.9340555556,964697.6666666666,1651093.8333333333,5350099.958333333,3018578.9166666665,821712.1666666666,570744.9600777779,3034127.923611111,637168.6111111111,12904872.888888888,820598.9444444445,982899.8888888889,4713306.340277778,831607.3888888889,1245357.5555555555,1940056.5208333333,3237306.5581611115,696968.5,1616045.8333333333,1628422.6666666667,1886012.2222222222,11006420.111111112,17869440.083333332,4832835.5,2156184.222222222,2262661.6666666665,756667.2777777778,6151630.166666667,1339988.4809333335,1891811,1196400.6666666667,2488011.722222222,1628068.3090277778,2464498.934027778,1877997.7222222222,2551856.423611111,2653266.4592,764326.4131944445,1955834.4444444445,2210253.982638889,2018533.4444444445,1476683.888888889,863023,1071810.046877778,2348728.611111111,11781099.819444444,1008626.8888888889,3344049.6666666665,1539793.2257,1178103.875,4879270.055555556,889756.0555555555,4265748.555555556,968399.2222222222,2735244.388888889,5426196.986111111,14495477.666666666,1125041.8333333333,4695563.222222222,2190217.9444444445,1164716.611111111,4346003.5625,1364185.6198000002,1588581.708361111,2249670.4444444445,1781012.888888889,1488860.7777777778,796673.559896111,3034127.923611111,4050775.3333333335,1161831.1276055556,1021534.5086833334,3642525.388888889,1087800.388888889,1160536.454861111,1127277.0885722223,840990.0355888889,3326410.0208333335,2839032.847222222,1738082.388888889,2329383.277777778,25286593.055555556,5181185.222222222,885284.7864611112,830105.9444444445,8147257.277777778,3676914.909722222,5628299.388888889,883925.4444444445,50399542.88888889,2764690.6180555555,863314.6111111111,986800.423638889,2163357.0208333335,11568623,16912611.333333332,1831479.388888889,3034127.923611111,2962653.1666666665,1794544.2552111112,1049405.2222222222,2241905.611111111,2591279.611111111,2596332.9444444445,789995.7222222222,5669061.447916667,2997632.5625277776,4963284.805555556,2863399.6319444445,4735928.044272223,2198561.107638889,1958168.6458333333,2387553.670138889,4876397.597222222,20741031.055555556,2643117.073783333,1134089.2777777778,3071817.1666666665,8695507.611111112,2459625.0555555555,6333081,3304074.611111111,5327603.836805556,3841447.722222222,4207581.298611111,2321357.965277778,1039471,1834689.8333333333,9426090.208333334,5348762.888888889,3848760.0069444445,2194376.6371555557,3364539.402777778,1583602.1666666667,4780275.944444444,5087278.75,3407826.222222222,3449050.722222222,2841360.111111111,1152407.3368055555,2935031.4444444445,1359241.611111111,11339143.402777778,2397024.9166666665,12042657.444444444,1276839.1041666667,998651.6007000001,1251390.6493055555,3690158.7916666665,7465088.083333333,7019364.388888889,5732684.5,1690530.8489666667,2045101.3194444445,1713547.5,799593.9444444445,2614863.142361111,3944419.4444444445,7897846.805555556,2343937.1875,9777547.5,4120475.722222222,14124613.638888888,6480661.951388889,7813995.611111111,693151.6666666666,3676914.909722222,2505260.75,2411772.3333333335,1890916.1788222222,2245529.722222222,2202858.1666666665,1000360.8888888889,6006365.888888889,1390664.388888889,1702365.5295444445,2314513.2812777776,2314513.2812777776,1539170.7777777778,3662650.152777778,1834689.8333333333,2469570.934027778,6498018.743055556,3317171.701388889,1907249.3680555555,2483670.3541666665,924678.7309333333,1236254.2326666666,7104165.368055556,549429.8888888889,3739863.0833333335,5283015.541666667,742760.6666666666,1067330.9526888889,800799.9079944444,931692.8125,1359115.263888889,1428818.0243055555,4597282.236111111,5615478.881944444,2479871.388888889,1023410.4444444445,1351975.486111111,2582020.184027778,9349810.847222222,9400250.722222222,2184104.986111111,4052356.3055555555,4078422.9244777774,713064.5,39406257.05555555,8432727.194444444,61440898.61111111,8829867.791666666,1240119.611111111,3150651.4444444445,12911155.111111112,2242353.277777778,3532874.767361111,8391958.444444444,858062.6666666666,715062.6666666666,17286106.138888888,3903368.777777778,1158758.7604166667,8952088.15625,960416.3263888889,1209450.3451605556,954780.0729166666,1016403.8819444445,1656893.5,4091384.798611111,2202018.8819444445,985516.0555555555,1535016.9409722222,2527740.5555555555,3409777.0278055556,2973259.277777778,1940002.7205166668,2555867.5,2184489.5290800002,677294.0555555555,1313934.8993333334,3218705.8819444445,2080834.9444444445,33355894,1912526.139761111,5067439.729166667,2946934.777777778,4584557.111111111,927800.8208549999,981013.1111111111,15731789.305555556,6150640.777777778,758459.9444444445,3469913.3785,1308518.2778055556,5645070.555555556,4546044.25,650508.6163277779,1263759.6666666667,1648078.2777777778,2630509.388888889,1726773.7274333334,913314.5555555555,1877269.388888889,1894707.9236388889,964865.9713666667,784333.5416722222,4793289.055555556,1149559.5,1562977,1209218.763888889,7449149.055555556,1346295.2777777778,5138706.579861111,74022334.83333333,12901909.888888888,2596332.9444444445,1125527.0555555555,3820194.962672222,799291.4444500001,961598.3333333334,2536747.888888889,2603952.722222222,1162008.4875216666,4381285,4706798,1080124.2777777778,1775344.2899333334,671906.8333333334,6223120.611111111,1559459.3819444445,1460668.2395833333,3653583.0937777776,6143184.333333333,2142989.5625,2106082.7916666665,2227121.722222222,1012172.8333333334,3924571.5555555555,871711.0555555555,9043614.5625,20975286.916666668,953552.6666666666,5654098.541666667,2242198.923611111,2212777.8541666665,893295.3888888889,1651792.7465277778,32919152.888888888,1134089.2777777778,2071032.7777777778,3694604.6744777774,1311776.6666666667,2950059.348961111,1103694.4444444445,2006027.8333333333,3254622.222222222,2037488.388888889,4309261.333333333,4117220.9444444445,8621538.166666666,1018068.3888888889,3427672.05295,4487208.287327778,1535516.6666666667,1276641.111111111,2497574.5,2788135.097222222,979900,2672923.0555555555,2997176.347222222,2637372.4444444445,7246576.340277778,1190948.388888889,4566141.659722222,2595833.722222222,4529679.208333333,2292068.888888889,1305068.4444444445,23419546.777777776,12931419.916666666,844203.4826444444,2048342.3333333333,2696374.7083333335,2354510.388888889,3091715.579861111,8011066.694444444,2062351.7777777778,2614302.611111111,14565124.583333334,768538.7777777778,770757.5581666668,903794.4444444445,2787233.3333333335,1886012.2222222222,752611.1666666666,4545229.888888889,5162323.111111111,886526.3333333334,18454927.194444444,949811.2986111111,975312.5742166667,31185466.166666668,1169732.7777777778,1005847.6666666666,3421829.2083333335,572152.5555555555,1305068.4444444445,2115021.3333333335,1334571.3333333333,43672136,961598.3333333334,1229134.4457444446,1275073.5555555555,2202858.1666666665,4739265.055555556,6195946.614583333,1105883.9444444445,2208337.1666666665,5955313.777777778,914891.0121555557,29362308.25,19238686.472222224,35298014.833333336,8757408.378472222,13824867.972222222,5092247.333333333,1434240.611111111,1688741.6666666667,1873067.048611111,1460101.1666666667,5466881.555555556,1978058.111111111,754468.3333333334,8986949.333333334,6278285.5,3557115,2418379.3090555556,1158281.3333333333,2750936.5,843989.6805555555,2164558.9444444445,2154718.3333333335,2160237.138888889,24705942.777777776,1327658,718325.2777777778,1016366.3888888889,1013251.3203388889,15679192.611111112,1888039.6666666667,3348283.6562777776,898503,1168087.388888889,13980401.222222222,39406257.05555555,21686273.333333332,6476193.291666667,2319475.048611111,2111224.777777778,5468425.513888889,708616.9392388889,1702848.5104166667,1323739.611111111,2205024.888888889,4437762.350694444,693180.1666666666,1242994.6666666667,754129.9444444445,2283944.039061111,12933346.652777778,2848189,1583290.6666666667,3746780.152777778,911998.4826388889,694606.9444444445,2700930.388888889,17291545.666666668,2737960.888888889,6356600.819444444,24198254.472222224,5951211.277777778,1595656.2777777778,50531556.666666664,4619379.777777778,6312181.055555556,7494947.166666667,2199679.277777778,49583121.666666664,2709909.809027778,7274546.861111111,8789326.5,2766304.277777778,1734964,2328509.5442999997,4571703.215277778,1044343.4444444445,1972587,2174831.4010444446,1476994.395861111,4155822.222222222,5505350.444444444,33880972.833333336,2828061.111111111,1418525.9444444445,2405155.7395833335,3691741.0555555555,2522778.8333333335,5307683.909722222,1063689.388888889,1021201.5850777777,4303544.166666667,2918542.111111111,9016252.25,885612.3177388889,6940386.833333333,1560261.7916722223,1602327.6284722222,2044095.4444444445,2594406.486111111,1175194.2777777778,2642957.423611111,4004100.7118055555,61260360.166666664,3463580.340277778,29039734.222222224,721862.1666666666,726323.9444444445,2614365.611111111,2042476.111111111,949381.3333333334,1050526.611111111,874762.1666666666,570744.9600777779,9136988.694444444,814921.3038222223,904588.0659722222,7610007.5,3575642.9444444445,862610.5,6479506.847222222,937263.5,6149126.638888889,26325280.916666668,770737.0555555555,1317405.9375,2237927.4444444445,4939955.083333333,15335687.555555556,19322851.722222224,1336883.1666666667,2245345.3333333335,3328476.0911444444,699958.3888888889,1816234.704861111,1548380.3229166667,1483846.6124111111,1820941.3333333333,810707.5,6237264.222222222,948567.9895833334,1930189.6840555556,2604449.5555555555,2512906,1590585.6493055555,2458228.9618055555,3262505.5,6444734.888888889,909803.0156555555,2846693.9444444445,1771735.8333333333,1193254.4444444445,2222083.753472222,1388029.7222222222,1845105.2743333334,1105079.013027778,1789976.6666666667,1202156.8333333333,20800527.555555556,2259387.6666666665,4187835.7664944446,1137094.5243055555,2135686.1805555555,45921096.55555555,1562977,3158032.4444444445,4120475.722222222,2419291.277777778,1040552.3159722222,1439530.0555555555,1715693,700911.2222222222,3985883,4309195.753472222,2536442.0625,875575.1441,1463993.388888889,2247462.972222222,862610.5,50113901.333333336,933581.5555555555,998521.7222222222,30661018.194444444,1271806.9444444445,798552.6892388889,997781.1597500001,936461.3888888889,10640641.069444444,2592491.4305555555,2542009.777777778,1230130.1015666665,3020200.722222222,2282077.722222222,9080909.527777778,6506612.875,2117496.190972222,3316082.840277778,35112393.722222224,1863016,2031005.7777777778,1078916.9444444445,2422646.034722222,2764306.5390611114,856533.2777777778,1141724.7777777778,551943.1302166667,2746789.1666666665,1673882.3272555554,1751894.3020833333,7364691.194444444,3322270.4539944446,3966895.388888889,9259350.055555556,1828536.201388889,7474869.347222222,4169062.034722222,3010537.1666666665,954780.0729166666,1036868.0277833334,1006045.4444444445,1529713.3368055555,1364499.9444444445,774919.83855,5496778.444444444,2022294.5555555555,810349.9722333333,3522191.277777778,5932043.923611111,10284256.611111112,4187835.7664944446,1573189.789938889,1127359.6631944445,2224658.1666666665,6657774.777777778,18698735.055555556,5154426.076388889,8876207.708333334,1303650.7222222222,1524178.545138889,1934566.4444444445,2909558.958388889,833412.8888888889,1109792.017361111,6353459.604166667,883936.3888888889,2212777.8541666665,17107215.277777776,15387778.666666666,1983163.888888889,4552594.791666667,5687431.75,1186061.793411111,766714.2222222222,15445985.666666666,2966488.261283333,1140019,977729.9444444445,1631730.4218833335,2781476.111111111,3679094.1458333335,1996559.3506944445,11522583.111111112,1215351.1666666667,5052928.222222222,3954587.097222222,7988444.763888889,4905088.208333333,8769984.805555556,890118.0902777778,4381780.444444444,941834.6111111111,2980637.7708333335,2125360.611111111,947617.8888888889,7281531.777777778,826942.4201444444,2534493.03125,5671686,16489676.333333334,6615963.395833333,935238.7777777778,2738922.902777778,2800220.753472222,7436921.222222222,1633603.9444444445,5504264.982638889,1366723.6666666667,2273911.0555555555,1337584.28125,1630602.9444444445,2545991.9444444445,2333493.9444444445,8319829.25,7015637.888888889,3034127.923611111,2541871.111111111,1598771.7222222222,885612.3177388889,20335797.888888888,7130750.333333333,3072243.9444444445,3822538.222222222,4470161.722222222,2266127.25,2184719.170138889,1743453.361111111,1095765.484377778,1978000.6666666667,2212777.8541666665,2047159,2213675.236111111,1803718.2222222222,3014887.7118055555,767148.1666666666,2431276.611111111,1904798.6666666667,7436034.666666667,3312817.5555555555,6195946.614583333,1320581.3333333333,1598771.7222222222,40022027.38888889,879697.5868055555,1440307.9219055558,2716784.7395833335,2294513.65105,3036180.111111111,1292691.6666666667,8929975.611111112,3900017.2508666664,965549.1875,1629513.0555555555,15544286.916666666,3629719.4791666665,1976689.6666666667,5281888.666666667,924107.5555555555,16484090.916666666,16776251.805555556,2994547.5833333335,1401219.1041666667,1604202.8333333333,1461413.9444444445,10915613.972222222,3362210.0416666665,2967773.6666666665,935574.6111111111,862610.5,764142.6666666666,989726.1944444445,8131729.916666667,2292047.1666666665,3798673.722222222,7698774.555555556,2544571.5191,12587024.722222222,3985007.673611111,2082086.8333333333,1037119.6111111111,3121612.111111111,811060.8923611111,968318.0555555555,13957147.138888888,976259.125,1307088.611111111,2480051.0208333335,2258856.263888889,7958187.097222222,16041145.388888888,5157756.805555556,9790485.763888888,4362687.620661112,784333.5416722222,4706798,12715055.888888888,579237.6666666666,1046448,1846248.2508666667,879274.1597222222,11741984.888888888,3087282.6875,1179294.875,758459.9444444445,2532435.90625,1023410.4444444445,725639.3333333334,1149734.3524333334,2297808.388888889,2731858.388888889,1143481.7899611113,3150651.4444444445,3255759.888888889,1131134.3333333333,1127359.6631944445,1813520.3333333333,4255858.284722222,8599357.729166666,14110799.638888888,42936722,1134599.111111111,1000455.0555555555,6101943.25,1833087.5555555555,1169732.7777777778,5314663.166666667,4967790.666666667,1084886,2224250.772577778,2555773.0555555555,1087800.388888889,2747999.2743055555,8552626.236111112,1794298.2882222224,1426198.697077778,1010405.4444444445,838631.3888888889,2659352.722222222,1251390.6493055555,1367582.9908833334,20330152.055555556,11189323.291666666,1703571.6883666667,934510.7482944445,3344963.215277778,4832835.5,2505548.6987833334,2495119.909722222,3996517.722222222,1026346.9444444445,12413027.666666666,2948383.5555555555,3206700.638888889,4144130.1744777774,1298811.388888889,2732240.388888889,1467845.2040222222,938688.7222222222,1255402.9479222223,7524315.013888889,13382886.222222222,6231469.055555556,2289239.611111111,1178103.875,2163357.0208333335,71569572.65277778,1758409.1909722222,7077201.152777778,2277314.3680555555,2000080.3333333333,1463848.5729166667,4742992.888888889,2380990.777777778,2627565.277777778,1598771.7222222222,2805517.6666666665,4170564.652777778,13584670.944444444,9177753.611111112,1410081.0555555555,2917511.8333333335,12837966.333333334,766738.8333333334,5354581.055555556,13609439.5,3544606.777777778,6018755.569444444,878309.9375,1734741.7222222222,734976.2552111112,1838691.3463555556,1552392.7361166668,3903368.777777778,551943.1302166667,3012235.8819444445,2152678.1666666665,6611902.333333333,1200465.4444444445,2333811.0069444445,1043469.1215277778,1039074.6423611111,1350978.022577778,2898319.076388889,1096255.484377778,4673270,1320573.6024333334,30690812.277777776,866439.6666666666,1276332.107638889,3305371.8480888885,1993681.111111111,1727989.5555555555,11807749.388888888,1628612.2777777778,2483234.076388889,898582.9444444445,3374623.0807277774,559887.3281333332,2377759.777777778,2903382.5,4327837.256944444,2405228.4444444445,2886588.954888889,1069850.8333333333,3762261.611111111,801470.4418433333,860639.7222222222,1389702.388888889,893807.8888888889,2011591.6666666667,11165490.166666666,18566853.722222224,12533611.777777778,1448061.5,1756781.8333333333,1125527.0555555555,3196184.9444444445,1364568.8333333333,1909194.8923888889,3286794.923611111,2288703.1666722223,1238125.364611111,4182230.722222222,3983150.777777778,4414141.5,1215351.1666666667,12424643.472222222,1049848.388888889,1753108.9444444445,8916629.833333334,6356004.555555556,6636869.013888889,2633109.888888889,933010.8993333334,14885206.444444444,4805339.135416667,2299366.625,965231.3888888889,1745264.86545,15712507.194444444,7227026.5,12115635.263888888,1391156.3333333333,6681468.666666667,7370365.604166667,2251437.090277778,2117496.190972222,1816234.704861111,11928758.611111112,2251437.090277778,2934568,4990581.5,7185997.614583333,121526853.1111111,10701737.666666666,11779419.138888888,1012855.5208333334,2091630.220488889,1017148.3576388889,923836.2478277779,1136206.039605,7865666.055555556,1621369.808161111,1529713.3368055555,1147303.888888889,3979386.138888889,2104052.722222222,3174756.5,799123.6111111111,909803.0156555555,1515994.558188889,5389606.444444444,1658597.4253777778,2184104.986111111,1905947.111111111,8678983.944444444,1591687.2534777778,950621.0382000001,16455244.638888888,14370623.972222222,870407.3550611112,4948890.888888889,8649631.902777778,1039162.8680555555,2449789,938688.7222222222,5508463.833333333,944213.4444444445,2378309.6857666667,1023410.4444444445,2948383.5555555555,801750.7708333334,4253895.833333333,1666935.8333333333,2378309.6857666667,2695524.660588889,693151.6666666666,758503.6111111111,1515706.7916944446,3184347.5,1915718.5,905723.0555555555,1106652.388888889,13373774.805555556,2141813.8333333335,968318.0555555555,3741750.375,1490958.7126722222,2205024.888888889,1479783.0399333334,7968940.298611111,993334.4444444445,672069.3888888889,2188846.954861111,695023.0555555555,5426196.986111111,13331426,1030094.4166666666,1046347.6770833334,5481068.777777778,6675969.260416667,4750989.399305556,33087312.888888888,1335407.9618055555,4382508.027777778,5680400.277777778,2948383.5555555555,2062351.7777777778,4658605.347222222,1393978.2777777778,11267821.25,2049313.2222222222,6724120.451388889,9638949.875,1479961.3333333333,1699882.989588889,5264532.388888889,3264568.222222222,27620312.083333332,944140.2378555555,4278227.426216667,5332915.645833333,5267913.277777778,1088299.2777777778,6809587.5,850476.2777777778,1085745.8402777778,4118500.965277778,2887290.786461111,1085903.2222222222,3145877.111111111,5679497.451388889,807174.0251738889,13256922.083333334,2582959.9444444445,2571532.4444444445,1081459.7777777778,2152880.7291666665,19731723.888888888,5640809.590277778,874762.1666666666,1346131.8993333334,2152678.1666666665,2549877.534722222,14270863.75,5054181.833333333,7719424,1331503.4635444444,2060315.888888889,2422742.6180555555,1686513.4305833334,1554966.8333333333,12958170.611111112,703550.1111111111,2433942.409722222,3476598.777777778,2297952.888888889,3125805.0555555555,1717490.888888889,3494455.871527778,4082629.6666666665,1699607.1562777779,1648078.2777777778,2908027.0555555555,16951761.166666668,1613880.888888889,2205361.5,971304.9444444445,14463392.861111112,2521639.8333333335,677294.0555555555,2086861.7222222222,2299599.388888889,1633603.9444444445,904462,7071240.5,3036591.194472222,3930872,5054484.638888889],"xaxis":"x","y":[1437220,1614322,1416488.75,780410,5912066,1318947.25,281191,9146727,53332648,600528,2842760,6711975,2177075,8054334,962971,766628,3009967,2161828,2962476,860443,2075833,1121259,2709429,1406687.5,764094.5,26689658,5276777,1372475,1078936,9162048,4678999,768325.6875,572654.3125,562495.8125,2504814.5,4696639,875943.3125,436604.0938,1210262,841891,437372,1267348.5,2214604.25,1696448,4933374.5,1691021,3920913,1813864,1545519.375,1988760.875,1471914.25,5827595.5,6508595,804620.5,4236396.5,1524620,25307744,1661448,15232788,9512829,847298.8125,1489363.875,642052,22104512,1904641.75,131810,2462408.25,1261714,577642,479358,1355062.75,715064,63835192,1479728.75,592739.3125,557863.6875,5543575,2634166,2756494,433871,508057,308400.4063,1430356.75,12187231,592899,2216979,1884194.625,690750,673834,2478045.5,847767,647792.875,1810353,13458948,1072855,1606981.25,2291194,347437.6875,1423687,2045592,835258,2978555,2203026.5,1750419,1070381,614044.6875,5337334,2409429,3093378,430500,5767765,3418003,4618360,3546459,1310226,2729167.75,1639849.25,695955.8125,5061274,5454482,1498457.875,9190442,2996502,2890379,1795493,2364351,610744.6875,1424157.25,1003659,5907068,784584,725487,585851,1513700.625,4547129,362874,917100.3125,674961,619545.125,1906597,2001936.5,980356,1883132,721549.625,939840,1098965,23230284,533669.125,805751,7860942,883197.6875,784584,1049436.25,48143224,745575,1792425,603223.1875,1938613.125,15088676,1297041,13631141,2131817,1153276,4397572,960987.8125,19609472,7768294,2757855.5,7559797,1325972.75,25475474,1841968,559805,12051984,1469556.25,1314300,3375091.5,4665069,12726789,508264,9796649,1973429,17074142,7542332,531737.1875,1672142,1739693.25,3307792.5,1436773.875,843622.875,987987.3125,9997511,25478086,713660,3594908,2755829,1147582,2349950,27717026,1982967,429591,1645748.25,895901,947534,3528086.75,841821,973242,188745.7031,7564643,20311228,1593160,495481,5570472,2594119,336745.4063,2284133.75,5229981,13146645,838394.8125,2520919,3218565,1514279,9177040,1048781,6456026,24882924,6772289,10813735,4993962,628676.375,888561,892227,2842819.5,2265836,196410,541451.625,2371304.5,1325206,17812788,481206,519317,971155.625,1509492,1419453,1093724.375,8141155.5,797435,1162131,451738,1162519,5696695,6769410.5,2467851,6376820,1821047,600543,2637656,1299001.625,1857387,436848,2062419,1082076,1145535.75,3242077.75,1021754.688,1298039.875,395346.3125,1489392,1858224.625,1406987,1056099,2055534,840549.375,2126288,8819864,956598,10420096,350264.5938,507976,1565475,788976,599390,2345314,402136,2220053.25,65225380,880590,1916309,1241023.75,1093627,3879728,7442314.5,1256575.875,448676,1198291,1452804,645451.8125,2557933.75,4785692,522972.1875,919746.3125,1118392,145573,1093757.75,1493131.75,975327,2015280.125,861643.6875,1199742,699382,18760766,6471427,967492.875,2788860,2104255,3632613.75,5657046,866343,59991984,3497215.5,462192,682249.625,13140574,13349651,6856153.5,3033340,2646879,3925806,2852102.75,595978,2559486,841242,456397,618366,1555420.25,2058532.875,4383100.5,2321336,14697487,2241280.5,3059758,1260212.75,1791715.625,19594950,0,1689631,4863803,8259071,1820085,3983784,14084606,10287986,2966309,2059083.25,3040978,667871.125,869835,4704936.5,2091869,2067352.5,1199392,1878442.25,1297872,4917420,3591653,3318151.5,2105451,1391569,520206.6875,3292049,540465,6219841.5,2014500.375,5328224,421430,844000.3125,649874.875,10557910,4841347.5,4606967.5,962577,439521.0938,1275236.125,1426023,1299232,2655387.5,4252035,6320060,520011.8125,22388122,3371659.75,10769230,17225444,8787353,722951,3493093.75,1403094.75,2994822,710275.375,6454983,4328527,722891,2311532,1539191,1186835.125,2213115.5,456953,2601945,3986982.75,3537977,2339234,5417598,4169431.5,1222949.25,3646053.75,593543.3125,1266387,1951856.25,609368,5535626,9097980,758322,1509775,1417728.25,2076289.625,849745.6875,2228083.75,4471029,2334260,1773880,892086,1392299.75,847419.375,3644982.25,18558162,1162007.625,1686765.875,1253745.5,967835,56762408,11061916,65047284,4217623,928409,1742870,9734582,2046914,4817062,9329202,690304,646994,8241967,3612830,925293.3125,13214416,2019672.875,2717863.25,706505.6875,1242666.25,2154119,4567879,1128634.875,1176615,747994.8125,2477499,876569.6875,2666634,627854.125,1525624,1245187.5,688145,1820814.75,2468322.5,1885930,36378588,826488.6875,4391348,2447861,4045980,688641.8125,977150,10055690,850568,609101,5669431,2160420,12086616,5401277.5,835093.1875,694189,1378155,2625396,438557,3766069,4777648,1248599,579888.3125,528411.875,3155496,1359579,1769137,1686262.25,5917000,1004253,2564004,163945984,84980760,595327,1327591,2551022.25,585077.125,868163,507947,1331031,864751.8125,2093011,3586479.25,2892779,2946756.25,724153,3603866,2998208.25,1134195.5,2213421.5,4180298,2828950.5,1034827.313,1552645,2168281,3168809,730765,27137190,16614404,1360340,3091348.25,12015936,1445538.625,652449,1245197.5,157606480,896403,2512443,1933811.625,474811,1533245.75,1369190,1180231,3988729,1032857,836348,3185509,4192895,747326,3835202,13962749,960550,873776,1989017,686019.1875,588965,1072500,3996541.5,1777554,7655751,5689125,4728488.5,2654804,4585070.5,591249,1227772,21346100,6481807.5,489557.0938,2926166,1712279.875,1815350,952635.375,8873485,2920918,4329966,18628834,739711,767642.8125,504656,715446,317581,577023,4983934,2982450,972608,10614234,576038.1875,905926.875,20657852,896976,1965529,544172.375,638201,866242,914023,1111858,41655852,2440980,3605755.5,1093743,1516377,10340282,1015060.188,1045640,2726269,12087498,566000.6875,284867168,33729336,39403320,5974993.5,11924933,2993610,766381,1608224,1056782.125,2347893,4405368,357915,633408,4634655,8204897,2271263,2536760.75,694479,1271021,1315298,1484807,846274,1348497.125,11320378,1840259,502254,666385,807185.5,6860521,1758581,2971013.5,1162226.375,876871,40183424,48479988,30083496,6649938,1435558.125,1259959,4369513.5,493969.6875,1696978.75,777915,9143764,1052244.75,758266,1109851,696840,1608884.25,10084950,1224554,507217,4855036.5,919266.125,631195,5375518,9341427,1998767,3730953.5,59107620,2751694,3473109,59757440,3714139,5424718,6344171,1427294.375,131373880,2520483.25,6012303,12931002,5040382,9620496,267135.0938,36436388,1380283,1240875,1240924.625,1133028.875,6205886,2503948,32552428,9393108,811870,2786648.75,3913410,1084048,6298131.5,523133,439266.9063,6981428,6668568.5,7574092,1872278.625,2757588,5170809.5,2640980.25,8381744,1750362.25,781890,2100813.5,7335905.5,63811044,2330242,24603420,609257,549834,3322250,1695593,1357116,756196,980418,493000,7325166,703847.375,1356874.75,4826430,2057288,1360847,5327249.5,697824,4039667,23262244,842923,713326.875,7067404,2577115.25,2512319,25909504,1647746,19061780,2125323.5,636266,4729846.5,1601955.625,0,913899,529072,3404992,522791.0938,1187540.25,1527917,1182575,926261.1875,1725336.25,1152639,6915293,739869.6875,4904803,1172785,1230757,1063269.125,3467070,971990.3125,576174.125,1929118,918177,18576900,1988784,9230223,1163748.875,1333016.625,47952868,2074152,2660396,1552051.25,2056814,781682.5,1070740,734495,1059424,8063959,2145603.5,2217046,625988.625,2188093,3220635.25,791640,37980712,912509,728876,53466684,763693,1027230.875,948676.5,654664,16373232,3518459.5,4335178,1581036.75,1241865,1140620,7459418,52960796,819003.3125,2161240.25,59124384,2083584,906103,792015,8687690,1441665.375,449955,557193,455798.9063,2196718,4519115,1313366,5459657,861972.875,1012341,6669954,1332591.5,1507270,3893073.5,3132453,789209.8125,561473.875,519573,2715249.75,1140137,1791071.375,12783255,340270,588856.125,1970956,6582088.5,11047668,2925780,1055033,839011.3125,1605522,624180,20828130,10394330,10716391,1026921,659652.625,1987073,1172903.375,609218,2595783.5,3534690.75,2171445,980773.375,14431245,13852986,2668769,859236.625,3276819,1242311.5,696522,19459304,595936.5,165472,2108012,1848878.5,1090032,1201781.125,6584966.5,8631444,1290284,8688676,2599171.75,6843855,2712983.75,5952875,1750099.75,2856934,666355,3050455.25,1881516.125,700899,15240688,1625308.625,2502081,8227025,11208498,5885427.5,908834,1866802.375,4590246.5,1976920,948657,763364.5,1097572,3468606,2827772.75,1959777,1834290,3347567,4253577.5,3085505,0,2254220,1718845,755856.875,8386493,1738376,7369591,1886007,3075425,3724098.25,2979413.5,6881405.5,473874.0938,455153,3300549.25,422133,7606084.5,831577,519806.0938,776427,3158743,793436,9974370,28229320,3574542.25,630040,1243282,27273538,864420.875,953059.875,430473.6875,1269847.75,2471623,1372706,3182301,2556131,1440554.25,849504,8976533,3133849.5,1302636,5549121,659265,13491170,8915523,1045838.5,1921801.25,1231546,1210229,45313836,2338567.75,2322314,2327611,1372834,983687,741999.6875,3590731,3635600,4376927,18074716,2108861.75,55073120,2105912.75,1456039,843520,4673827,620154.375,2248776,21042926,547084.375,936336,1688612.5,3735308.5,5328051.5,4759256,9546165,14856199,6345009.5,601813.125,10077532,19036416,472532,653366,1856230.625,784329.5,42566736,2495819,1083156.5,362823,378714.4063,957632,488181,867184.375,1800331,1692323,3778355,1983509,2337146,1187261,618670.875,2219190,1650279.25,6042399.5,8555790,45163308,1415494,1125724,5965171,7291418,887403,5851209,3162976,1605669,161311.0938,4367202,731092,3548858.25,0,1619177.125,478690.0938,641713,572139,876304,783651.5,1852926.625,11110405,28731502,609921.6875,4857581.5,586589.1875,629036,1586388.75,1675510.875,3371455,1270582.125,20361158,1633499,3704086.5,6858533.5,832985,4202335,1906384.875,606886,743528.125,16016644,4130053,13951571,1743198,2266983,1331122.125,35891484,1386983.375,6062770.5,1904480,2674034,764832.6875,3282191,662012,1406648.125,1024033,1612797,5662234,11521702,16246106,664539,1465535,24017770,610512,3022275,7273155.5,390726,25631512,650205.8125,1350190,1740568.25,924192.6875,1139980.625,2185193,471484.8125,15473117,1922953.25,7577723,563298,1119640.25,915286.1875,960790.8125,863703,1591528.5,1358519,8835104,862945.125,36681188,571384,673907.3125,2222097.5,1681424,3005926,10444297,2356607,1754477.25,901679,2331952.75,535649.875,1870069,13924967,1835804.875,1346952,1152368.25,885865.875,2799874,776248.6875,1615855,4952487,767720,2139713,11788719,15571642,9686734,656980,985236,1834570,8442782,935442,1748629.875,2352982.75,1832808.75,932083.5,1612965,2650112,5471735,1324965,8690179,827618,1079004,19490284,13720714,5571501.5,1457118,833747,17186624,4345698,1371061.25,942089,1100499.75,14361382,6578370,5987079,1011399,6579348,3017709.25,1360400.5,2313978.75,44293.5,5118939,1497496.5,2439061,4660015,9343428,57514116,8422861,13256662,2181833.5,1076733.25,987288.8125,692127.875,671004.625,5143676.5,1073156.375,982153.875,391058,8058913.5,2277316,1377184,454296,690259,4801472,5755308,823032.375,1416470.25,1460307,9274650,1261634.5,3532966.5,13223740,8890235,1870457.75,3361743,3459543,953432.1875,4476997,1494252,7657568,597210,866126.375,648022,3494383,1258978.375,1679127,842593,2771820.5,930611.875,991283,622051,3936421.5,1961982,1160989,672915,3678939,9683979,1381407,777696,318364.8125,2470642,11007235,1517410.5,2845946,497556,549438,2520310,660508,3736960,75073888,1166069.875,705621.6875,1417778,16865978,3652708.75,14856602,97690.39844,4028075.5,14064603,1113488,1387773,1388812,1392153,8355988.5,1528642,17695420,61762380,1305578,1814947,2671115,2303288,20781390,2473302,605971.1875,2115666,4407932,804002,7753080,524144.6875,2466100,3579008.75,2188051.25,788027,2558573,7802205.5,560823.625,8987012,984657,1100575,1459786,1078535.75,33270518,5475367,872555,770175.8125,872114.5,1963481.625,45252356,2802428,3073778,979442.3125,752441,4145920.5,1294615.25,1730886,41696464,1826203,3234787.75,1188444,1501767,1819329,1049088,10105379,16238985,8305255,978463,2549892,16467486,1095744,440658,532670,7945081,444458,1122837,2454812,1247972,1130097,701422,6235947,1128311.125,6424029,2960159.75],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle KNeighborsRegressor(n_jobs=-1) vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predkNN"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["n_neighbors = np.linspace(0, 100, dtype=int)\n","param_gridkNN = {'kneighborsregressor__n_neighbors': n_neighbors}\n","\n","\n","GridkNN, \\\n","BestParametreskNN, \\\n","ScoreskNN, \\\n","SiteEnergyUse_predkNN, \\\n","figkNN = reg_modelGrid(model=KNeighborsRegressor(n_jobs=-1),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=SiteEnergyUse_train,\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predkNN',\n","                         score=score,\n","                         param_grid=param_gridkNN)\n","\n","print(BestParametreskNN)\n","ScoreskNN\n","figkNN.show()\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[null,14604618.268413207,14112352.579474393,14016357.451701578,14129203.517899245,14038495.570327297,13945944.102807477,13929957.667590335,13916602.706140319,13757234.537187025,13910239.901208526,13915428.792108918,13985429.563598031,14064730.976430839,14131629.532250768,14174790.340535939,14190766.354774702,14201296.165771801,14221211.779531162,14203249.096891165,14250248.339334939,14275813.52074705,14306595.529711222,14341483.350632,14386407.775412222,14447815.552198593,14463848.359397357,14499187.863624487,14517035.601158783,14539570.400983393,14560494.697830623,14540310.095415879,14569618.13415278,14598848.933176706,14615410.55882208,14644652.683646375,14681308.171523143,14709360.83718143,14736130.675720045,14764648.906625101,14788901.205981996,14813551.758851757,14843267.44478049,14870205.175797945,14890654.54858837,14919539.843754899,14940720.95916583,14966411.35308953,14984087.430103958,14998612.712443933]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[null,20857228.70937034,20296296.221860223,19713333.782584667,19955431.85552182,20145350.939306147,20351078.356041014,20211158.74540613,20365881.494824175,20224386.084186293,20404731.41781006,20509712.806575157,20634418.909708574,20763412.27292595,20799753.225534186,20912054.096237093,21002653.42239154,21069009.770001885,21089506.719383582,21107847.8122764,21173712.245556433,21235172.173351534,21307561.117570773,21377625.888157837,21454618.952552166,21561563.878275268,21614143.078921366,21669284.566760566,21713271.899108935,21747421.579697102,21774219.027876984,21692924.762819096,21745525.02003472,21795196.133014128,21811719.80842171,21856997.09116141,21905255.845529996,21946793.532522336,21978124.72898764,22015780.981259644,22053198.73801728,22082539.569815926,22122495.769128814,22157941.078338217,22188925.537183326,22222804.36346876,22251260.864773087,22284184.85519833,22307891.623942994,22325326.848641682]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"y":[null,8352007.827456072,7928408.937088563,8319381.120818487,8302975.18027667,7931640.201348445,7540809.849573938,7648756.589774543,7467323.917456464,7290082.990187755,7415748.384606991,7321144.77764268,7336440.217487487,7366049.679935726,7463505.838967351,7437526.584834785,7378879.287157862,7333582.561541717,7352916.83967874,7298650.38150593,7326784.4331134455,7316454.8681425685,7305629.941851674,7305340.813106165,7318196.59827228,7334067.226121917,7313553.639873349,7329091.160488408,7320799.30320863,7331719.222269683,7346770.367784261,7387695.428012663,7393711.24827084,7402501.7333392855,7419101.309222454,7432308.27613134,7457360.497516291,7471928.141840526,7494136.622452449,7513516.83199056,7524603.673946711,7544563.947887587,7564039.120432165,7582469.273257672,7592383.559993415,7616275.324041038,7630181.053558572,7648637.850980728,7660283.23626492,7671898.576246182]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,9757367.835717132,8189102.714966665,8829454.545797296,8556384.728395501,7979703.398429253,7585882.743421573,7665775.752562455,7395310.748130999,7153104.158876655,7350154.552666488,7157879.814898326,7047551.457148414,7190132.394620659,7318317.132828622,7210346.675254686,7057930.418312787,6984661.1265040375,6915539.2087499695,6781678.871967295,6780380.164151528,6688871.710159452,6672788.47000391,6596824.042017564,6577314.621582674,6548644.057191236,6466089.511027987,6455182.664665171,6435327.092219836,6432762.159744278,6419127.703065814,6431490.457628959,6406262.36758398,6409186.259989952,6383163.405472665,6386934.081385529,6401440.379909539,6397349.216575241,6405318.024951617,6408161.83013025,6406888.152704425,6408732.287930951,6419646.486133137,6433191.347277866,6438476.786133639,6461779.570591584,6461606.746347523,6471392.625057372,6476472.861382662,6459565.731053293],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,25811172.62511976,24311367.95711588,23559037.725443427,24008755.6645481,24371507.7955855,24880615.42359782,24605262.136358578,24921202.329458117,24688634.97506077,24849911.70768967,24997635.827894617,25124891.878250334,25292444.436713226,25254247.493396033,25360374.111443695,25504800.742336452,25582223.489687495,25541612.385931697,25483454.947259087,25536793.270481646,25610061.536531974,25695575.069318105,25762284.803416345,25850065.23068311,25971090.271161754,26033321.91036317,26085662.463253252,26135496.41313164,26179736.126206767,26182259.786965188,25990048.755978197,26045040.875791535,26120326.052394107,26104354.779751282,26145352.018882293,26195933.115191963,26236441.19518461,26266854.18021641,26305601.508408748,26355337.77951731,26377642.60179737,26425471.198149145,26467134.84833613,26504144.08576026,26538786.547159255,26567107.289247267,26605868.821394686,26633788.66454054,26653693.342490725],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,8077615.500531307,7210196.575453103,7617302.419725391,8029127.809054509,7781833.613009088,7543983.112009839,7613692.722853184,7632853.115403798,7393656.2814488895,7385281.094068553,7353251.00423839,7423679.8844794,7346420.877705483,7342707.103255069,7327495.802571797,7371636.0927898865,7323345.390853583,7351629.63822235,7262392.920305114,7287516.873407592,7335086.725199803,7302234.1157991355,7347769.344420928,7372996.755680876,7417537.965322385,7444352.857817569,7465170.786996456,7453415.4843942495,7479804.180147783,7490047.729116274,7487274.210485792,7511770.808765041,7535726.871070012,7562905.080925357,7566055.435875731,7594749.092597341,7616584.733378222,7647820.052158377,7678365.614708529,7703199.07097986,7730028.214496484,7760297.967918099,7781965.538189254,7794516.552921173,7814882.37549639,7838344.7444246495,7865602.486772756,7883885.650933459,7923423.230896363],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,13314211.029152744,14714116.092036188,14239375.315342495,13966662.419758547,14060837.928491974,13751333.508042175,13797322.035519931,13545757.51848818,13463560.07928859,13697515.131958526,13734630.507866714,13944493.468840744,14000886.539118972,14142252.436405178,14263312.807893153,14255853.310505755,14286523.145398831,14408316.187944487,14495531.472919516,14586692.563126568,14675894.346647033,14759857.96599534,14836895.248742966,14931853.992794257,15006680.108091725,15047608.344971722,15121090.812256021,15159685.68070804,15186146.64890221,15258621.82405077,15320545.44736594,15366903.91678371,15398427.259203952,15458163.752458042,15520577.663269298,15571160.602563152,15620289.117310433,15663376.407946322,15704959.757226445,15741459.060306937,15789384.71982431,15814973.752917945,15853426.789083362,15879613.314546201,15917137.741468383,15947047.102053408,15980539.11707828,16001559.85895619,16042319.732304351],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,100],"xaxis":"x","y":[null,16062724.351545084,16136979.557800125,15836617.252199288,16085086.967739565,15998595.116120676,15967905.726965984,15967735.690657524,16087889.819220504,16087217.191260224,16268337.01965938,16333746.805646533,16386531.129271252,16493770.633995857,16600623.495368935,16712422.305516357,16763611.209928617,16829727.67641505,16888961.476807307,16993187.27200481,17059858.825507354,17069153.285196997,17102522.02743963,17163643.31456219,17199808.2763202,17295125.359225858,17327869.17280634,17368832.590951536,17401253.335340146,17419402.889915936,17452416.44595507,17472191.605620507,17518112.701839626,17530578.223225508,17568465.77550305,17604344.218819037,17643257.667353716,17676139.923458647,17697284.713327494,17726155.82265154,17737621.966401454,17761970.97020967,17795947.81878413,17815307.35610311,17836522.00358058,17865112.984058883,17889498.9137563,17908653.715144556,17924730.11470693,17914061.52547494],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle kNN en fonction de l'hyperparamètre n le nombre de voisins"},"xaxis":{"title":{"text":"n neighbors"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE kNN pour tout les paramètres de GridSearchCV\n","fig1 = go.Figure([\n","    go.Scatter(name='RMSE moyenne',\n","               x=n_neighbors,\n","               y=GridkNN.ScoresMean,\n","               mode='lines',\n","               marker=dict(color='red', size=2),\n","               showlegend=True),\n","    go.Scatter(name='SDup RMSE',\n","               x=n_neighbors,\n","               y=GridkNN.ScoresMean + GridkNN.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               showlegend=False),\n","    go.Scatter(name='SDdown RMSE',\n","               x=n_neighbors,\n","               y=GridkNN.ScoresMean - GridkNN.ScoresSD,\n","               mode='lines',\n","               marker=dict(color=\"#444\"),\n","               line=dict(width=1),\n","               fillcolor='rgba(68, 68, 68, .3)',\n","               fill='tonexty',\n","               showlegend=False)\n","])\n","\n","fig2 = px.line(GridkNN,\n","               x=n_neighbors,\n","               y=[\n","                   'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2',\n","                   'ScoresSplit3', 'ScoresSplit4'\n","               ])\n","\n","fig3 = go.Figure(data=fig1.data + fig2.data)\n","fig3.update_xaxes(type='log', title='n neighbors')\n","fig3.update_yaxes(title='RMSE')\n","fig3.update_layout(\n","    title=\n","    \"RMSE du modèle kNN en fonction de l'hyperparamètre n le nombre de voisins\"\n",")\n","fig3.show()\n","if write_data is True:\n","    fig3.write_image('./Figures/graphRMSEkNN.pdf')\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                             paramètre RandomForestRegressor()\n","0  randomforestregressor__n_estimators                       2\n","1  randomforestregressor__max_features                    log2\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"SiteEnergyUse_predRF=%{x}<br>SiteEnergyUse_test=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","showlegend":false,"type":"scattergl","x":[2444817,1525995.875,845435,1045428.4375,2326306.75,947099.875,348061,19933096.5,44984468,740386,3465714,6482380,3305791.5,4189356.5625,960441,784234.0625,1000796.875,1843512,20400012.625,810067.40625,1827643.5,1273651,1977673.625,988802.9375,711118.375,156347890,3655696.5,760008,791669.5,7686952,1894449,2653890,565952,1543833.125,2478960,3909668,14293972,475255.95314999996,1185122.5,838571,348051.5,1694930.375,2666798.75,2621663.5,4421754,2076491,6016997.5,1853245,1728704,561314.9375,3255037,3454452.625,7588321.5,813641,1269026.32815,2010481.5,20416053,1554342,5530630.5,12309632.5,1933021.625,2826828,776217.90625,18906180.5,1711702.375,847220.5,3321035.75,1742665,557699.15625,645756,1400708.125,1136732.5,14966360,1399992,782549.15625,752296.0469,7223466.75,4795070,10140074,1705515.25,659155,1021631.875,1599376,12029600,890977.8125,1422264,1971877.125,718259.0625,769685,1754276.375,847220.5,514917.29689999996,2301147,30738725.5,7486408.375,1604129.75,1315143.5,659761.875,777272.5,4753524.375,1987311.84375,3004836,4686474.375,9232576,1240904,882399.65625,1973638.875,1723318.5,7392513,1290113.5,2066817,2972493.5,4303904.25,871877.5,2704889,2333589.25,5388224.375,694834.09375,3132571.5,5906832.75,1666450.1875,9621807,3356077.375,2724601.9375,1216867,1827643.5,830508.625,701143.70315,919970.5625,7383288.5,794129.9065,1127132.25,740610,1579442,5830242.5,1046198,673227.65625,918846.5,807508.5,2157011.3125,2547125.625,840157.5,5375658.75,726724.59375,1186682,772860.5,18906180.5,1166930.75,1029539.09375,5531740.5,1427921.75,1324507.5,1481311.8125,34404057,780850.65625,1893654.875,1575606.8125,4190973.5,29405063.5,1130186.125,13578421.5,2344781.125,1156866.25,2247700,974278,17008364.5,4364451.6875,6239245,4675259,1669592.125,156347890,1937241.9375,704152,2358588.25,1372484.875,1281083.0625,4429459.375,10087869.5,13116256,712138.40625,9765777,4135490.875,11595455,1444351,568339.3594,645738.5,1163359.8125,3762646.625,2353758.75,1932328.5,840943.34375,2849795.5,22965152,1330615.625,2993968,5363073.75,5257510.5,2324897.375,21936285,1797216.5,596757,2479604.75,913760,1184732.4375,2671694.125,2242405.125,1527393.5,521341.9063,7170780.875,19963399,4799206.5,570304.84375,3471581,2268605.625,1443629.25,4392907.0625,3857930,4998015,893416.25,1381167,2980237,2121426,7911977.5,2080898.625,904550.75,22199466.5,6468600,14929239,3212406.25,628838.6094,867643.5,1490939.84375,9163510.25,2582936.625,1386331,488683.90625,4749972.625,1056303.5,19110967.5,567521,531097,1984721.125,1151027,1038567.5,1672148.125,7271106,1386331,1239974.9375,299828.25,1234149,16804125.5,28942573.5,6493374.5,4586442.75,1832192.5,508533.5,3835245,2068250,2057679,604895.3125,2805872,948985.6875,4167253.75,4330950.75,2547125.625,1933826.875,1226698.25,1742665,1683387.625,2228321,1876968.9375,1446951,601818.625,583205,8114442.5,1432618.1875,1964347.625,648363.6875,1269832.9375,1647086.5,886942,5725979.9375,724401.7969,2070465.5,3442190.625,15891776.5,709043,8978772,1600521.1875,2224029.5,4730550.375,1992205,2527168.875,916038.59375,1500473.1875,1130488,880917.03125,4749972.625,4223340.5,811646.875,1728857.1875,3365101.5,317211,1386422.3125,1173318.5,913260.28125,2899392.4375,5882726.5,1278891,5766136,33222398,5552759,621649.6875,497132,4621785.5,3415561.25,3170063.5,924569.65625,60479702,1964024.125,1570731,798322.375,2502601.25,15387140.5,14573490,1903395.5,1541897.875,2129718,683725.875,1466382.25,6454077,5557662,794946.95315,840157.5,1512239.25,1584276.5,5387386.625,2687338.3125,7475577.5,2095702.625,2370836.4375,1833364.875,4723868.375,25766285,1469046.4375,1321658.625,4089677.5,4532736.25,1782822.5,14405717.5,2507335,6076372.75,3291036,2563311.125,2663421.375,1141705.625,3551997.9375,3107435.5,20018184.5,3869834.375,2305171.5,2343812.125,1630814.5,4737743,4882486.5,3780443,2145607.75,1199211.9375,564457.8125,2456689.5,773313,5161486.75,1419922.90625,10263839,1027546.375,1728857.1875,1149383.9375,5825405.25,9432369,4815715,6533841,387279,1577294.6875,1374477.5,1151027,4428658.75,3059025.5,6132429,1277786.5,17288709.5,2485519.1875,14726457.5,3506373,12098956,1462090.5,3415561.25,1436510.375,3941309.25,4818495.375,4788053,3303989,654491.5,4235501,1932112.375,869988.84375,1148380.9065,3052799.1565,816972.5,4074349.125,2260916.375,758008.875,4031059.25,4574477.5,1294691.4375,2416029.1875,816510.875,1164378.0625,3506373,631078.5,2350286.75,4522570.5,911444.8125,1513581.625,793185.725,935224.0625,2950365.75,1410230.1875,5652350.75,2292846.5,9178704.5,760090,1520798.15625,2559516.5,6975564,6700027,2400116.5,1369136.5,3063336.25,1076207.5,156347890,11026945,46510960,3971190.625,1565981.25,2292846.5,6774974.5,2391060.09375,3982726.625,12546458.25,755036.5,733556.40625,13774726.5,2928099,915990.59375,13172813,684538.8125,531078.14065,623107.6875,995117.75,1971729.5,2545237.1875,1469524.4375,944057,954246.375,2027507,1310832.1875,8933153,1238568.71875,1305883,998247.125,806696.3333333334,1060647.375,2741578.5,3907808.25,32381836,1313682.875,8648909.5,1795702,8963609,869100.875,532348,19461907.5,20018184.5,1210067,2335327.6565,1508229.625,2358588.25,7563492,1015388.21875,3081014.875,1771469,1399989,5447469.375,626465.5,2689928.75,1637532.625,439065.9063,1342167.375,2697159.75,963569.5,1214874,1163510.6875,2666801,1147217,2010542.9375,231734322,6154806.5,4093065,881108,7548807,650830.09375,1089631,851000,4032104.5,1296970.9375,1372109,5825405.25,1154379,1133443.5625,575984.15625,7043861,762223.5,1518970.75,5911044.25,7021017.5,1785080.875,977106,1986640.9375,1903831,2392271.125,708851.59375,14446886.5,74740237,1023531.5,6015033.75,3446820.75,3071249.5,896603.5,1398660.4375,198627744,1293465.90625,1744656.5,2154448.6875,548820,1101596.0625,848847,2304125,2323987,1792306.5,2034225.9375,2646616.9375,4269049.625,1708182.2678571427,1058831.125,3606416,1654242,608694.5,1074495,1333564.0625,744041.5,2708863,5660829,1444351,7319546,4510960.625,10538030.25,3250570,10732507.5,436729,1169058,20599941,17088971,563620.25,1776645,1958763.5,2173143.75,2934874.5,19165116,2301147,2738276,11419472.5,729593.34375,638507.125,646691,2418269.5,1249068.34375,630771,4869255.5,2503177.25,594039,31032046.5,596960.125,4475650,141559804.5,1193773.5,1024042,4560492,560442.15625,914670,2425730,2063837,41213584,1386090.5,2586995.5,854601.5,1529165,7005489,2023948.25,1008312,2225165.5,5062054.5,1332753.875,162790728,19077031,47746006,4159371.5,11547400.5,3291635.5,812236,1296082.5,1210443.3125,849690.5,4870595,920454.90625,1191316.5,4258081,13798017.5,7427781.5,1596825,1903831,1811506.375,977376.125,6480157.5,1609126.5,5642831,25533580,1011410.5,644039.5,941238.4375,1291500.34375,27695495,2160491.0625,2268605.625,727703.5,415453,25517322,156347890,25933585,5884100.75,4404783.25,2178709.125,4078339,697442.125,2374305.625,1122459.75,9741283,7591140,688593,3014691,840157.5,4305086.25,46701127,1871780,788527.5,2628652.125,1296970.9375,655663.5,3007781.25,8972008,3022903.125,2953201.75,20622825.5,5852094.5,2614333.375,60479702,5267682.375,4085361.5,4097958,2376817.0625,37025880,1675314,7201629.25,15981961,1021734,1075708.5,1143070.875,5762778.3125,1469005,2423737,1578270.375,2013768.375,5070866.75,1632504,33626246,2381583.375,851078,1386445.375,4031591.375,3496912.5,5718604.375,566682.2969,1167045.875,7226362.5,2143327.6875,8575988.25,2009833.5,5913032.5,735199.375,2240076.125,2706796.875,3695289.625,684698.9375,2212058.5,4730989.5,61055854,1852581.9375,22952852,925208,668338,4586442.75,2411957.5,2073342,988802.9375,772860.5,466303.3125,8777371.5,646162.6875,1064752.9375,9150901.75,13459221,1036062.375,6687997,657914.5,6066020.75,19461907.5,1081253,948224.625,1278948,1694053.59375,9568212.5,29479232,2331662,10790923.5,1965380.8125,560406.65625,3110479.65625,1698657.375,2127283.5,1641885.59375,1169122,4106364.125,1098003.46875,881430.5,1881138,1867995.5,1013301.6875,1702206.5625,993983.5469,6779893,888014.4065,3036021,2469377.5,1749506.125,817216.9375,5004710.5,1267684.125,397540.26170000003,2043820,871877.5,31032046.5,4403182,6583690.5,1032412.5,883374.53125,44323848,1767836.5,4424674.5,3742204.35155,2015889.5,602618.0625,929676,535873,921769.0625,2172841,2133424.3125,4874586.375,1332753.875,1604846,3860449.25,533249,34751674,1229504.5,729535,46701127,4510960.625,697442.125,1059023.03125,765356.84375,19289423,1714731.3125,2569733,325221.45315,1120272,1858029,7131481,7336138.125,1387647.28125,8258586.5,33626246,4252881.25,1336502.25,1079290.5,6060799.25,1811506.375,1350266,1058701,706690.34375,2947274,2589096.4375,1115711.40625,8603973,4462074.75,5696097.5,12096002.5,1380293.625,5087000,6163200.75,2993968,766913.1875,730893.125,1190464.5,1922480.0625,1431466,5061402.40625,5681574,2456132,310039.00005,1557976.5,6761383.5,14001612,151754.2969,2345008.875,799504.3125,906158,4097958,33108786,5505493,8042527,1292298,888198.00025,2295613.5,2638394.6875,1104479.5,5883623.5,3028834.25,1081253,3071249.5,23119448.5,15891776.5,1623315,2701833.75,6238649.25,1035002.8125,710978.5,19031600,3621958.8125,367562.5,906306,1275146.78125,3162660.5,1265642.5469,4509031.25,6812549.25,707057,5552759,2518993,2243189.25,3005687.5,6953606.5,1218032.53125,6796335,1110661.5,534588,5077339.125,860191,6139671.5,1167045.875,3580761.5,6086744,11650917.5,5016379.25,917808.5,1704460.5,3022903.125,11930979,975543.6875,13990503.5,1631479,1497070,1806325.90625,1640950.8125,4200822,2455429.5,5863667.625,5836165.5,1312442.3125,2495696.5,226375,647321.5,9003558,18016406.5,1503579.5,7350656,2959714.375,2260917.25,2958270.625,4818495.375,591710.84375,426349.32814999996,3071249.5,416885.5938,7481768,1534669.5,1268276,581562,2642997.375,743805,6538401.75,6121635.5,3506373,548820,1067784,49669397,780294.875,926379.344,3095192.84375,2203134.75,10684625,827664,13651884,934865.25,646373.90625,1286269.5,18412081,1761683,1222329.6875,5039447.75,642825,12309632.5,32001059,1195370.8125,1921890,3565799.5,1367815,7777140.5,1866473.75,3199946.5,5004710.5,883411.78125,493708.5,1742513.4241071427,4423371.625,5517991,4199443,18900809,4191921.5,7259600,2298511,4257525.5,621781,3512584.375,639566.875,925317.5,14321623.5,602618.0625,1121020.1875,1576966.6875,2208348.375,4381553.625,5016379.25,8754111,4172280.5,3912611.75,672263.59375,5304860.875,19018762,700783.5,981327.5,1820292,706321.4375,31863457,2410264.5,906805.625,578417,5759589.5,862938,893875.9375,824525.59375,1279166.5,2050927.5,2516852.375,2292846.5,13059993,1104505,688927.15625,3565799.5,3375211,2557114,7450969,59867740,2454875.5,1064161.5,4143831.375,1515466,932049,17810313,2943117.875,1224276,2203134.75,777272.5,279039,4561991.875,5005318.75,1013154.6875,850615.8125,1197276.5,703819.5,2857697.34375,1360231.875,1938353.875,22718052.5,12587259.5,806398.1875,975927.375,1732884.625,1565826,1219742.375,1744041.625,2149322,871502.8125,13670384,1981516.5,3425294.5,3742204.35155,914670,2119761.5,793914.47655,968172,752296.0469,9801029.5,4795070,4985175.5,2327170.5,1027546.375,2178865.1875,90270118,1029079.625,2668405.875,2157011.3125,5911044.25,876319.594,7512011.75,1845068,1525995.875,1424907,2562690.5,2481449.6875,12028450.5,9763040,374804.1875,1529510.25,46734189,572082,4281823,8859899.5,1290113.5,4818450.25,845435,1503831,1398263.1875,3233150.75,644704.82815,3411120.75,706690.34375,2722408.5,1392569.0625,8751704,1646659,1705314.0625,2112398.6875,652705.59375,639944.89844,1686248,1209464.9375,8262080,1585692.5,36892128,1097922,1219816.1875,875886.375,1703644,765730,9568212.5,543017.5,1847364.625,1068038,1218516.85155,548496.51565,1744656.5,10653783,2373031,1526102.6875,2797998.8125,1059467,2582936.625,817409.625,1018459.5,2458686,715043,1566950,9154781.5,13969552,8935132.5,1677877,961476.1875,881108,5234092.75,1328842,1976511,3745618.625,2657625,526571.09375,3701114.5,5338522,4021141,707057,10161080.5,1031526.4375,653672,4556029.5,7648211.5,2820579.375,3202636.125,584575.625,14323140.5,2688956.3125,1577294.6875,836210.5,1536332.25,15991064,7539348.5,5579023.75,423797,9193842,2481781.1875,2916132.25,1387647.28125,502684.15625,7964519.75,7962472.25,2910494.5,5039447.75,7929071,68516896,6476419.5,19739412.5,1093276.46875,2709650.875,925596.3125,3920757.3125,16808.90039,5252609.5,2094000.875,1827317.5625,660219.5,4965751.5,1588699,1965832.5,714287,805216.81275,605186.3125,4349292,1430149.469,1705314.0625,2569733,12412165.25,1941980.5625,521648.29689999996,22902540.5,6710600,1937583.3125,9586556,13068731.5,961491.3125,2859201,636268,10087869.5,696709,1386449.8125,760090,1981516.5,676476.0625,8963609,1711458.25,2103515.0625,3470365.125,486711,391459,2187525.625,2557533,850285,288402,1043960.5,8529409.75,1281093.5,619901.5,5825405.25,1662195.90625,9741283,2607136.25,3640934.625,876339,492443.75,2937359.125,669418.5,2990238.75,6589678,652705.59375,2586995.5,23213473,13130122,7905245.25,21037390,2823184,6371580,3269709.5,1981516.5,2255714,4953777.625,468548.5,7454585,1992168,4164446,44567398,1204586.875,1142558.34375,3127945.5,2071404,24755086,1748712.375,3912611.75,5730346.25,4771648.5,658645.5,7946961,1086547,703452.34375,3833162,2040154.25,821751.5,1965485.5,2582936.625,1501713.75,6714960.5,4623266.5,1970560.5625,9085108,3848646.25,28184223,11681159.5,450518,1949914.375,873711.125,3036397.25,7450969,276107,4031054,1237655.25,1534669.5,2197129.9375,1499578.5625,517368.5,6648769,4583966,1851260.5,2294605.5,2071006,2651757,2539368,5984786.25,4073978,3172159.25,1939370.5,2268605.625,19148070,2450633.5,1289858.09375,596281.3125,10800669,2955439.5,806696.3333333334,2583968,1278513,975543.6875,1170597,1852393.6875,1516597.75,7227516.5,3400224.875],"xaxis":"x","y":[1437220,1614322,1416488.75,780410,5912066,1318947.25,281191,9146727,53332648,600528,2842760,6711975,2177075,8054334,962971,766628,3009967,2161828,2962476,860443,2075833,1121259,2709429,1406687.5,764094.5,26689658,5276777,1372475,1078936,9162048,4678999,768325.6875,572654.3125,562495.8125,2504814.5,4696639,875943.3125,436604.0938,1210262,841891,437372,1267348.5,2214604.25,1696448,4933374.5,1691021,3920913,1813864,1545519.375,1988760.875,1471914.25,5827595.5,6508595,804620.5,4236396.5,1524620,25307744,1661448,15232788,9512829,847298.8125,1489363.875,642052,22104512,1904641.75,131810,2462408.25,1261714,577642,479358,1355062.75,715064,63835192,1479728.75,592739.3125,557863.6875,5543575,2634166,2756494,433871,508057,308400.4063,1430356.75,12187231,592899,2216979,1884194.625,690750,673834,2478045.5,847767,647792.875,1810353,13458948,1072855,1606981.25,2291194,347437.6875,1423687,2045592,835258,2978555,2203026.5,1750419,1070381,614044.6875,5337334,2409429,3093378,430500,5767765,3418003,4618360,3546459,1310226,2729167.75,1639849.25,695955.8125,5061274,5454482,1498457.875,9190442,2996502,2890379,1795493,2364351,610744.6875,1424157.25,1003659,5907068,784584,725487,585851,1513700.625,4547129,362874,917100.3125,674961,619545.125,1906597,2001936.5,980356,1883132,721549.625,939840,1098965,23230284,533669.125,805751,7860942,883197.6875,784584,1049436.25,48143224,745575,1792425,603223.1875,1938613.125,15088676,1297041,13631141,2131817,1153276,4397572,960987.8125,19609472,7768294,2757855.5,7559797,1325972.75,25475474,1841968,559805,12051984,1469556.25,1314300,3375091.5,4665069,12726789,508264,9796649,1973429,17074142,7542332,531737.1875,1672142,1739693.25,3307792.5,1436773.875,843622.875,987987.3125,9997511,25478086,713660,3594908,2755829,1147582,2349950,27717026,1982967,429591,1645748.25,895901,947534,3528086.75,841821,973242,188745.7031,7564643,20311228,1593160,495481,5570472,2594119,336745.4063,2284133.75,5229981,13146645,838394.8125,2520919,3218565,1514279,9177040,1048781,6456026,24882924,6772289,10813735,4993962,628676.375,888561,892227,2842819.5,2265836,196410,541451.625,2371304.5,1325206,17812788,481206,519317,971155.625,1509492,1419453,1093724.375,8141155.5,797435,1162131,451738,1162519,5696695,6769410.5,2467851,6376820,1821047,600543,2637656,1299001.625,1857387,436848,2062419,1082076,1145535.75,3242077.75,1021754.688,1298039.875,395346.3125,1489392,1858224.625,1406987,1056099,2055534,840549.375,2126288,8819864,956598,10420096,350264.5938,507976,1565475,788976,599390,2345314,402136,2220053.25,65225380,880590,1916309,1241023.75,1093627,3879728,7442314.5,1256575.875,448676,1198291,1452804,645451.8125,2557933.75,4785692,522972.1875,919746.3125,1118392,145573,1093757.75,1493131.75,975327,2015280.125,861643.6875,1199742,699382,18760766,6471427,967492.875,2788860,2104255,3632613.75,5657046,866343,59991984,3497215.5,462192,682249.625,13140574,13349651,6856153.5,3033340,2646879,3925806,2852102.75,595978,2559486,841242,456397,618366,1555420.25,2058532.875,4383100.5,2321336,14697487,2241280.5,3059758,1260212.75,1791715.625,19594950,0,1689631,4863803,8259071,1820085,3983784,14084606,10287986,2966309,2059083.25,3040978,667871.125,869835,4704936.5,2091869,2067352.5,1199392,1878442.25,1297872,4917420,3591653,3318151.5,2105451,1391569,520206.6875,3292049,540465,6219841.5,2014500.375,5328224,421430,844000.3125,649874.875,10557910,4841347.5,4606967.5,962577,439521.0938,1275236.125,1426023,1299232,2655387.5,4252035,6320060,520011.8125,22388122,3371659.75,10769230,17225444,8787353,722951,3493093.75,1403094.75,2994822,710275.375,6454983,4328527,722891,2311532,1539191,1186835.125,2213115.5,456953,2601945,3986982.75,3537977,2339234,5417598,4169431.5,1222949.25,3646053.75,593543.3125,1266387,1951856.25,609368,5535626,9097980,758322,1509775,1417728.25,2076289.625,849745.6875,2228083.75,4471029,2334260,1773880,892086,1392299.75,847419.375,3644982.25,18558162,1162007.625,1686765.875,1253745.5,967835,56762408,11061916,65047284,4217623,928409,1742870,9734582,2046914,4817062,9329202,690304,646994,8241967,3612830,925293.3125,13214416,2019672.875,2717863.25,706505.6875,1242666.25,2154119,4567879,1128634.875,1176615,747994.8125,2477499,876569.6875,2666634,627854.125,1525624,1245187.5,688145,1820814.75,2468322.5,1885930,36378588,826488.6875,4391348,2447861,4045980,688641.8125,977150,10055690,850568,609101,5669431,2160420,12086616,5401277.5,835093.1875,694189,1378155,2625396,438557,3766069,4777648,1248599,579888.3125,528411.875,3155496,1359579,1769137,1686262.25,5917000,1004253,2564004,163945984,84980760,595327,1327591,2551022.25,585077.125,868163,507947,1331031,864751.8125,2093011,3586479.25,2892779,2946756.25,724153,3603866,2998208.25,1134195.5,2213421.5,4180298,2828950.5,1034827.313,1552645,2168281,3168809,730765,27137190,16614404,1360340,3091348.25,12015936,1445538.625,652449,1245197.5,157606480,896403,2512443,1933811.625,474811,1533245.75,1369190,1180231,3988729,1032857,836348,3185509,4192895,747326,3835202,13962749,960550,873776,1989017,686019.1875,588965,1072500,3996541.5,1777554,7655751,5689125,4728488.5,2654804,4585070.5,591249,1227772,21346100,6481807.5,489557.0938,2926166,1712279.875,1815350,952635.375,8873485,2920918,4329966,18628834,739711,767642.8125,504656,715446,317581,577023,4983934,2982450,972608,10614234,576038.1875,905926.875,20657852,896976,1965529,544172.375,638201,866242,914023,1111858,41655852,2440980,3605755.5,1093743,1516377,10340282,1015060.188,1045640,2726269,12087498,566000.6875,284867168,33729336,39403320,5974993.5,11924933,2993610,766381,1608224,1056782.125,2347893,4405368,357915,633408,4634655,8204897,2271263,2536760.75,694479,1271021,1315298,1484807,846274,1348497.125,11320378,1840259,502254,666385,807185.5,6860521,1758581,2971013.5,1162226.375,876871,40183424,48479988,30083496,6649938,1435558.125,1259959,4369513.5,493969.6875,1696978.75,777915,9143764,1052244.75,758266,1109851,696840,1608884.25,10084950,1224554,507217,4855036.5,919266.125,631195,5375518,9341427,1998767,3730953.5,59107620,2751694,3473109,59757440,3714139,5424718,6344171,1427294.375,131373880,2520483.25,6012303,12931002,5040382,9620496,267135.0938,36436388,1380283,1240875,1240924.625,1133028.875,6205886,2503948,32552428,9393108,811870,2786648.75,3913410,1084048,6298131.5,523133,439266.9063,6981428,6668568.5,7574092,1872278.625,2757588,5170809.5,2640980.25,8381744,1750362.25,781890,2100813.5,7335905.5,63811044,2330242,24603420,609257,549834,3322250,1695593,1357116,756196,980418,493000,7325166,703847.375,1356874.75,4826430,2057288,1360847,5327249.5,697824,4039667,23262244,842923,713326.875,7067404,2577115.25,2512319,25909504,1647746,19061780,2125323.5,636266,4729846.5,1601955.625,0,913899,529072,3404992,522791.0938,1187540.25,1527917,1182575,926261.1875,1725336.25,1152639,6915293,739869.6875,4904803,1172785,1230757,1063269.125,3467070,971990.3125,576174.125,1929118,918177,18576900,1988784,9230223,1163748.875,1333016.625,47952868,2074152,2660396,1552051.25,2056814,781682.5,1070740,734495,1059424,8063959,2145603.5,2217046,625988.625,2188093,3220635.25,791640,37980712,912509,728876,53466684,763693,1027230.875,948676.5,654664,16373232,3518459.5,4335178,1581036.75,1241865,1140620,7459418,52960796,819003.3125,2161240.25,59124384,2083584,906103,792015,8687690,1441665.375,449955,557193,455798.9063,2196718,4519115,1313366,5459657,861972.875,1012341,6669954,1332591.5,1507270,3893073.5,3132453,789209.8125,561473.875,519573,2715249.75,1140137,1791071.375,12783255,340270,588856.125,1970956,6582088.5,11047668,2925780,1055033,839011.3125,1605522,624180,20828130,10394330,10716391,1026921,659652.625,1987073,1172903.375,609218,2595783.5,3534690.75,2171445,980773.375,14431245,13852986,2668769,859236.625,3276819,1242311.5,696522,19459304,595936.5,165472,2108012,1848878.5,1090032,1201781.125,6584966.5,8631444,1290284,8688676,2599171.75,6843855,2712983.75,5952875,1750099.75,2856934,666355,3050455.25,1881516.125,700899,15240688,1625308.625,2502081,8227025,11208498,5885427.5,908834,1866802.375,4590246.5,1976920,948657,763364.5,1097572,3468606,2827772.75,1959777,1834290,3347567,4253577.5,3085505,0,2254220,1718845,755856.875,8386493,1738376,7369591,1886007,3075425,3724098.25,2979413.5,6881405.5,473874.0938,455153,3300549.25,422133,7606084.5,831577,519806.0938,776427,3158743,793436,9974370,28229320,3574542.25,630040,1243282,27273538,864420.875,953059.875,430473.6875,1269847.75,2471623,1372706,3182301,2556131,1440554.25,849504,8976533,3133849.5,1302636,5549121,659265,13491170,8915523,1045838.5,1921801.25,1231546,1210229,45313836,2338567.75,2322314,2327611,1372834,983687,741999.6875,3590731,3635600,4376927,18074716,2108861.75,55073120,2105912.75,1456039,843520,4673827,620154.375,2248776,21042926,547084.375,936336,1688612.5,3735308.5,5328051.5,4759256,9546165,14856199,6345009.5,601813.125,10077532,19036416,472532,653366,1856230.625,784329.5,42566736,2495819,1083156.5,362823,378714.4063,957632,488181,867184.375,1800331,1692323,3778355,1983509,2337146,1187261,618670.875,2219190,1650279.25,6042399.5,8555790,45163308,1415494,1125724,5965171,7291418,887403,5851209,3162976,1605669,161311.0938,4367202,731092,3548858.25,0,1619177.125,478690.0938,641713,572139,876304,783651.5,1852926.625,11110405,28731502,609921.6875,4857581.5,586589.1875,629036,1586388.75,1675510.875,3371455,1270582.125,20361158,1633499,3704086.5,6858533.5,832985,4202335,1906384.875,606886,743528.125,16016644,4130053,13951571,1743198,2266983,1331122.125,35891484,1386983.375,6062770.5,1904480,2674034,764832.6875,3282191,662012,1406648.125,1024033,1612797,5662234,11521702,16246106,664539,1465535,24017770,610512,3022275,7273155.5,390726,25631512,650205.8125,1350190,1740568.25,924192.6875,1139980.625,2185193,471484.8125,15473117,1922953.25,7577723,563298,1119640.25,915286.1875,960790.8125,863703,1591528.5,1358519,8835104,862945.125,36681188,571384,673907.3125,2222097.5,1681424,3005926,10444297,2356607,1754477.25,901679,2331952.75,535649.875,1870069,13924967,1835804.875,1346952,1152368.25,885865.875,2799874,776248.6875,1615855,4952487,767720,2139713,11788719,15571642,9686734,656980,985236,1834570,8442782,935442,1748629.875,2352982.75,1832808.75,932083.5,1612965,2650112,5471735,1324965,8690179,827618,1079004,19490284,13720714,5571501.5,1457118,833747,17186624,4345698,1371061.25,942089,1100499.75,14361382,6578370,5987079,1011399,6579348,3017709.25,1360400.5,2313978.75,44293.5,5118939,1497496.5,2439061,4660015,9343428,57514116,8422861,13256662,2181833.5,1076733.25,987288.8125,692127.875,671004.625,5143676.5,1073156.375,982153.875,391058,8058913.5,2277316,1377184,454296,690259,4801472,5755308,823032.375,1416470.25,1460307,9274650,1261634.5,3532966.5,13223740,8890235,1870457.75,3361743,3459543,953432.1875,4476997,1494252,7657568,597210,866126.375,648022,3494383,1258978.375,1679127,842593,2771820.5,930611.875,991283,622051,3936421.5,1961982,1160989,672915,3678939,9683979,1381407,777696,318364.8125,2470642,11007235,1517410.5,2845946,497556,549438,2520310,660508,3736960,75073888,1166069.875,705621.6875,1417778,16865978,3652708.75,14856602,97690.39844,4028075.5,14064603,1113488,1387773,1388812,1392153,8355988.5,1528642,17695420,61762380,1305578,1814947,2671115,2303288,20781390,2473302,605971.1875,2115666,4407932,804002,7753080,524144.6875,2466100,3579008.75,2188051.25,788027,2558573,7802205.5,560823.625,8987012,984657,1100575,1459786,1078535.75,33270518,5475367,872555,770175.8125,872114.5,1963481.625,45252356,2802428,3073778,979442.3125,752441,4145920.5,1294615.25,1730886,41696464,1826203,3234787.75,1188444,1501767,1819329,1049088,10105379,16238985,8305255,978463,2549892,16467486,1095744,440658,532670,7945081,444458,1122837,2454812,1247972,1130097,701422,6235947,1128311.125,6424029,2960159.75],"yaxis":"y"}],"layout":{"legend":{"tracegroupgap":0},"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Visualisation des données prédites par le modèle RandomForestRegressor() vs les données test"},"xaxis":{"anchor":"y","domain":[0,1],"title":{"text":"SiteEnergyUse_predRF"}},"yaxis":{"anchor":"x","domain":[0,1],"title":{"text":"SiteEnergyUse_test"}}}}},"metadata":{},"output_type":"display_data"}],"source":["n_estimatorsRF = np.logspace(0, 3, 10, dtype=int)\n","param_gridRF = {\n","    'randomforestregressor__n_estimators': n_estimatorsRF,\n","    'randomforestregressor__max_features': ['auto', 'sqrt', 'log2'],\n","}\n","\n","GridRF, \\\n","BestParametresRF, \\\n","ScoresRF, \\\n","SiteEnergyUse_predRF, \\\n","figRF = reg_modelGrid(model=RandomForestRegressor(),\n","                         scaler=scaler,\n","                         X_train=BEBM_train,\n","                         X_test=BEBM_test,\n","                         y_train=SiteEnergyUse_train.ravel(),\n","                         y_test=SiteEnergyUse_test,\n","                         y_test_name='SiteEnergyUse_test',\n","                         y_pred_name='SiteEnergyUse_predRF',\n","                         score=score,\n","                         param_grid=param_gridRF)\n","\n","print(BestParametresRF)\n","ScoresRF\n","figRF.show()\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"marker":{"color":"red","size":2},"mode":"lines","name":"RMSE moyenne","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[16140472.946640868,13001674.879052872,14375440.444431394,13591863.903745303,13695702.712942779,13600245.850107253,13100666.76019845,13143199.66569785,13204186.854890313,13200398.913658362]},{"line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDup RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[21534605.454878587,17689082.651385766,20363717.593129475,19190019.90126709,19663805.4934588,19624333.115192857,19287755.1213552,19211524.301007964,19306674.8880181,19276669.744321883]},{"fill":"tonexty","fillcolor":"rgba(68, 68, 68, .3)","line":{"width":1},"marker":{"color":"#444"},"mode":"lines","name":"SDdown RMSE","showlegend":false,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"y":[5687533.442657779,6069657.841660472,4419559.515936641,4715150.285771031,5754600.611378415,5434954.2279815655,4970906.776293305,4745949.90110988,4334640.660245183,4237035.316017094]},{"hovertemplate":"variable=ScoresSplit0<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit0","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[10549984.7934587,7191062.407837197,9319853.383316075,9038775.826529413,7632764.928163861,7721994.331542338,7057711.858544244,7095062.702762532,7223937.272238204,7227722.918090495],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit1<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit1","line":{"color":"#EF553B","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit1","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[24812397.305061586,18750118.899623647,25407937.909481734,23328012.44478594,23659801.21557933,23676947.30954608,23444264.419287354,23406772.04501215,23547607.60849186,23511085.1318082],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit2<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit2","line":{"color":"#00cc96","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit2","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[10339552.068784773,7894229.910888962,8855521.873741847,7236416.320371616,7446526.580233747,7155606.009022792,6470579.005379628,6896151.088798604,6847996.270295569,6896300.489869016],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit3<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit3","line":{"color":"#ab63fa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit3","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[16583968.52588949,14244340.27332607,13295935.302133227,13960579.134944227,14523307.54086159,13917637.24012717,13483796.387648355,13413151.450916812,13471025.034168247,13495032.215248335],"yaxis":"y"},{"hovertemplate":"variable=ScoresSplit4<br>x=%{x}<br>value=%{y}<extra></extra>","legendgroup":"ScoresSplit4","line":{"color":"#FFA15A","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"ScoresSplit4","orientation":"v","showlegend":true,"type":"scatter","x":[1,2,4,10,21,46,100,215,464,1000],"xaxis":"x","y":[18416462.040009785,16928622.903588478,14997953.753484087,14395535.792095324,15216113.299875367,15529044.360297877,15046982.130132662,14904861.04099915,14930368.089257687,14871853.813275762],"yaxis":"y"}],"layout":{"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"RMSE du modèle RF pour le paramètre max_features=log2 en fonction de l'hyperparamètre alpha"},"xaxis":{"title":{"text":"n_estimators"},"type":"log"},"yaxis":{"title":{"text":"RMSE"}}}}},"metadata":{},"output_type":"display_data"}],"source":["# graph visualisation RMSE ElasticNet pour tout le meilleur paramètre max features\n","for i in BestParametresRF['RandomForestRegressor()'][\n","        BestParametresRF['paramètre'] ==\n","        'randomforestregressor__max_features']:\n","    fig1 = go.Figure([\n","        go.Scatter(\n","            name='RMSE moyenne',\n","            x=n_estimatorsRF,\n","            y=GridRF.ScoresMean.where(\n","                GridRF.randomforestregressor__max_features == i).dropna(),\n","            mode='lines',\n","            marker=dict(color='red', size=2),\n","            showlegend=True),\n","        go.Scatter(\n","            name='SDup RMSE',\n","            x=n_estimatorsRF,\n","            y=GridRF.ScoresMean.where(\n","                GridRF.randomforestregressor__max_features == i).dropna() +\n","            GridRF.ScoresSD.where(\n","                GridRF.randomforestregressor__max_features == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            showlegend=False),\n","        go.Scatter(\n","            name='SDdown RMSE',\n","            x=n_estimatorsRF,\n","            y=GridEN.ScoresMean.where(\n","                GridRF.randomforestregressor__max_features == i).dropna() -\n","            GridRF.ScoresSD.where(\n","                GridRF.randomforestregressor__max_features == i).dropna(),\n","            mode='lines',\n","            marker=dict(color=\"#444\"),\n","            line=dict(width=1),\n","            fillcolor='rgba(68, 68, 68, .3)',\n","            fill='tonexty',\n","            showlegend=False)\n","    ])\n","\n","    fig2 = px.line(\n","        GridRF.where(GridRF.randomforestregressor__max_features == i).dropna(),\n","        x=n_estimatorsRF,\n","        y=[\n","            'ScoresSplit0', 'ScoresSplit1', 'ScoresSplit2', 'ScoresSplit3',\n","            'ScoresSplit4'\n","        ])\n","\n","    fig3 = go.Figure(data=fig1.data + fig2.data)\n","    fig3.update_xaxes(type='log', title='n_estimators')\n","    fig3.update_yaxes(title='RMSE')\n","    fig3.update_layout(\n","        title=\n","        \"RMSE du modèle RF pour le paramètre max_features={} en fonction de l'hyperparamètre alpha\"\n","        .format(i))\n","    fig3.show()\n","    if write_data is True:\n","        fig3.write_image('./Figures/graphRMSERF{}.pdf'.format(i))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"117c35e9bc21f93c21d2b781ffd59753bc10bfa2757aefbea5ab108f880d10a5"},"kernelspec":{"display_name":"Python 3.9.9 64-bit ('.env': venv)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
